#Concurrency[concurrency]

Concurrency is about doing more than one thing at once. Concurrency in programming languages is important for two basic reasons: modern processors are easily capable of executing many tasks in parallel (and by implication, if your program does not then you may not be making good use of the machine) and many applications have to be able to respond to events without having to process everything in a fixed order.

Building concurrent applications is hard for programmers primarily because many of the assumptions built into modern programming languages are not valid when performing activities concurrently. The most important of these assumptions relates to state — for example, the values of mutable variables.

In particular, the most basic assumption

>the value of a variable stays the same until it is modified

is _not_ valid when the variable in question is shared by multiple concurrent activities. Or rather, technically, it is, but when a variable is shared, it is both difficult and tedious to determine when or if the variable has the expected state. It is difficult to overstate the importance of this.

>An interesting parallel shows up in dealing with weightlessness in space: astronauts 'complain' of items drifting about in the space station — 'nothing stays where you left it'. As a result, astronauts must get used to taping everything down while they are working.

>Programmers building concurrent systems have a similar experience: they are constantly having to 'tape down' pieces of state to make sure that they 'stay put'.

There are many potential models for concurrency in programming. However, two core ideas seem to stand out: the idea of _sequence_ and the idea of _sharing_. Sequencing actions in most programming languages is almost completely implicit: the idea of performing actions in sequence is so hard-wired in languages like Java and C/C++ that one must think hard to step outside that frame of reference. However, in a concurrent execution, sequentiality is lost: two activitivies can proceed in parallel[^We reserve the right to conflate strict parallelism with apparent parellelism here.] and the relative order of actions between the activities is not possible to predict.

The second concept that is so implicit is that of sharing: in a sequential program we assume that subsequent actions share the effects of earlier actions. But, again in a concurrent context, sharing is not obvious; and it is hard to reason about.

Our approach is to make _explicit_ that which is _implicit_. We reason about actions and their order explicitly -- by employing the classic software engineer's approach of taking an extra level of indirection: we take a handle on the computation itself using the concept of a first-class task as a unit of execution. Similarly, we reason about sharing by focusing on explicit _coordination_ between tasks and on explicitly modeling sharing by _communicating_ between them.

>One thing that may strike the reader is that tasks, and as we shall see more generally computation expressions, are associated with actions. This is new in our text so far -- we have mostly focused on functions and related concepts.
>The reason for the change is straightforward: concurrency is inherently about the relative ordering of computation. That makes the action paradigm a natural fit for tasks and concurrency. However, we are fairly disciplined in our approach to actions; in particular, we strongly regulate implicit sharing of state. In fact, it is the implicit sharing of state that is the biggest issue with sequential programming.

##Tasks[tasks]

A task expression denotes a possibly suspended _computation_ — which itself is a sequence of actions — and which may have a value when the computation is performed. Furthermore, tasks have the potential to be executed concurrently or even in parallel with other tasks.

>Like tasks, functions can also be viewed as representing suspended computations. However, unlike functions, a task expression represents a single suspended computation.

A `task` expression is written as a sequence of actions, enclosed in a `task` block; for example, in:[^As in other programming languages, actions are separated by semi-colons in **Star**.]

```
T = task{
  X = 1;
  Y = 2;
  valis X+Y
}
```

the variable `T` is bound to a `task` expression whose returned value will be `3` — the value returned by the action:

```
valis X+Y
```

Task expressions have a type: `task[t]` where `t` is the type of the value returned by the `task`. So, for example, the type of `T` above is given by

```
T:task[integer].
```

as it produces an `integer` value.

###The Value of a Task[the-value-of-a-task]

As we saw above, a task computation produces a value by performing the `valis` action:

```
valis X+Y
```

The _value_ of a `task` value is obtained using the `valof` operator:

```
valof T
```

denotes the value returned by the task computation `T`.

It is important to remember, in thinking about tasks, that there is the extra level of indirection.

Note that the `valof` operator also has the effect of forcing the completion of the task `T`; in particular, the `valof` operator _is blocked_ until the task has completed and its value can be returned.

>Although the `valof` operator denotes the end of the computation, it does not denote the start of the computation.

If a `task` does not return a value then its type will be

```
task[()]
```

Obviously a `task` that does not have a value does not require it's body to contain a `valis` action; however, if it does, then that action should be:

```
valis ()
```

In effect, tasks that do not purport to return values _do_ return a value — the void tuple value. To avoid the clumsiness of handling empty tuples, especially in the context of actions, it is better to `perform` such a task instead. Like the `valof` expression form, the `perform` action forces completion of its task argument — thus achieving some synchronization with the performed task.

For example, the action:

```
perform task {
  nothing
}
```

performs the `task` (which does `nothing`) and waits for nothing to finish (sic).

Void tasks are occasionally useful — for example, multi-tasked applications often contain a _server_ component: a component that responds to requests made by other tasks. Such a component typically is not expected to return and so does not have a useful return value.

####Getting the value more than once[getting-the-value-more-than-once]

Note that it is possible to access a `task` value multiple times. However, the actions in the task body will only ever be performed once: subsequent attempts to get the value of the task simply return the value previously computed.

###Background Tasks[background-tasks]

A `task` can be started in the background — i.e., concurrently with other `task`s — by using the `background` operator, which is a function from `task`s to `task`s. It's type is given by

```
background:all t ~~ (task[t])=>task[t].
```

Applying `background` to a task not only yields a task of the same type, but also one yielding the same value.

The `background` operator has a simple effect — it starts its argument `task` so that it will operate in parallel with other computations.

>If it happened to be the case that a backgrounded task already completed, then the `background` operator has no effect.

###Computing with tasks[computing-with-tasks]

Like other values, `task`s are first class: they can be assigned to variables, kept in data structures, passed to and from functions and so forth. This flexibility leads to great expressive power — many patterns of computations can be readily encoded as `task`-valued functions.

For example, consider the `mp` function — which is a facsimile of the standard `map` function specialized for `list` values:

```
mp:all s,t ~~ ((s)=>t,list[s])=>list[t].
mp(F,[]) => [].
mp(F,[X,..Y]) => [F(X),..mp(Y)].
```

In this `mp` function,[^Notice that we do not need to mark the sequence expressions as `list` sequences -- type inference takes care of this for us.] the functional variable `F` denotes the computation to be applied to each element of the input list. Suppose that each computation of `F` were non-trivial, and we wanted to spread the load across multiple cores — i.e., to perform the map operations in parallel. The `task` notation, in particular `background` tasks will help us to achieve that.

>Recall that `map` is a standard **Star** function that is defined in the `mappable` contract and is implemented for many different collection types.

We will first of all transform `mp` to use tasks rather than simply calling the function:

```
taskmap:all s,t ~~ ((s)=>t,list[s])=>list[t].
taskmap(F,[]) => [].
taskmap(F,[X,..Y]) => [valof task{ valis F(X) } ,.. taskmap(F,Y)].
```

As it stands, this function has very similar performance characteristics to `mp`; except that we are using the `task` expression notation. In order to run the different elements in parallel we need to also use the `background` operator:

```
parmap1:all s,t ~~ ((s)=>t,list[s])=>list[t].
parmap1(F,[]) => [].
parmap1(F,[X,..Y]) => [valof background task{valis F(X)} ,.. parmap1(F,Y)].
```

This program computes each element of the result in a separate background task. However, it is not a true parallel map because we wait for each element before continuing to the next element.

A better approach is to first of all construct a list of `task`s and then to separately collect their values in a second phase:

```
parmap2:all s,t ~~ ((s)=>t,list[s])=>list[t].
parmap2(F,L) => let{
  spread:(list[s])=>list[task[t]].
  spread([]) => [].
  spread([X,..Y]) => [background task{ valis F(X)} ,.. spread(Y)].

  collect:(list[task[t]])=>list[t].
  collect([]) => [].
  collect([T,..Ts]) => [valof T,..collect(Ts)].
} in collect(spread(L))
```

This function does not wait for any task to complete until they have all been spun out into background activities. This gives the maximum opportunity for the independent tasks to complete before we actually need their values.

A good rule of thumb is:

>when you are programming with tasks, everything that is not a completely task-less subcomputation should be enclosed in `task` brackets.

So, a more idiomatic way of writing the `parmap` function is to make it a `task` valued function; we also generalize away from the specific `list` collection type and use the standard `map` function to allow for any collection type:

```
parmap:all s,t,c/1 ~~ mappable[c] |: ((s)=>t,c[s])=>task[c[t]].
parmap(F,L) => let{
  spread:()=>list[task[t]].
  spread() => map((X)=>background task{valis F(X)},L).
  collect:(list[task[t]])=>list[t].
  collect(Lt) is map((T)=>valof T,Lt).
} in task{
  valis collect(spread())
}
```

##Computation Expressions and the **_M_** word [computation-expression]

Task expressions are instances of a more general **Star** feature: the _computation expression_. Task expressions are oriented towards the concurrent execution of computations; other forms of computation expression have different purposes. In particular, the simple `valof` expression:

```
valof{
  x = 2;
  y = f(x);
  valis x+y
}
```

is actually a degenerate case of an `action` computation expression:

```
valof action computation {
  x = 2;
  y = f(x);
  valis x+y
}
```

There are several forms of computation expression in the **Star** library; they are used for expressing tests for example. In fact, computation expressions are _syntactic sugar_ for _monadic_ expressions — and play the same role in **Star** as the `do` notation plays in Haskell.

Monads represent a generalization of the kind of deferred computation we see in `task` expressions. In effect, monads represent a standard way of _composing computations_.

The computation expression is syntactic support for using the monad contract — actually called `computation`. We can see the extent of this support by looking at the 'raw' version of the `action` function:

```
AA(f) is action computation {
  def x is 2
  def y is valof f(x)
  valis x+y
}
```

The `AA` function becomes the somewhat more complex form:[^This is actually a somewhat sanitized version of the raw code. This is because we have hidden a required transformation from action sequences to functional expressions that encode the computation's ordering as function calls.]

```
AA:((integer)=>integer)=>integer.
AA(f) => _delay(() => valof{
  x = 2;
  y = _perform(f(x), raiser_fun);
  valis _encapsulate(x+y)
}).
```

Note how the inner `valof` expression is transformed to a call to `_perform`; whereas the result of the `action computation` is encapsulated in a call to the function `_delay`. These are all functions in the standard `computation` contract; and so will actually be further specialized to be specific for the `action` expression.

It is beyond the scope of this book to explore more deeply the handling of monadic computations. However, if you wish to explore further, a more complete description of computation expressions is given in the **Star** language definition.

##Rendezvous[rendezvous]

Tasks support a very simple communication pattern between parts of a computation: a task computes for a while, and then (if appropriate) returns a result to anyone asking for its value — typically a parent task. While a task can spawn sub-tasks, there is no immediate way for tasks to communicate with each other _while_ they are running. Many applications, however, require more fine-grained coordination between tasks.

To permit this better coordination, and to permit data to flow between tasks, we have the _rendezvous_. A rendezvous is a meeting between two or more tasks or other computations. Data can flow between tasks at a rendezvous; with the guarantee that the data is consistent for the parties at the rendezvous.

The rendezvous mechanism is inspired by Reppy's Concurrent ML _events_ [][#Reppy1999], and by Hoare's Communicating Sequential Processes[][#hoare78] [][#hoare85] (which also involves the rendezvous concept). As we shall see, the rendezvous is a simple mechanism that allows implementing almost arbitrarily complex choreographies between concurrent computations. However, we shall also see that the rendezvous mechanism is relatively low-level and there are good higher-level abstractions that make writing concurrent applications significantly easier.

###A `rendezvous` is a Future Event[the-rendezvous-type]

A rendezvous is an object describing an event that may or may not happen in the future. The basic operation for a rendezvous is _waiting for_ the event, and when the event happens, it may yield or consume a value.

This is reflected in the `rendezvous` type itself:

```
all t ~~ rendezvous[t]
```

One of the simplest forms of rendezvous denotes a _timeout_:

```
timeoutRv:(integer) => rendezvous[()].
```

The rendezvous returned by `timeoutRv(10)` describes an ''alarm clock'' event that happens 10 milliseconds in the future. Note that the type of the rendezvous returned specifies `()` as the type of the value yielded by the event - the timeout event provides no information other than that it happened.

It may seem a little odd that a rendezvous — which is after all a meeting — is denoted by an actual value. The primary reason for this is that is allows specific synchronizations between tasks to be _computed_, not just _programmed_. This is a simple but major departure from the approach taken to concurrency by languages that support locking features; but is actually highly reminiscent of Unix's `select` function.

Timeouts can be useful in their own right, but they are commonly used in conjunction with other rendezvous. The pattern being that if one of the other 'things going on' does _not_ happen, then the timeout will tell us that. However, _recovering_ from a timeout can be very problematic — since you now have to decide what to do about the other activities that were pending.

###Waiting for Events[waiting-for-events]

The `wait for` operator takes a rendezvous and yields a `task` that waits for the event described by the rendezvous to happen. So, for example:

```
wait for timeoutRv(2000)
```

is a task that, when `perform`ed, does nothing for 2 seconds, and yields the `()` value.

Note that `wait for` returns a task - it does not execute it. To actually wait for an event, the task must be performed. Typically, within a task block, it is used like so:

```
task {
  ...
  perform wait for timeoutRv(2000)
  ...
}
```

There are quite a few pieces here: the `timeoutRv` event itself, the `wait for` task function and the `perform`ance of the task itself.

This separation into distinct phases helps in flexibility of the concurrency features but, of course, can make straightforward scenarios complex to construct. We shall see below that there is a library of concurrency features that are quite high-level and may be more appropriate than 'rolling your own' setup.

###Channels of Communication[channels-of-communication]

In many cases the requirement for synchronization between tasks is based on needing reliable communication between them. Instead of building communication on top of a model of 'shared resources', we use direct 'message passing': one task sends a message to another. The mechanism we use is called the `channel`.

**Channel**
: A `channel` is a means by which two (or more) tasks can pass data in a type-safe way between themselves.

Channels are _synchronous_: in order for data to be passed between tasks they must both be paused. There are rendezvous associated with receiving messages from a channel and for placing messages on a channel — actual communication only occurs when both the sender and the receiver have waited for the event to occur.

>One might ask 'why is communication synchronous?' Why not asynchronous? The basic answer is that synchronous communication seems to be more basic than asynchronous communication.

It is straightforward to implement a more asynchronous communication pattern based on synchronous primitives. It is harder to do the converse: to implement synchronous communication using asynchronous primitives.

However, it is also true that synchronous communication is essentially impossible when that communication involves multiple computers. The concurrency features in **Star** support multiple threads of activity within a single application but do not directly support networked applications. We believe that, for other reasons, such networked applications are better served with different features — features that directly address issues involving agenthood. We shall see some of these in our treatment of [Concurrent Actors][concurrent-actors].

The `channel` function creates such a synchronous channel:

```
channel:for all t ~~ () => channel[t]
```

There are two rendezvous operators for sending and for receiving data on a channel:

```
sendRv:for all t ~~
  (channel[t], t) => rendezvous of ()
recvRv:for all t ~~
  (channel[t]) => rendezvous of t
```

Notice that the `sendRV` operator returns a void-valued rendezvous; whereas the `recvRv` operator returns a rendezvous with a value.

_Type safety_ is ensured by the type carried by the `channel` value: the sender and receiver of a channel must also agree on the type of the data being communicated. Channels have no fixed concept of direction. A task can send a message over a channel in one step, and receive a message over the same channel in the next.

Here is a simple scenario involving a transmission over a channel:

```
def ch is channel()
ignore background task {
  def msg is valof (wait for recRv(ch))
}
ignore background task {
  perform wait for sendRv(ch,15)
}
```

The communication occurs when both `background`ed tasks reach their appropriate rendezvous: one is waiting for the message to arrive and the other waiting to be able to send it.

In our message passing sequence, we actually do _not_ want the task arguments to complete in a sequential order — so `ignore`ing (sic) the task is the appropriate command.

###Computing Primes With Tasks[computing-primes-with-tasks]

Before, we looked at implementing the sieve of Eratosthenes using operations over collections. The Sieve, as it is affectionately known, is also a great demonstrator of concurrent programming.

Recall that The Sieve has two parts: a collection of activities that filter a sequence of numbers looking for multiples of previously found primes and a master that grows the set of filters as new primes are found.

####Filtering for Primes[filtering-for-primes]

Let us start with modeling each filter as a concurrent task; which can be implemented using:

```
fun filter(P,inChannel) is let{
  def outChannel is channel()
  fun loop() is task{
    while true do {
      def I is valof (wait for recvRv(inChannel))
      if I%P!=0 then -- not a multiple, pass it on
        perform wait for sendRv(outChannel,I)
    }
  }

  { ignore background loop() }
} in outChannel
```

The heart of the `filter` function is a concurrently executing task that listens for input, checks to see if the input number is a multiple of 'its' prime, and if it is not passes it on.

Notice that the `filter` function is written in such a way that it does not directly know what the recipient of the output messages will be. This is quite common in programs of this type and is the concurrent analog of returning a value from a function. In this case the returned 'value' is presented as a sequence of messages on a channel.

The sharp-eyed reader will spot something else that is novel here: the `let` environment has a local action as well as two definitions. The form:

```
{ ignore Action }
```

within a `let` definition environment denotes an action that is performed as part of 'constructing' the `let` environment.

One might ask, why do we `ignore` the background task and not `perform` it? Well, the simple answer is that performing a task implies waiting for the task to finish. In fact, `perform`ing a task is equivalent to ignoring the result of taking its value:

```
ignore valof background loop()
```

will clearly not finish until the loop finishes; which it will not.

Our `filter` task will never terminate, indeed the entire Sieve consists of a network of tasks that do not terminate of their own accord.

####A Stream of Naturals[a-stream-of-naturals]

Before we look at the main sieve, let us see how we construct a generator stream. In the case of the sieve, we need a stream of positive integers (naturals) that will act as the source of potential prime numbers (to be filtered of course).

The `naturals` function is set up to return a `channel` on which will be placed the odd integers greater than `2`:

```
def naturals is let {
  def natChannel is channel()
  { ignore background task {
      var counter := 3
      while true do{
        perform wait for sendRv(natChannel,counter)
        counter := counter+2
      }
    }
  }
} in natChannel
```

This has a similar structure to the `filter` function except that the loop does not wait for any input — it keeps on generating odd numbers. This is where the synchronous nature of communication is important: the action:

```
perform wait for sendRv(natChannel,counter)
```

will only complete if there is a task that is waiting for the message. So this streamer will _not_ gush out numbers unless there is someone listening.

####Sieving for Primes[sieving-for-primes]

The final piece of The Sieve is the `sieve` function which constructs a network of filters to sieve out the primes.

At any one time there is an existing network of filters removing multiples of prime numbers found so far. For example, after processing `13`, the next number to look at will be 15 and the network of filters will look like:

![The Sieve of Eratosthenes at 13][eras13]

[eras13]:images/eras13.png width=350px

The `3` filter removes `15` because 15 is a multiple of 3. However, the following number — 17 — survives all the filters and the sieve responds by spinning off a new filter for `17` at the end of the chain:

![The Sieve of Eratosthenes at 17][eras17]

[eras17]:images/eras17.png width=350px

I.e., The Sieve also receives input from a source, but its response is to spin off a new filter. We can program this using the `sieve` function:

```
fun sieve0(inChannel) is valof{
  def nxPrime is valof (wait for recvRv(inChannel))
  valis sieve0(filter(nxPrime,inChannel))
}
```

The crux of the `sieve` function is that the same channel that it is listening to for the next prime number is then passed to the newly created `filter` task and the result of _that_ filter will be the input for the next cycle of the `sieve`.

This variant of The Sieve does not report the primes it finds; in fact, it will not terminate until the stack overflows. A more appropriate variant will find a list of primes up until some number. We first of all engineer another task that listens to a channel for a sequence of terms and collects the result into a list:

```
fun collectTerms(ch,Count) is background task{
  var data := list of []
  for cx in range(0,Count,1) do {
    def nxt is valof ( wait for recvRv(ch) )
    data := list of [data..,nxt]
  }
  valis data
}
```

Given this function, we augment our sieve with the additional channel for sending the results to:

```
sieve(inChannel,results) => valof{
  nxPrime = valof (wait for recvRv(inChannel))
  perform wait for sendRv(results,nxPrime)
  valis sieve(filter(nxPrime,inChannel),results)
}
```

The complete system can be put together into a `primes` function that uses multiple tasks to sieve for primes:

```
primes(Max) => let{
  resltCh = channel().
  { ignore background task { valis sieve(naturals, resltCh)}}
} in valof collectTerms(resltCh,Max)
```

###Philosophers Have to Eat
Too[philosophers-have-to-eat-too]

The analog of the 'hello world' example in concurrent programming is probably the in-famous dining philosophers problem. Made famous in this form by C.A.R. Hoare (also of QuickSort fame), the set up for this puzzle is quite straightforward: there are four philosophers sitting at a table wanting to eat. The catch being that there are only four forks on the table and a philosopher needs to use two forks to eat something from the table.

![Four Dining Philosophers][phil]

[phil]:images/phil.png width=350px

Of course, it is only a little eccentric that the philosophers eat with two forks rather than a knife and fork!

Being philosophers, they don't eat constantly: they spend some time talking too; which means that it is possible for them to _share_ the forks. Each philosopher has access to the two forks nearest him; each fork is shared by two philosophers; but a given fork can not be in use by more than one philosopher simultaneously.

This problem is one of _resource contention_ and the underlying issue is to avoid the different kinds of contention issues — such as _deadlock_ (where everyone is stuck and no progress can be made), _livelock_ (where no effective progress is made) and _starvation_ where one or more of the philosophers receives an unfair allocation of the forks — preventing others from using the forks.

There are two well-known solutions to the dining philosophers problem; one is due to Hoare and the other is due to Mani Chandy. We will show how to realize Hoare's solution as it introduces some additional features of our concurrency platform.

In essence, the Hoare solution depends on a combination of _semaphores_ to manage the status of the various forks and an additional _arbitrator_ to ensure deadlock-free execution.

####Using Channels and Rendezvous to Implement
Semaphores[using-channels-and-rendezvous-to-implement-semaphores]

A semaphore is a general structure for implementing mutual exclusive access to some resource.

**Semaphore**
: A **semaphore** is a variable, with associated **grab** and **release** operations, that can be used to
manage access to a shared resource by multiple concurrent activities.

It is traditional in designing semaphores to allow an arbitrary — but controlled — number of clients access to the resource. A non-recursive _lock_ is equivalent to a semaphore with a count of 1.

Given a semaphore, it is used in a sequence that looks like:

```
S.grab()
... -- access the shared resource
S.release()
```

The idea is, of course, that if the resource is not currently available then activities that attempt to **grab** the semaphore will be blocked. When the resource becomes available (by someone else performing the semaphore's **release** action) then the **grab** may be re-attempted.

Semaphores are useful, but they are prone to mis-programming. Use of a semaphore relies on the programmer ensuring that the **release** is invoked after a **grab** and, furthermore, any connection between the semaphore and the governed resource is purely implicit. Both of these are common sources of errors in concurrent programs.

To build a semaphore we have to arrange for a unique resource[^Not to be confused with the resource that is governed by the semaphore.] together with coordinated access to it; preferably without exposing the resource itself (to prevent it from being _contaminated_ by external code).

One way of implementing such a unique resource is to use an internal background task and to map calls to `grab` and `release` to communication to the task. This internal task only needs a simple structure: it is simply listening for messages either to `grab` or to `release`. The crux is that, at any one time, either a grab or a release message might arrive and we cannot control this ahead of time. In effect, we have to be able to listen to more than one potential message, and the choice rendezvous allows us to do this.

The main loop of the semaphore's internal task is modeled using a mutual recursion between the `grabR` function, `releaseR` function and the `semLoop` functions. The forms of `grabR` and `releaseR` are actually very similar:

```
grabR(X) =>
  wrapRv(recvRv(grabCh), (_) => semLoop(X-1).
releaseR(X) =>
  wrapRv(recvRv(releaseCh), (_) => semLoop(X+1)).
```

The `wrapRv` operator is a function that takes a rendezvous and a function as argument; it returns a new 'wrapped' rendezvous as its result. If the argument rendezvous fires then the result of that rendezvous is passed to the function argument — which in turn becomes the value returned by the `wrapRv` rendezvous.

In the case of `grabR`, the effect is that a message on the `grabCh` channel is passed to the lambda; which ignores the actual message but recursively invokes the semaphore loop function — with an decremented 'resource counter'. The `releaseR` function is similar except that `semLoop` will be invoked with an incremented counter and that it is listening to the `releaseCh` channel rather than the `grabR` channel.

The `wrapRv` function is a useful way to 'do something' during a rendezvous. It is also the best way of converting a rendezvous of one form into that of another — to allow, for example, a combination rendezvous.

There is an analogous operator — `guardRv` that has the effect of enabling a rendezvous only if some condition is met.

In both cases, the result is a new invocation of `semLoop` which — depending on whether the counter is zero or not — will either listen for a `release` or both a `releaseR` or `grabR`:

```
fun semLoop(0) is wait for releaseR(0)
 |  semLoop(X) default is wait for grabR(X) or releaseR(X)
```

>In implementations of **Star** that do not support tail recursion, this code would need to be rewritten as a loop. However, we leave it as a recursion for expository purposes.

If we focus on the disjunction in:

```
wait for grabR(X) or releaseR(X)
```

This is a rendezvous that is composed of two rendezvous; and the `wait for` operator waits for either one to occur. Disjunctive rendezvous like this are critical for many concurrent applications.

It is important to emphasize that the disjunctive `wait for` waits for exactly one rendezvous to occur. In the situation that both happen simultaneously then only one will be picked for this choice. The unchosen rendezvous may be 'picked up' by another `wait for` operation.

Notice that the disjunctive rendezvous is only entered when the value passed to `semLoop` is non-zero (it will actually be positive). If the value was zero then the task will only wait for a `releaseR` rendezvous. This captures the essential requirement that a rendezvous will limit access to its associated resource to a fix number of clients.

We are now in a position to show the complete workings of the `semaphore` function in:

```
semaphore:(integer) => { grab:()=>rendevous[()], release:()=>rendezvous[()]. }.
semaphore(Count) => let{
  grabCh:channel[()] = channel().
  releaseCh:channel[()] = channel().

  releaseR:(integer)=>rendevous[()].
  releaseR(X) =>
    wrapRv(recvRv(releaseCh), (_) => semLoop(X+1)).

  grabR(X) =>
    wrapRv(recvRv(grabCh), (_) => semLoop(X-1).

  semLoop:(integer) => rendevous[()].
  semLoop(0) => wait for releaseR(0).
  semLoop(X) default => wait for grabR(X) || releaseR(X)

  { ignore background semLoop(Count) }
} in {
  grab() => wait for send(grabCh,()).
  release() => wait for send(releaseCh,()).
}
```

Notice that the value returned by the `semaphore` function is itself a record — with the two functions `grab` and `release` embedded in it. Thus an expression of the form:

```
S.grab()
```

is an invocation of the `grab` function from that record — and denotes an attempt to 'grab' the resource managed by the semaphore. Similarly, `S.release()` counts as releasing the resource.

Because the inner workings of the semaphore are protected by a `let` expression, they cannot be seen externally by other programs — only the `grab` and `release` functions are exposed; hence meeting one of the key requirements of the semaphore implementation.

####A Meeting of Philosophers[a-meeting-of-philosophers]

We can model a philosopher as a `task` function that iteratively acquires a left and right fork, 'eats' for a random period, and then stops eating after relinquishing its two forks (so another philosopher can eat).

The classic Hoare solution to the deadlock problem is to invoke an additional element: a central 'table'. The role of the table is to ensure that only one philosopher at a time is requesting forks; this, in turn, prevents a deadlock situation where two or more philosophers can start the process of acquiring their forks but are unable to complete because the another philosopher 'has' the other fork.

The table can be modeled straightforwardly as a semaphore with a limit of 1: only one philosopher can be starting to eat at any given time. Note that this does not mean that only one philosopher is eating at any one time: as soon as a philosopher has gotten two forks, another philosopher can start the process. Of course, given that there are four forks, only two philosophers can actually be eating simultaneously.

The main cycle for the philosopher task then looks like:

```
sleep(random(10L))  -- chat some
perform T.grab()    -- get permission first
perform L.grab()    -- get left fork
perform R.grab()    -- get right fork
perform T.release() -- release the table

sleep(random(15L))  -- eat some

perform L.release() -- let go of left fork
perform R.release() -- let go of right fork
```

The `sleep` calls in this sequence stand for some arbitrary activity — such as chatting or eating — that does not involve forks and/or the central table.

The complete `phil` function returns a `task` that performs this cycle a fixed number of times. The program below shows the `phil` function that takes the semaphores for the left and right forks as argument, together with an identifying philosopher number for debugging purposes.

```
fun phil(n,L,R) is task{
  for Ix in range(0,Count,1) do{
    sleep(random(10L))  -- chat some
    perform T.grab()    -- get permission first
    perform L.grab()    -- get left fork
    perform R.grab()    -- get right fork
    perform T.release() -- release the table
    sleep(random(15L))  -- eat some
    perform L.release() -- let go of left fork
    perform R.release() -- let go of right fork
  }
}
```

>The eagled-eyed reader will notice that while `L` and `R` are parameters of the `phil` function, the `T` variable is not. Indeed, it is a free variable that must be bound in an outer scope.

The complete setup for the dining philosophers involves creating separate semaphores for each of the forks and for the table, spinning off background `task`s for the philosophers, and then forcing them to run with a sequence of `perform` actions is given in:

```
prc dining(Count) do let{
  def T is semaphore(1)  -- The table

  fun phil(n,L,R) is task{ ... }
} in {
  def fork1 is semaphore(1)
  def fork2 is semaphore(1)
  def fork3 is semaphore(1)
  def fork4 is semaphore(1)

  def phil1 is background phil(1,fork1,fork2)
  def phil2 is background phil(2,fork2,fork3)
  def phil3 is background phil(3,fork3,fork4)
  def phil4 is background phil(4,fork4,fork1)

  perform phil1
  perform phil2
  perform phil3
  perform phil4
}
```

This program will run the simulation for four philosophers for a `Count` number of times. We have not added any trace information, so there will not actually be any output; however, that is relatively straightforward to do.

The dining philosophers is a toy example; nevertheless it highlights many of the issues found in regular parallel applications where there is some shared resource.

##Asynchronous Communication[asynchronous-communication]

We stated at the beginning that 'synchronous communication was more basic' than asynchronous communication. Nevertheless, there are times when asynchronous communication is called for.

>In general, asynchrony increases the potential for parallelism; it also increases the complexity of coordination.

One pattern that makes inherent use of asynchronous communication is the worker-queue pattern. The worker-queue pattern consists of a source of 'work', a queue to hold unfinished work items and one or more 'workers' that perform a typically compute intensive task on each item. For example, an image rendering farm might consist of an image generator, a queue and a set of image renderers. (Image rendering, such as resizing an image encoded in JPEG or converting from one form to another, often takes considerable compute resources.)

>Our focus here will be on the implementation of the queue, rather than the implementation of any image rendering process.

Other patterns that make use of asynchronous communications include the _publish-subscribe_ and the _actor_ patterns. In applications where there is essentially a one-way flow of information from publishers to subscribers there is less need to tightly synchronize communications. Similarly, actors denote semi-autonomous activities that collaborate with each other to solve shared _goals_. In fact, almost _every_ multi-user application can be modeled in terms of agents.

###A Message Queue[a-message-queue]

We start by defining a type that denotes the interface to the queue. For convenience, we use a _type alias_ to a record with a `post` function and a `poll` function within it:

```
type messageQ of a is alias of {
  post:(a)=>task of ()
  poll:()=>task of a
}
```

In effect, the record type is a specification of an API for accessing a message queue.

>One might ask why do we use `task` oriented types like `task of ()` and `task of a` types? I.e., why wrap the type of the message in a `task` type?

The `messageQ` is likely to be used in the context of `task` expressions; and certainly its implementation requires tasks. By exposing this task-nature we actually improve the potential responsiveness of the message queue and of functions that use it.

Like the semaphore that we saw earlier, we will build the message queue using an internal background task that manages the actual queue of messages.

```
msgQ:for all a ~~
  ()=>messageQ of a
fun msgQ() is let{
  def postMsgChnl is channel()
  def grabMsgChnl is channel()

  fun qLoop(Q) where isEmpty(Q) is wait for postM(Q)
   |  qLoop(Q) default is wait for postM(Q) or pollM(Q)

  fun postM(Q) is
    wrapRv(recvRv(postMsgChnl), (A) => qLoop([Q..,A]))

  fun pollM([A,..Q]) is let{
    fun reply(R) is valof{
      perform send(R,A)    -- reply on the one-time channel
      valis qLoop(Q)
    }
  } in wrapRv(recvRv(grabMsgChnl), reply)

  { ignore background qLoop(queue of []) }
} in {
  fun post(A) is wait for sendRv(postMsgChnl,A)
  fun poll() is task{
    var ReplyChnl is channel()
    perform wait for sendRv(grabMsgChnl,ReplyChnl)
    valis valof (wait for recvRv(ReplyChnl))
  }
}
```

This code is quite complex in places. However, if we break it down and examine it piece by piece its secrets will be exposed. Let us start with the `qLoop`/`postM`/`pollM` triumvirate. These three functions form the heart of the message queue's mechanism.

The `qLoop` function has a similar structure to the `semLoop` function we saw in the semaphore. This time, instead of deciding what branch to take based on a resource counter, we decide based on the size of the actual queue of data.

>The `queue` data type is a builtin type to **Star**. It implements a queue functionality allowing elements to be posted at one end and removed from the other. Like other collection types, the `sequence` contract is implemented for it; which means that we can use sequence notation when writing queue expressions and patterns.

The `postM` function — like its cousin the `releaseM` function in the semaphore — is a rendezvous-valued function that the `qLoop` function will `wait for`. It's inner core is a recursive call to `qLoop` with a `queue` that is enhanced with the new work item.

The `pollM` function has a similar structure; but it is more complex because — in addition to setting up the next `qLoop` recursion — we also want to export the entry from the queue itself.

Polling from the message queue is accomplished by sending a special one-time channel to the message Q task. Somewhat paradoxically, the `poll` function asks for the next element of the message queue by sending the one-time channel to the message queue's `grabMsgChnl`:

```
fun poll() is task{
  def ReplyChnl is channel()
  perform wait for sendRv(grabMsgChnl,ReplyChnl)
  valis valof (wait for recvRv(ReplyChnl))
}
```

The first two actions in the `task` expression of the `poll` function are fairly normal: we create a channel, and send that on the `grabMsgChnl` channel. The third line bears some more examination: what exactly is that `valis valof` doing?

The simple answer here is that this is a `task`-valued function, and the value of the `wait for` expression is also a `task` — it happens to be the task that we want to return. But, normally, a task expression that performs a `valis E` action would have type

`task of` E~t~

so, apparently, we take the value of the `task` returned by the `wait for` and rewrap it back as a `task`!

In fact, this particular combination is a part of the overall task notation: instead of performing this unwrap and rewrap, the value returned by the `wait for` will be returned directly _without necessarily waiting for the task to finish_. We can do this safely because the overall value of the sequence is also a task.

>There is a further complication here. Recall that the task notation is based on monads. There are potentially many kinds of monads and the `valis valof` combination allows us to _change monads_. This, however, is a topic beyond the scope of this book.

We can use our message queue to knock up a quick simulation of a system of workers and a work queue:

```
worksheet{
  ...
  fun sender(Q) is task{
    for i in range(0,1000,1) do
      perform Q.post(i)
    sleep(3000L)
    logMsg(info,"all done")
  }

  fun worker(W,Q) is task{
    while true do{
      def Nx is valof Q.poll()
      logMsg(info,"$W doing $Nx")
      sleep(random(100L))
    }
  }

  def MQ is msgQ()
  ignore background worker("alpha",MQ)
  ignore background worker("beta",MQ)

  ignore valof sender(MQ)
}
```

This example spins up 1000 work items and expects the `"alpha"` and `"beta"` workers to process them. Typical bosses attitude!

##Comment[comment]

The concurrency features of task expressions and rendezvous avoid the synchronization problems that plague concurrent and parallel programming in traditional languages. Moreover, tasks and rendezvous scale easily to hundreds of thousands of threads.

However, it should also be clear that they are also a little low-level: in order to program something like a semaphore or a message queue requires fairly careful attention to detail.

The true merit of **Star**'s concurrency features is that they do allow these higher level constructions to be programmed in a way that does not require extreme effort.

Indeed, as we shall see in [agent programming][chattering-agents] it is possible to implement a system of parallel agents with extremely minor tweaks to the regular sequential versions of the program.
