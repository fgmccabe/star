= Functional Programming

There is a perception of functional programming that it is
_weird_ and _difficult_. This is unfortunate for a number of
reasons; the most important being that functional programming is
_not_ weirder than procedural programming and that all
programmers can benefit by programming functionally.

As for being difficult, a more accurate description would be that
there is a deeper _range of features_ in functional programming
than in most modern programming languages: so a perception of
complexity can arise simply because there is a lot more to say about
functional programming languages. However, the simplest aspects of
functional programming are very simple and the ramp need not be steep.

What may be surprising to the reader who is not familiar with
functional programming is that it is _old_: predating the origins
of modern computing itself, that there is a huge amount that can be
expressed functionally, and that functional programming is often at
least as efficient and sometimes more efficient than procedural
programming.

Functional programming is also _new_ in a way that may be
surprising. As a paradigm, functional programming encourages a
different approach to programming. In particular, for many functional
programmers, the focus is on _data transformation_ rather than
_making recipies_.

In recent years some extremeley impressive advances have been made on
_how to program_; in particular the use of monads and related
concepts represent quite advanced programming
techniques.footnote:[While we will explore monads, we will likely not
touch on other more advanced topics in this book.]

This leads to an emphasis on _composition_ and _generality_
-- often over optimization of individual elements. We trust the
compiler's optimization strategies to pull the rabbit out of the hat
when it comes to performance.

In this chapter we will show how we can utilize Star as a vehicle
for functional programming. As a side-goal, we also hope to demystify
some of the language and ideas found in functional programming.

== What is functional programming?

The foundations of functional programming rest on two principles:

. Programs are expressed in terms of functions, where a somewhat
mathematical view of functions is taken; i.e., functions always
produce the same output for the same input.
+
This is what people mean when they say that functional programs are
_declarative_.footnote:[The term `declarative' has a technical
definition; but this captures much of the essence of declarativeness
(sic).]
. Functions are values: they can be passed as arguments to functions,
returned from functions, and they can be put into and retrieved from
data structures.

This last principle is what people mean when they refer to functions
as being _first class_ values. It is also what is meant when we
say that functional programming languages are _higher order_.

It is worth asking why these two principles are so important. Although
most early programmers were mathematicians; the extreme constraints
imposed by early machines meant those early programs were far from
functional in style.

However, almost by accident, programming and mathematics share some
attributes: the importance of _composition_ and the power of
_abstraction_. Functional programming is almost ideally placed to
exploit both of these to the full.

Star is not actually a _pure_ functional programming language;
i.e., it is possible to write programs that violate the declarative
principle. However, it is a _functional-first_ programming language: a
declarative style is strongly encouraged. In this book we shall mostly
focus on writing pure functional programs.

== Basics

It is often easier to introduce functional programming using numerical
examples. Last chapter we saw, for example, the factorial
program. This is mostly because most programmers are already familiar
with numbers. Continuing that tradition, here is a function that
returns the sign of a number:

[source,star]
----
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) where X>0 => 1.
----

Each of these equations applies to different situations: the first
equation applies when the input argument is negative, the second when
it is exactly zero and the third when it is strictly positive. These
represent the three possible cases in the definition of the sign
function.

A function may be built from any number of rewrite equations; however,
they must all be contiguous within the same group of statements.

Although it is good practice to ensure that equations in a function
definition do not overlap, the equations in a function definition are
tried in the order they are written in.footnote:[The _Church
Rosser Theorem_ guarantees some independence on the order of the
rewrite equations provided that the different rewrite equations that
make up function definitions do not overlap. Usually, however, it is
too fussy to _require_ programmers to ensure that their equations
do not overlap; hence the reliance on ordering of equations.] We could
have relied on this and written `sign` using:

[source,star]
----
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) => 1.
----

There is something somewhat unsatisfying about that third equation --
it is not actually always true -- and it depends on the previous
equations not applying for its meaning. We address this by using the
concept of a _default_ case: i.e., an equation that should be
used if none of the other cases apply:

[source,star]
----
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) default => 1.
----
An explicitly marked `default` equation does not need to be the last
equation; but, wherever it is written, default equations are only
attempted after all other equations have failed to apply.

=== Functions
A function is defined as a sequence of _rewrite equations_ each
of which consist of a _pattern_ and an _expression_. There
are three general forms of rewrite equations:

[source,star,subs="quotes"]
----
_Pattern_ => _Expression_
----

or
[source,star,subs="quotes"]
----
_Pattern_ where _Condition_ => _Expression_
----

or
[source,star,subs="quotes"]
----
_Pattern_ default => _Expression_
----

The left hand side of a rewrite equation consists of a pattern which
determines the applicability of the equation; and the right hand side
represents the value of the function if the pattern matches.

.Pattern
[aside]
****
A pattern represents a test or guard on an (implicit) value. Patterns
can be said to succeed or fail depending on whether the value being
tested matches the pattern.
****

We also refer to a pattern being _satisfied_ when matching a
value.footnote:[This terminology originates from Logic -- where a
formula can be satisfied (made true) by observations from the world.]

The pattern in a rewrite equation is a guard on the arguments of the
function call. For example, given a call
[source,star]
----
sign(34)
----
the patterns in the different equations of the ``sign`` function
will be applied to the integer value ``34``.

When the pattern on the left hand side of a rewrite equation succeeds
then the equation _fires_ and the value of the expression on the
right hand side of the equation becomes the value of the function.

There are many kinds of pattern; here, we look at three of the most
common kinds of pattern, and in later sections, we look at additional
forms of patterns.

.Variable Pattern
****
A variable pattern is denoted by an identifier; specifically by the
first occurrence of an identifier.
****

A variable pattern always succeeds and has the additional effect of
_binding_ the variable to the value being matched.

For example, the ``X`` in the left hand side of
[source,star]
----
double(X) => X+X
----
is a variable pattern. Binding ``X`` means that it is available for
use in the right hand side of the equation -- here to participate in
the expression ``X+X``.

The part of the program that a variable has value is called its _scope_.

* Variables in rewrite equations always have scope ranging from the
initial occurrence of the variable through to the whole of the right
hand side of the equation.
* Variable patterns are the _only_ way that a variable can get a
value.

Subsequent occurrences of variables in a pattern are semantically
equivalent to an equality test; specifically a call to the ``==``
predicate. For example, in the equation:

[source,star]
----
same(X,X) => true.
----
the second occurrence of ``X`` is regarded as an equality test;
i.e., this equation is equivalent to:

[source,star]
----
same(X,X1 where X==X1) => true.
----

Sometimes, the earlier occurrence of a variable is not in the pattern
itself but in an outer scope.

._Literal Pattern_
****
A literal pattern -- such as a numeric literal or a string literal --
only matches the identical number or string.
****

Clearly, a literal match amounts to a comparison of two values: the
pattern match succeeds if they are identical and fails otherwise.

Equality is based on _semantic equality_ rather than
_reference equality_. What this means, for example, is that two
strings are equal if they have the same sequence of characters in
them, not just if they are the same object in memory.

NOTE: not all types admit equality: for example, functions are not
comparable; similarly, implementing equality for circular structures
is problematic.footnote:[None of the standard data types are circular
in nature.]

There is no automatic coercion of values to see if they _might_
match. In particular, an integer pattern will only match an integer
value and will not match a float value -- even if the numerical values
are the same. I.e., there will be no attempt made to coerce either the
pattern or the value to fit.

This, too, is based on the desire to avoid hard-to-detect bugs from
leaking into a program.

._Guard Pattern_
****
Sometimes known as a _semantic guard_, a guard pattern consists
of a pair of a pattern and a condition:

[source,star,subs="quotes"]
----
_Pattern_ where _Condition_
----
****

A guard succeeds if both its pattern matches and if its condition is
_satisfied_.

Note that conditions _may_ bind variables. This is why we do not state
that conditions are boolean-valued expressions.

There is a normal complement of special conditional forms which we
shall explore as we encounter the need. In the case of the equation:
[source,star]
----
sign(X) where X>0
----
the guard pattern is equivalent to:
[source,star]
----
X where X>0
----
We can put guard pattern anywhere that a pattern is valid; and, for
convenience, we can also put them immediately to the left of the
rewrite equation's ``=>`` operator.

[sidebar]
Any variables that are bound by the pattern part of a guarded pattern
are _in scope_ in the condition part of the guard. Furthermore,
any variables that are bound by the condition part of the guarded
pattern have the same scope as variables introduced in the pattern.

In the pattern above, the variable ``X`` will be bound in the
variable pattern ``X`` and will then be tested by evaluating the
condition ``X>0``.

=== Order of evaluation
Star is a so-called _strict_ language. What that means is that
arguments to functions are evaluated prior to calling the function.

We do not specify the order of evaluation of the arguments of the
function; except that:

* Arguments are evaluated before entry to the function.
* The function is evaluated before entry to the function.

Most programming languages are strict; for two main reasons:

. It is significantly easier for programmers to predict the evaluation
characteristics of a strict language.
. It is also easier to implement a strict language efficiently on modern
hardware. Suffice it to say that modern hardware was designed for
evaluating strict languages, so this argument is somewhat circular.

There many possible styles of evaluation order; one of the great
merits of programming declaratively is that the order of evaluation
does not affect the actual results of the computation.

It may, however, affect whether you get a result. Different strategies
for evaluating expressions can easily lead to differences in which
programs terminate and which do not.

One other kind of evaluation that is often considered is _lazy_
evaluation. Lazy evaluation means simply that expressions are only
evaluated _when needed_. Lazy evaluation has many potential
benefits: it certainly enables some very elegant programming
techniques.

[aside]
The modern name for lazy evaluation in functional programming
languages is _normal order evaluation_. This evaluation order
amounts to evaluating all expressions in a strictly left-to-right
order. Surprisingly, this amounts to a form of lazy evaluation because
the function is evaluated before its arguments.

Essentially for the reasons noted above, Star does not use lazy
evaluation; however, as we shall see, there are features that allow us
to recover some of the power of lazy evaluation.footnote:[Even
predominantly lazy languages like Haskell have features which enforce
strict evaluation. It reduces to a question of which is the
_default_ evaluation style.]

The other dimension in evaluation order relates to the rewrite
equations used to define functions. Here, Star uses an in-order
evaluation strategy: the equations that make up the definition of a
function are tried in the order that they are written -- with the one
exception being any default equation which is always tried last.

== Another look at types
Organizing data is fundamental to any programming language. Star's
data types are organized around the algebraic data type.

=== Quantifier types

A _generic_ type is one which has one or more type variables in
it. For example, the type expression:
[source,star]
----
(x,x)=>boolean
----
is such a generic type (assuming that ``x`` is a type variable --
see below).

All type variables must be bound by a quantifier in some enclosing
scope. If a type variable is not bound within a particular type
expression, it is considered _free_ in that type expression.

A _quantified type_ is a type that introduces (i.e., binds) a
type variable. There are two quantifiers: a universal quantifier and
an existential quantifier.

The most common quantifier is the _universal quantifier_ and
universally quantified types correspond closely to generic types in
other languages.

Universally quantified types are often used to denote function types
and collection types. For example, the type
[source,star]
----
all x ~~ (x,x)=>boolean
----
denotes the type of a generic binary function that returns a boolean
value. The standard type for the equality predicate ``==`` is
similar to this type.

A universally quantified type should be read as `for all possible
values' of the bound variable. For example, this function type should
be read as denoting functions that:

[aside]
for any possible type -- ``x`` -- the function takes two such
``x``'s and returns a ``boolean``.

Star also supports _existentially_ quantified types -- these
are useful for denoting the types of modules and/or abstract data
types. For example, the type expression:

[source,star]
----
exists e ~~ { f1 : e }
----
denotes the type of a record with a field -- ``f1`` -- which has a
type. The analogous reading for this type expression would be:

[aside]
there is a type -- ``x`` -- such that the record has a field --
called ``f1`` -- of this type.

As may be guessed, this is kind of obscure: ``f1`` has a type, but
we don't know much else about it!

However, existential types can be very useful, especially in
combination with record types like this one. We will leave our more
detailed exploration of existential types for later.

=== Contract constrained types
We noted above that the type of the standard equality predicate was
almost the same as:
[source,star]
----
all x ~~ (x,x)=>boolean
----
This type denotes a universally quantified function type that can be
applied to arguments of any given type. However, equality in a normal
programming language is _not_ universal: not all values admit to
being reliably tested for equality. A great example of such a
limitation are functions -- equality between functions is not well
defined.footnote:[Strictly, equality of functions is not decidable.]

To capture this, we need to be able to constrain the scope of the
quantifier; specifically to those argument types that do admit
equality.

We can do this by adding a _contract constraint_ to the type --
the constraint states that equality must be defined for the type. We
do this by prepending an ``equality`` constraint clause to the
type:
[source,star]
----
all x ~~ equality[x] |: (x,x)=>boolean
----
The ``equality[x] |:`` clause states that the type variable
``x`` must satisfy the ``equality`` contract.

What is implied when we state this? This is captured in the definition
of the contract itself, in this case:
[source,star]
----
contract all t ~~ equality[t] ::= {
  (==) : (t,t) => boolean.
}
----
In effect, the ``equality`` contract states that there must be a
single function defined -- ``==`` -- that is defined for any type that
claims to satisfy the ``equality`` contract.

If this seems a little circular, it is not. The ``equality``
contract is effectively saying that the ``==`` function must be
defined for the type; and we constrain function (and other) types with
the ``equality`` contract constraint when we need to ensure that
``==`` is defined.

We provide evidence for contracts through the ``implementation``
statement. This declares that a given type satisfies a contract by
providing implementations for the functions in the contract.

For example, we can provide evidence that the ``equality`` contract
applies to strings using a built-in primitive to actually implement
equality for strings:
[source,star]
----
implementation equality[string] => {
  s1 == s2 => _string_eq(s1,s2).
}
----
We shall explore more fully the power of this form of type constraint
in later sections and chapters. For now, the core concept is that
quantified types can be constrained to allow very precise formulations
of types.

=== Algebraic data types

An algebraic data type definition achieves several things
simultaneously: it introduces a new type into scope, it gives an
enumeration of the legal values of the new type and it defines both
constructors for the values and it defines patterns for decomposing
values. This is a lot for a single statement to do!

For example, we can define a type that denotes a point in a
two-dimensional space:
[source,star]
----
point ::= cart(float,float).
----
This kind of statement is called a _type definition statement_
and is legal in the same places that a function definition is legal.

The new type that is named by this statement is point; so, a variable
may have point type, we can pass point values in functions and so on.

The constructor cart allows us to have expression that allow new
point structures to be made:
[source,star]
----
cart(3.4,2.1)
----
``cart`` is also the name of a _pattern_ operator that we can
use to take apart point values. For example, the euclid function
computes the Euclidian distance associated with a point:
[source,star]
----
euclid:(point)=>float.
euclid(cart(X,Y)) => sqrt(X*X+Y*Y).
----
Of course, this particular ``point`` type is based on the
assumption that point values are represented in a cartesian coordinate
system. One of the more powerful aspects of algebraic data types is
that it is easy to introduce multiple alternate forms of data. For
example, we might want to support two forms of point: in cartesian
coordinates and in polar coordinates. We can do this by introducing
another case in the type definition statement:
[source,star]
----
point ::= cart(float,float)
        | polar(float,float).
----
Of course, our ``euclid`` function also needs updating with the new
case:
[source,star]
----
euclid:(point)=>float.
euclid(cart(X,Y)) => sqrt(X*X+Y*Y).
euclid(polar(R,T)) => R.
----
``cart`` and ``polar`` are called _constructor
functions_. The term _constructor_ refers to the common
programming concept of constructing data structures. They are called
functions because, logically, they _are_ functions.

For example, we might give a type to ``polar``:
[source,star]
----
polar : (float,float)=>point
----
In fact, constructor functions are one-to-one functions. Variously
known as _free functions_ (in logic), _bijections_ (in
Math), one-to-one functions are guaranteed to have an inverse. This is
the logical property that makes constructor functions useful for
representing data.

Of course, we are talking of a _logical_ property of constructor
functions. Internally, when implementing functional languages, the
values returned by constructor functions are represented using data
laid out in memory -- just like in any other programming language.

Constructor functions have a special type; so the correct type of
``polar`` is given by:
[source,star]
----
polar : (float,float)<=>point
----
The double arrow representing the fact that constructor functions are
bijections.

In addition to constructor functions, an algebraic type definition can
introduce two other forms of data: _enumerated symbols_ and
_record functions_. Enumerated symbols are quite useful in
representing symbolic alternatives. The classic example of an
enumerated type is ``daysOfWeek``:
[source,star]
----
daysOfWeek ::= .monday
             | .tuesday
             | .wednesday
             | .thursday
             | .friday
             | .saturday
             | .sunday.
----
Another example is the standard ``boolean`` type which is defined:
[source,star]
----
boolean ::= .true | .false.
----
Unlike enumerated symbols in some languages, there is no numeric value
associated with an enumeration symbol: an enumerated symbol `stands
for' itself only. The reason for this will become clear in our next
type definition which mixes enumerated symbols with constructor
functions:
[source,star]
----
sTree ::= .sEmpty | .sNode(sTree,string,sTree)
----
In addition to mixing the enumerated symbol (``.sEmpty``) with the
``.sNode`` constructor, this type is _recursive_: in fact, this
is a classic binary tree type where the labels of the non-empty nodes
are strings. (We shall see shortly how to generalize this).

Whenever you have a recursive type, its definition must always include
one or more cases that are not recursive and which can form the base
case(s). In that sense, an enumerated symbol like ``.sEmpty`` plays
a similar role that null does in other languages; except that
``.sEmpty`` is only associated with the sTree type.

We can use ``sTree`` to construct binary trees of string value; for
example:
[source,star]
----
.sNode(.sNode(.sEmpty,"alpha",.sEmpty),
      "beta",
      .sNode(.sEmpty,"gamma",.sEmpty))
----
denotes the tree in:

.A Binary string Tree
image::images/stree.png[pdfwidth="50%"]

[aside]
One of the hallmarks of languages like Star is that _every_
value has a legal syntax: it is possible to construct an expression
that denotes a literal value of any type.

Just as we can define ``sTree`` values, so we can also define
functions over ``sTree``s. For example, the ``check`` function
returns ``true`` if a given tree contains a particular search term:
[source,star]
----
check(.sEmpty,_) => .false.
check(sNode(L,Lb,R),S) => Lb==S || check(L,S) || check(R,S).
----
Here we see several new aspects of Star syntax:

* An empty pattern -- marked by ``_`` -- matches anything. It is
called the _anonymous pattern_ and is used whenever we don't care
about the actual content of the data.
* The ``||`` disjunction is a _short-circuit_ disjunction; much
like ``||`` in languages like Java. Similarly, conjunction
(``&&``) is also short-circuiting.
* Functions can be recursive; including _mutually recursive_: there
is no special requirement to order function definitions in a program.

We can use ``sTest`` to check for the occurrence of particular
strings:
[source,star]
----
T .= sNode(sNode(.sEmpty,"alpha",.sEmpty),
          "beta",
          sNode(.sEmpty,"gamma",.sEmpty));

show check(T,"alpha");          -- results in true
show check(T,"delta");          -- results in false
----

== Functions as values
The second principle of functional programming is that functions are
first class. What that means is that we can have functions that are
bound to variables, passed into functions and returned as the values
of functions. In effect, a function is a legal _expression_ in
the language. It also means that we can have _function types_ in
addition to having types about data.

We can see this best by looking at a few examples. One of the benefits
of passing functions as arguments to other functions is that it makes
certain kinds of parameterization easy. For example, suppose that you
wanted to generalize ``check`` to apply an arbitrary test to each
node -- rather than just looking for a particular string.

We will first of all define our ``fTest`` function itself:
[source,star]
----
fTest:(sTree,(string)=>boolean)=>boolean.
fTest(.sEmpty,_) => .false.
fTest(sNode(L,Lb,R),F) => F(Lb) || fTest(L,F) || fTest(R,F).
----
The substantial change here is that, rather than passing a string to
look for, we pass ``fTest`` a boolean-valued function to apply;
within ``fTest`` we replace the equality test ``Lb==S`` with a
call ``F(Lb)``.

Notice that the type annotation for ``fTest`` shows that the type
of the second argument is a function type -- from ``string`` to
``boolean``.

Given ``fTest``, we can redefine our earlier ``check`` function
with:
[source,star]
----
check(T,S) => fTest(T,(X)=>X==S).
----
We have a new form of expression here: the _anonymous function_
or _lambda expression_. The expression
[source,star]
----
(X)=>X==S
----
denotes a function of one argument, which returns ``true`` if its
argument is the same value as ``S``.

Interestingly, it would be difficult to define a top-level function
that is equivalent to this lambda because of the occurrence of the
variable ``S`` in the body of the lambda. This is an example of a
_free variable_: a variable that is mentioned in the body of a
function but which is defined outside the function. Because ``S``
is free, because it is not mentioned in the arguments, one cannot
simply have a function which is equivalent to the lambda as a
top-level function.

Free variables are a powerful feature of functional programming
languages because they have an _encapsulating_ effect: in this
case the lambda encapsulates the free variable so that the
``fTest`` function does not need knowledge of ``S``.

=== Functions and closures
If a function is an expression, what is the value of the function
expression? The conventional name for this value is _closure_:

Closure:: A closure is a structure that is the value of a function expression
and which may be applied to arguments.

It is important to note that, as a programmer, you will never `see' a
closure in your program. It is an implementation artifact in the same
way that the representation of floating point numbers is an
implementation artifact that allow computers to represent fractional
numbers but which programmers (almost) never see explicitly in
programs.

Pragmatically, one of the important roles of closures is to capture
any free variables that occur in the function. Most functional
programming languages implement functions using closure
structures. Most functional programming languages do not permit direct
manipulation of the closure structure: the only thing that you can
_do_ with a closure structure is to use it as a function.

[aside]
In the world of programming languages, there is a lot of confusion
about closures. Sometimes you will see a closure referring to a
function that captures one or more free variables.

=== Let binding

We noted that it is difficult to achieve the effect of the ``(X)=>X==S`` lambda
expression with named functions. The reason is that the lambda is _not_ defined
in the same way that named functions are defined -- because it occurs as an
expression not as a statement. If we wanted to define a named function which
also captures ``S``, we would have to be able to define functions inside
expressions.

There is an expression that allows us to do this: the ``let`` expression. A
``let`` expression allows us to introduce local definitions anywhere within an
expression. We can define our lambda as the named function ``isS`` using the
``let`` expression:

[source,star]
----
let{
  isS(X) => X==S
} in isS
----
The region between the braces is a _definition environment_ and
_any_ definition statement may be in such an environment. We can
define check using a ``let`` expression:
[source,star]
----
check(T,S) => fTest(T,
    let{
      isS(X) => X==S.
    } in isS)
----
This is a somewhat long-winded way of achieving what we did with the
anonymous lambda function -- we would not normally recommend this way
of writing the ``check`` function as it is significantly more
complicated than our earlier version. However, there is a strong
inter-relationship between anonymous lambdas, let expressions and
variable definitions. In particular, these are equivalent:
[source,star]
----
let{
  isS = (X) => X==S
} in isS
----
and
[source,star]
----
(X)=>X==S
----
Apart from being long-winded, the ``let`` expression is
significantly more flexible than a simple lambda. It is much easier
within a ``let`` expression to define functions with more than one
rewrite equation; or to define multiple functions. We can even define
types within the let binding environment.

Conversely, lambda functions are so compact because they have strong
limitations: you cannot easily define a multi-rewrite equation
function with a lambda and you cannot easily define a recursive
function as a lambda.

In short, we would use a ``let`` expression when the function being
defined is at all complex; and we would use a lambda when the function
being defined is simple and small.

==== Recursive Let Binding

Assembling functions in this way, either by using anonymous lambdas or
by using ``let`` expressions, is one of the hallmarks of functional
programming.

Star actually has _two_ forms of let expressions -- the one we
have used so far does not allow function definitions to be recursive -- and
a potentially mutually recursive form.

Sometimes, it is useful to have let expressions which allow recursive
definitions within them. For that we use a special form of
brace notation:
[source,star]
----
let{.
  f(X) => g(X).
  g(X) => f(X).
.} in f
----

The extra periods signal that the definitions within the _let
environment_ may refer to each other. This form of let expression also
allows type definitions and other forms of definition.

For the most part, where the _let environment_ actually contains
non-recursive definitions, either form may be used. However, there are
situations -- especially when it comes to implementing contracts --
where it is important that the contained definitions cannot refer to
themselves.

=== Generic types

What actually makes ``fTest`` above more constrained than it could
be is the type definition of ``sTree`` itself. It too is
unnecessarily restrictive: why not allow trees of any type? We can,
using the type definition for ``tree``:
[source,star]
----
all t ~~ tree[t] ::= .tEmpty | .tNode(tree[t],t,tree[t]).
----
Like the original ``sTree`` type definition, this statement
introduces a new type: ``tree[t]`` which can be read as _tree
of something_. The name ``tree`` is not actually a type identifier
-- although we often refer to the tree type -- but it is a _type
constructor_.

In an analogous fashion to constructor functions, a type constructor
constructs types from other types. Type constructors are even
bijections -- one-to-one functions from types to types.

[aside]
Unlike constructor functions though, type functions play no part in
the run-time evaluation of programs.

The identifier ``t`` in the type definition for ``tree`` denotes
a _type variable_. Again, similarly to regular variables and
parameters, a type variable denotes a single unspecified type. The
role of the type variable ``t`` is like a parameter in a function:
it identifies the unknown type and its role.

The ``tree`` type is a universally quantified type. What that means
is that instead of defining a single type it defines a family of
related types: for example:
[source,star]
----
tree[string]
tree[integer]
...
----

are ``tree`` types. We can even have trees of trees:
[source,star]
----
tree[tree[string]]
----
We capture this genericity of the tree type by using a _universal
quantifier_:
[source,star]
----
all t ~~ tree[t]
----
What this type expression denotes is a set of possible types: for any
type ``t``, ``tree[t]`` is also a type. There are infinitely
many such types of course.

The ``all`` quantifier is important: as in logic, there are two
kinds of quantifiers: the _universal_ quantifier all and the
_existential_ quantifier exists.

The types of the two constructors introduced in the ``tree`` type
definition are similarly quantified:
[source,star]
----
tEmpty:all t ~~ ()<=>tree[t].

tNode:all t ~~ (tree[t],t,tree[t)]<=>tree[t].
----
The type ``tree[t]`` on the right hand side of ``.tEmpty``'s type
annotation raises a couple of interesting points:

. This looks like a type annotation with no associated definition. The
fact that the ``.tEmpty`` symbol was originally introduced in a type
definition is enough of a signal for the compiler to avoid looking for
an implementation for the name.
. The type of ``tEmpty`` also takes the form of a constructor type --
using the ``<=>`` operator.
+
There is a subtle difference between the type of ``tEmpty`` vs
``.tEmpty``. The latter actually denotes the application of the
``tEmpty`` constructor -- as though it had been written ``tEmpty()``.
. The type of a literal ``.tEmpty`` expression -- assuming that no
further information is available -- will be of the form
``tree[t34]`` where ``t34`` is a `new' type variable not
occurring anywhere else in the program. In effect, the type of
``.tEmpty`` is tree of _some type_ ``t34`` where we don't
know anything more about ``t34``.

=== Generic functions
Given this definition of the tree type, we can construct a more
general form of the tree test function; which is almost identical to
fTest:footnote:[This time too, we must use an explicit type
annotation.]
[source,star]
----
test:all t ~~ (tree[t],(t)=>boolean)=>boolean.
test(.tEmpty,_) => .false.
test(.tNode(L,Lb,R),F) => F(Lb) || test(L,F) || test(R,F).
----
and our original string check function becomes:
[source,star]
----
check(T,S) => test(T,(X) => X==S)
----
The type of check is also more generic:
[source,star]
----
check:all t ~~ (tree[t],t)=>boolean
----
I.e., ``check`` can be used to find any type of element in a tree
-- providing that the types align of course.

Actually, this is not the correct the type for ``check``. This is
because we do not, in general, know that the type can support
equality. The precise type for ``check`` should take this into
account:
[source,star]
----
check:all t ~~ equality[t] |: (tree[t],t) => boolean
----

== Going further
Although better than the original ``sTest`` program there is still
one major sense in which the ``test`` program is not general enough. We
can see this by looking at another example: a function that counts
elements in the tree:
[source,star]
----
count:all t ~~ (tree[t]) => integer.
count(.tEmpty) => 0.
count(.tNode(L,_,R)) => count(L)+count(R)+1
----
This code is very similar, but not identical, to the ``test``
function.

The issue is that ``test`` is trying to do two things
simultaneously: in order to apply its test predicate to a binary tree
it has to implement a walk over the tree, and it also encodes the fact
that the function we are computing over the tree is a boolean-value
function.

We often need to do all kinds of things to our data structures and
writing this kind of recursion over and over again is tedious and
error prone. What we would like to do is to write a single
_visitor_ function and specialize it appropriately when we want
to perform a specific function.

This principle of separating out the different aspects of a system is
one of the core foundations of good software engineering. It usually
goes under the label _separation of concerns_. One of the
beautiful things about functional programming is that it directly
supports such good architectural practices.

Since this visitor may be asked to perform any kind of computation on
the labels in the tree we will need to slightly generalize the type of
function that is passed to the visitor. Specifically, the type of
function should look like:
[source,star]
----
F : (t,a)=>a
----
where the ``a`` input represents accumulated state, ``t``
represents an element of the tree and the result is another
accumulation.

Using this, we can write a ``tVisit`` function that implements tree
walking as:
[source,star]
----
tVisit:all a,t ~~ (tree[t],(t,a)=>a,a)=>a.
tVisit(.tEmpty,_,A) => A.
tVisit(.tNode(L,Lb,R),F,A) => tVisit(R,F,F(Lb,tVisit(L,F,A))).
----
Just as the accumulating function acquires a new `state' parameter, so
the ``tVisit`` function also does. The ``A`` parameter in the
two equations represents this accumulated state.

The second rewrite equation for ``tVisit`` is a bit dense so let us
open it out and look more closely. A more expanded way of writing the
``tVisit`` function would be:
[source,star]
----
tVisit(.tEmpty,_,A) => A.
tVisit(.tNode(L,Lb,R),F,A) => let{
    A1 = tVisit(L,F,A).
    A2 = F(Lb,A1).
  } in tVisit(R,F,A2)
----
where ``A1`` and ``A2`` are two local variables that represent
the result of visiting the left sub-tree and applying the accumulator
function respectively. We have used the let expression form to make
the program more obvious, rather than to introduce new functions
locally; but this is a legitimate role for let expressions.

The ``tVisit`` function knows almost nothing about the computation
being performed, all it knows about is how to walk the tree and it
knows to apply functions to labels in the tree.

Given ``tVisit``, we can implement our original ``check`` and
``count`` functions as one-liners:
[source,star]
----
check(T,S) => tVisit(T,(X,A)=>(A || X==S),.false).
count(T) => tVisit(T,(X,A)=>A+1,0).
----
The lambda that is embedded in the definition of check bears a little
closer scrutiny:
[source,star]
----
(X,A)=>(A || X==S)
----
In this lambda, we return ``A`` -- if it is ``true`` -- or we
return the result of the test ``X==S``. This is a common pattern in
such programs: the accumulator ``A`` acts as a kind of state
parameter that keeps track of whether we have already found the value.

Functional programs are not actually _state-free_; often quite
the opposite. However, the state in a functional program is never
_hidden_. This is the true distinction between functional and
regular procedural programs.

Notice that we have effectively hidden the recursion in the function
definitions of ``check`` and ``count`` -- all the recursion is
encapsulated within the ``tVisit`` function.

[aside]
One of the unofficial mantras of functional programming is _hide
the recursion_.

The reason we want to hide recursions that this allows the designer of
functions to focus on _what_ is being computed rather than
focusing on the structure of the data and, furthermore, this allows
the implementation of the visitor to be _shared_ by all users of
the tree type.

Notice that, while ``a`` and ``t`` are type variables, we did
not put an explicit quantifier on the type of ``F``. This is
because the quantifier is actually put on the type of ``tVisit``
instead:
[source,star]
----
tVisit:all a,t ~~ (tree[t],(t,a)=>a,a)=>a
----
Just like regular variables, type variables have scope and points of
introduction. Also like regular variables, a type variable may be
_free_ in a given type expression; although it must ultimately be
_bound_ by a quantifier.

[#goingEvenFurther]
=== Going even further
We have focused so far on generalizing the visitor from the
perspective of the tree type. But there is another sense in which we
are still _architecturally entangled_: from the perspective of
the ``check`` and ``count`` functions themselves.

In short, they are both tied to our ``tree`` type. However, there
are many possible collection data types; Star for instance has some
5 or 6 different standard collection types. We would prefer not to
have to re-implement the ``check`` and ``count`` functions for
each type.

The good news is that, using contracts, we can write a single
definition of ``check`` and ``count`` that will work for a range
of collection types.

Let us start by defining a contract that encapsulates what it means to
visit a collection:
[source,star]
----
contract all c,t ~~ visitor[c->>t] ::= {
  visit:all a ~~ (c,(t,a)=>a,a)=>a
}
----
This ``visitor`` contract defines a single function that embodies
what it means to _visit_ a collection structure. There are quite
a few pieces here, and it is worth examining them carefully.

A contract header has a template that defines a form of _contract
constraint_. The clause
[source,star]
----
visitor[c ->> t]
----
is such a constraint. The sub-clause
[source,star]
----
c ->> t
----
refers to two types: ``c`` and ``t``. The presence of the
``->>`` term identifies the fact that ``t`` _depends_ on
``c``.

The ``visitor`` contract itself is about the collection type
``c``. But, within the contract, we need to refer to both the
collection type and to the type of elements in the collection: the
``visit`` function is over the collection, it applies a function to
elements of the collection.

Furthermore, as we design the contract, we _do not know_ the
exact relationship between the collection type and the element
type. For example, the collection type may be generic in one argument
type -- in which case the element type is likely that argument type;
conversely, if the type is _not_ generic (like ``string``
say), then we have no direct handle on the element type.

We _do know_ that within the contract the element type is
_functionally determined_ by the collection type: if you know the
collection type then you should be able to figure out the element
type.

We express this dependency relationship with the the ``c ->> t``
form: whatever type ``c`` is, ``t`` must be based on it.

The body of the contract contains a single type annotation:
[source,star]
----
visit:all a ~~(c,(t,a)=>a,a)=>a
----
This type annotation has three type variables: the types ``c`` and
``t`` come from the contract header and ``a`` is local to the
signature. What the signature means is

[aside]
Given the visitor contract, the ``visit`` function is from the
collection type ``c``, a function argument and an initial state and
returns a new accumulation state.

It is worth comparing the type of ``visit`` with the type of
``tVisit``:
[source,star]
----
tVisit:all t,a ~~(tree[t],(t,a)=>a,a)=>a
----
The most significant difference here is that in ``tVisit`` the type
of the first argument is fixed to ``tree[t]`` whereas in
``visit`` it is left simply as ``c`` (our collection type).

Given this contract, we can re-implement our two ``check`` and
``count`` functions even more succinctly:
[source,star]
----
check(T,S) => visit(T, (X,A)=>A || X==S,.false)
count(T) => visit(T, (X,A)=>A+1,0)
----
These functions will apply to _any_ type that satisfies -- or
implements -- the ``visitor`` contract. This is made visible in the
revised type signature for ``count``:
[source,star]
----
count:all c,t ~~ visitor[c->>t] |: (c)=>integer.
----
This type is an example of a _constrained type_. It is generic in
``c`` and ``t`` but that generality is constrained by the
requirement that the ``visitor`` contract is appropriately
implemented. The eagled-eyed reader will notice that ``count`` does
not actually depend on the type of the elements in the collection:
this is what we should expect since ``count`` does not actually
care about the elements themselves.

The type signature for ``check``, however, does care about the
types of the elements:
[source,star]
----
check:all c,t ~~
  visitor[c->>t], equality[c] |: (c,t)=>boolean
----
This type annotation now has two contract constraints associated with
it: the collection must be something that is visitable and the
elements of the collection must support ``equality``.

Given the work we have done, we can implement the ``visitor``
contract for our ``tree[t]`` type quite straightforwardly:
[source,star]
----
implementation all t ~~ visitor[tree[t]->>t] => {
  visit = tVisit
}
----
Notice that header of the implementation statement provides the
connection between the collection type (which is ``tree[t]``) with
the element type (``t``). The clause
[source,star]
----
visitor[tree[t]->>t]
----
is effectively a declaration of that connection.

Now that we have disconnected ``visit`` from ``tree`` types, we
can extend our program by implementing it for other types. In
particular, we could also implement the visitor for the ``sTree``
type:
[source,star]
----
implementation visitor[sTree ->> string] => {
  visit = sVisit
}
----
however, we leave the definition of ``sVisit`` as a simple exercise
for the reader.

Our final versions of ``count`` and ``check`` are now quite
general: they rely on a generic implementation of the ``visit``
function to hide the recursion and are effectively independent of the
actual collection types involved.

If we take a second look at our visitor contract we can see something
quite remarkable: it counts as a definition of the famous
_visitor pattern_. This is remarkable because, although visitor
patterns are a common design pattern in OO languages, it is often hard
in those languages to be crisp about the semantics of visiting; in
fact, they are called patterns because they represent patterns of use
which may be encoded in Java (say) whilst not necessarily being
definable in them.

The combination of contract and implementation represents a quite
formal way of defining patterns like the visitor pattern.

There is something else here that is quite important too: we are able
to define and implement the visitor contract _without_ having to
modify in any way the type definition of ``tree`` or
``sTree``. From a software engineering point of view this is quite
important: we are able to gain all the benefits of interfaces without
needing to entangle them with our types. This becomes critical in
situations where we are not able to modify types -- because they don't
belong to us and/or we don't have access to the source.

== Polymorphic arithmetic
There are other ways in which programs can be polymorphic. In
particular, let us focus for a while on arithmetic. One of the issues
in arithmetic functions is that there are many different kinds of
numbers. Pretty much every programming language distinguishes several
kinds of numbers; for example, Java distinguishes byte, short, int,
long, float, double, BigInteger and BigDecimal -- and this does not
count the wrapped versions. Other languages have even more choice.

One question that might seem relevant is why? The basic answer is that
different applications call for different properties of numbers and no
one numeric type seems to fit all needs. However, that variety comes
at a cost: when we use numbers we tend to have to make too early a
choice for the numeric type.

For example, consider the ``double`` function we saw earlier:
[source,star]
----
double(X) => X+X
----
What type should ``double`` have? In particular, what should the
type of ``+`` be? Most people would be reluctant to use different
arithmetic operators for different types of numbers.footnote:[Although
some languages -- such as SML -- do require this.] This is resolved by
relying on contracts for the arithmetic operations.

The result is that the most appropriate type signature for
``double`` is exquisitely tuned:
[source,star]
----
double:all t ~~ arith[t] |: (t)=>t
----
This type is precisely the most general type that ``double`` could
have. Any further constraints result in making a potentially premature
choice for the numeric type.

If we take another look at our original ``fact`` function:
[source,star]
----
fact(0) => 1
fact(N) => N*fact(N-1)
----
this is constrained to be a function from ``integer`` to
``integer`` because we introduced the literal integers ``0`` and
``1``. However, the ``arith`` contract contains synonyms for
these very common literals. Using ``zero`` and ``one`` allow us
to be abstract in many arithmetic functions:
[source,star]
----
genFact:all a ~~ arith[a] |: (a)=>a.
genFact(zero) => one.
genFact(N) => N*genFact(N-one).
----
We call out ``zero`` and one for special treatment because they
occur very frequently in numerical functions. We can introduce other
numeric literals without compromising our type by using
_coercion_; although it is more clumsy:
[source,star]
----
factorialC:all t ~~
  arith[t],coercion[integer,t] |: (t)=>t.
factorialC(N) where N==0::t => 1::t.
factorialC(N) => N*factorialC(N-1::t).
----
The expressions ``0::t`` and ``1::t`` are coercions from
``integer`` to ``t``.

Of course, coercion is also governed by contract, a fact represented
in the type signature by the coercion contract constraints on the type
of ``t``.

In any case, using these techniques, it is possible to write numeric
functions without unnecessarily committing to specific number
types. That in turn helps to make them more useful.

=== Optional computing
There are many situations where it is not possible to guarantee that a
computation will succeed. The simplest examples of this include
scenarios such as accessing external files; but may also apply to
getting the first element of a list or the label of a ``tree``
node. The great unknown of accessing elements of a collection is `is
it there?'. Its not guaranteed of course, and we need to be able to
handle failure.

Many languages employ the concept of a special ``null`` value to
denote some of these cases -- like a ``someOne`` not having a
``spouse``. However, the special ``null`` value brings its own
problems: the type of ``null`` is problematic (it is a legal value
for every type) and there are many situations where ``null`` is
never possible.

We address this by handling those situations where failure is possible
differently than where it is not. Specifically, we do this via the
``option`` type.

The type definition of ``option`` is quite straightforward:
[source,star]
----
all t ~~ option[t] ::= .none | some(t).
----
where ``none`` is intended to denote the non-existence of a value
and ``some`` denotes an actual value.

The ``option`` type is intended to be used in cases where functions
are known to be partial.footnote:[A partial function does not have a
value across the whole range of its arguments.] An ``option``
return type signals that the function may not always have a value.

Normal pattern matching can be used to access a value wrapped in a
``some``; for example, to access someone's ``spouse`` we can use
the condition:
[source,star]
----
isMarriedTo(P,J) where .some(JJ).=P.spouse => J==JJ.
isMarriedTo(_,_) default => .false.
----
The important detail here is that all access to a ``option``
wrapped value is gated by some form of pattern matching and that,
normally, this takes place in a condition.

[aside]
****
The condition
[source,star]
----
.some(JJ).=P.spouse
----
represents a pattern matching condition: it is satisfied if
``P.spouse`` matches ``.some(JJ)`` and has the additional effect
of defining and binding the variable ``JJ`` to the matched spouse.
****

=== Special syntax for ``option``al values
Of course, the code above _is_ kind of clumsy. There is a range
of operators to make using ``option`` values more pleasant.

The most important of these is the ``?=`` operator which combines
the ``.=`` with the ``.some`` match. Using this, the
``isMarriedTo`` function becomes:
[source,star]
----
isMarriedTo(P,J) where JJ?=P.spouse => J==JJ.
isMarriedTo(_,_) default => .false.
----
The meaning of ``?=`` is similar to the pattern match condition
``.=``; except that the pattern is assumed to be for a ``some``
value.

While the ``?=`` operatorfootnote:[Read as `has a value'.] is very
useful in unpacking an optional value, the ``^|`` operator allows
us to handle cases where we always need to be able to give some kind
of value. For example, normally a ``map`` returns none if an entry
is not present. However, a _cache_ is structured differently: if
a value is not present in a cache then we must go fetch it:

[source,star]
----
cacheValue(K) => cache[K] ^| fetch(K)
----

We can also apply a match _in line_ to an ``option``al value. The
``^`` operator allows a pattern to be formed by applying a
``option`` valued function directly in place. For example, the
equation:
[source,star]
----
head(first^(1)) => "alpha"
----
is equivalent to:
[source,star]
----
head(X) where .some(1).=first(X) => "alpha"
----
We will see more examples of this when we look more closely at
sequences and collections processing.

Finally, we can promote an optional expression into an enclosing
expression (which is therefore also optional). For example, in the
expression:

[source,star]
----
nameOf(^P.spouse)
----
if ``P.spouse`` is ``none``, then the value returned by the
``nameOf`` expression is also ``none``, otherwise it takes the
form:
[source,star]
----
some("P_spouse_name")
----
(assuming that ``nameOf`` is defined for ``Person``s.)

Overall, the ``option`` type is part of an elegant approach to
nullability that is easily incorporated into the type system. However, it is not the only Star feature oriented to _failable_ computation.

== Exceptions and short circuits

Not all computation goes according to plan; some computations can fail. The
simplest examples of this are in arithmetic: division by zero is not defined,
and taking the square root of a negative number is not defined over the real
numbers. Other examples typically involve something to do with input/output.

Star has a so-called exception handling mechanism. However, the inspiration of
it owes more to using optional values than the recovery from errors.

A `try` `catch` expression looks like:

[source,star,subs="quotes"]
----
(try _Exp~1~_ catch { _Ptn_ => _Exp~2~_})
----

For example, we can define a _safe_ divide as:
[source,star]
----
safeDivide(X,Y) => (try X/Y catch { _ => 0}).
----

Some special features of this:

* The type checker is able to verify that the type of the `_Ptn_` in the catch
  handler is consistent with exceptions that might be thrown in `_Exp~1~_`.
* Furthermore, the type of `_Exp~2~_` must be the same as that of `_Exp~1~_` and
  is the type of the whole expression.
* Only one type of exception may be thrown within `_Exp~1~_`.

The way to think about expression forms like `try` catch` (and their analogous
actions) is that the form allows a kind of short circuit evaluation: if the body
of the `try` `catch` throws an alternate value then the overall evaluation if
short circuited and evaluation continues with the catch handler.

NOTE: This is somewhat restricted compared to exception handling found in other
languages. However, this restriction has benefits also.

One benefit is that exceptions are fully integrated into the type system. We
can, for example, have functions that throw exceptions, as in the definition of the division operator itself:

[source,star]
----
(/):(x,x) throws exception
----

(This is part of the standard `arith` contract.)

We can also have function signatures that are generic in the type thrown:

[source,star]
----
consMap:all x,y,e ~~ (cons[x],(x)=>y throws e) => cons[y] throws e
----

This type signature expresses the type of a `consMap` function that takes a
function that throws _something_ and lifts that to a function over `cons` lists
that may also throw that same _something_.

Exceptions in Star are intentionally simple and lightweight: exception types can
be of any Star type (there is no special designated exception
type).footnote:[There is a standard `exception` type, but it is not required to
be used.] In addition, there are no additional features -- such as stack
traces. This results in a mechanism that is very efficient and has very minimal
implementation footprint.

== A word about type inference

We have seen some powerful forms of types in this chapter: recursive types
defined using algebraic type definitions, generic types, function types and even
function types that can throw. Recall also that we only require programmers to
explicitly declare the types of quantified variables and functions. It is worth
pausing a little to see how this might be done.

Recall our original ``fact`` function:
[source,star]
----
fact(0) => 1.
fact(N) => N*fact(N-1).
----
The compiler is able to compute the types of the various variables
automatically through a process known as _type inference_. Type
inference may seem magical, but is actually (mostly) quite simple. Let
us take a look at the expression:
[source,star]
----
N-1
----
which is buried within the recursive call in ``fact``. Although it
looks like a special operator, arithmetic expressions are not special;
the ``-`` function is just a function from numbers to numbers; its
type is given by:footnote:[We put the (-) in parentheses to highlight
the use of an operator as a normal symbol.]
[source,star]
----
(-) : all t ~~ arith[t] |: (t,t)=>t
----
However, we should simplify this type a little in order to make the
explanation of type inference a little simpler. In what follows, we
assume that the type of (``-``) is:
[source,star]
----
(-) : (integer,integer)=>integer
----
Type inference proceeds by using _type inference rules_
which relate expressions to types, in this case the applicable rule is
that a function application is consistent if the function's parameter
types are consistent with the types of the actual arguments. If they
are consistent, then the type of the function application is the
return type of the function.

The type inference process initially gives every variable an unknown
type -- represented by a new type variable not appearing anywhere
else. For our tiny ``N-1`` example, we will give ``N`` the type
_t~N~_.

The (``-``) function has two arguments whose types can be expressed
as a tuple of types:
[source,star]
----
(integer,integer)
----
and the types of the actual arguments are also a tuple:
[source,star,subs="quotes"]
----
(_t~N~_,integer)
----
In order for the expression to be type correct, the actual types of
the arguments must be consistent with the expected types of the
function; which we can do by making them _the same_. There is a
particular process used to do this -- called _unification_.

.Inferring the Type of N-1
image::images/minustype

_Unification_:: An algorithm that replaces variables with values in such a way as to
make two terms identical.

Unification matches equals with equals and handles (type) variables by
substitutions -- for example, we can make these two type expressions equal by
_binding_ the type variable _t~N~_ to ``integer``.

We initially picked the type of ``N`` to be an arbitrary type variable, but the
process of checking consistency leads us to refine this and make the type
concrete. I.e., the use of ``N`` in a context where an integer is expected is
enough to allow the compiler to infer that the type of ``N`` is indeed
``integer`` and not _t~N~_.

Of course, if there are multiple occurrences of ``N`` then each of those
occurrences must also be consistent with integer; and if an occurrence is not
consistent then the compiler will report an error -- a given expression may only
have one type.

The bottom line is that types are based on a combination of unification for
comparing types and a series of type rules that have the effect of introducing
_constraints_ on types based on which language features are present in the
text. The type checker is really a constraint solver: if the constraints are not
satisfiable (for example by trying to `call' a variable and add a number to it)
then there is a type error in the program.

The magic of type inference arises because it turns out that solving
these constraints is sufficient for robustly type checking programs.

A sharp-eyed reader will notice that Star's type system is different in nature
to that found (say) in OO languages. In Star's type system, types are considered
to be consistent only if they are _equal_. This is quite different to the notion
of consistency in OO languages where an argument to a function is consistent if
its type is a _sub-type_ of the expected type.

However, we would note that the apparent restriction to the type system imposed
by type equality is much less severe in practice than in theory -- and that OO
languages' type systems also often incorporate some of the same restrictions.

=== Why is type inference restricted?

We have stated a few times that the type system only infers types of variables
that are _not_ quantified. In fact, it is fairly straightforward to build a type
inference system that can infer quantified types. For example, such a complete
type inference system would infer from these equations:

[source,star]
----
conc(.nil,x)=>x.
conc(cons(h,tl),x) => cons(h,conc(tl,x))
----
the generalized type for ``conc``:
[source,star]
----
conc:all t ~~ (cons[t],cons[t])=>cons[t].
----
However, several technical and non-technical considerations stay our
hand at building such a type inference system:

* There are still types that cannot be correctly inferred; and would
therefore require explicit type annotations to correctly type the
program.
* Having explicit type annotations is 'good style' in general and
definitely aids in debugging type errors.
* Being explicit about quantification makes unification of quantified
types simpler and more reliable.

On the other hand, requiring type annotations for _every_
variable would be extremely tedious and verbose. An extreme version of
this policy would require the ``conc`` program above to be written:

[source,star]
----
conc:all t ~~ (cons[t],cons[t])=>cons[t].
conc(.nil,x:list[t])=>x.
conc(cons(h:t,tl:list[t]),x:list[t]) => cons(h,conc(t,x))
----
The design of Star strikes a balance between useability and rigor:
most variables do not require explicit type annotations. We require
them only when something 'special' is being indicated; one of those
special circumstances is when defining a generic function.

Even there, there are many situations where explicit type annotations
are not needed: for example when defining a field in a record, or a
function in the implementation of a contract, there already is a type
that the type system can use to verify the program.

So, the precise rule for type inference is:

[aside]
If the type of a variable is known from context, then use that type to
verify the type of any value the variable may be bound to. Otherwise,
use type inference on the value to infer a type for the variable but
do not attempt to generalize it by adding quantifiers to the inferred
type.

We are only able to scratch the surface of the type system here. It is certainly
true that -- like many modern functional languages -- Star's type system is
complex and subtle. The primary motivation for this complexity is to reduce the
burden for the programmer: by being able to infer types automatically, and by
being able to address many programming subtleties, the type system comes to be
seen as the programmer's friend rather than as an obstacle to be `gotten
around'.

== Are we there yet?

The straightforward answer to this is no. There is a great deal more to
functional programming than can be captured in a few pages. In particular, we
have not touched on one of the foundations of modern functional programming --
_Category Theory_.

However, we have covered some of the key features of functional programming --
particularly as it applies to Star. In subsequent chapters we will take a closer
look at collections, at modular programming, at concurrency and even take a pot
shot at Monads.

If there is a single idea to take away from this chapter it should be that
functional programming is natural. If there is a single piece of advice for the
budding functional Star programmer, it should be to _hide the recursion_. If
there is a single bit of comfort to offer programmers it should be that _Rome
was not built in a day_.

In the next chapter we look at collections, one of the richest topics in
programming.
