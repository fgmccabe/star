# Collections

Modern programming -- whether it is OO programming, functional programming or just plain C programming -- relies on a rich standard library. Given that nearly every program needs to be able to manage collections of *things*, the central pearl of any standard library is the *collections* library. Recalling our mantra of hiding recursion; a well designed collections library can make a huge difference to the programmer’s productivity, often by hiding many of the recursions and iterations required to process collections.

The collections architecture in **Star** has four main components:

1. a range of standard collection types -- including array-like lists, cons lists, red-black trees, first-in first-out queues, and dictionaries;
2. a range of standard functions -- mostly defined in contracts -- that define the core capabilities of functions over collection;
3. special notations that make programming with collections in a type independent way more straightforward; and
4. the final major component of the collections architecture is *queries*. **Star** has a simple yet powerful set of features aimed at simplifying querying collections.


## Sequence notation

A sequence is simply an ordered collection; a sequence expression is an expression involving a complete or partial enumeration of the values in the collection. Star has a simple notation for expressing sequences of any underlying type; for example, a cons sequence of integers from 1 through 5 can be written:
```
cons of [1, 2, 3, 4, 5]
```
In situations where we do not know or do not wish to specify the collection type, we can write instead:
```
[1, 2, 3, 4, 5]
```
This term -- it could be either an expression or a pattern -- denotes the sequence *without* specifying the underlying collection type. The difference in the types of the two terms is telling:
```
cons[integer]
```
and
```
stream[c->>integer] |: c
```
respectively -- where `c` is a type variable. The first is a concrete type expression, the second is a constrained type --  in this case `c` must implement the `stream` contract.

Although the second type expression is longer, and a bit more complex to read, it is also actually less constraining. The type expression `cons[integer]` does not allow for variation of the underlying collection type; the second type expression allows the term to be used in contexts that require different concrete types.

### Partial sequence notation
The sequence notation also allows for the specification of partial sequences; this is particularly useful in writing functions that construct and traverse sequences. The sequence term:
```
[1,2,..X]
```
denotes the sequence whose first two elements are `1` and `2` and whose remainder is denoted by the variable `X` -- which must also be a sequence of the correct type. Similarly, the term:
```
[F..,23]
```
denotes the sequence obtained by gluing `23` to the back of the sequence `F`.

There is a strong relationship between the normal sequence notation and the partial sequence notation. In particular, the sequence expression
```
cons of [1,2]
```
is equivalent to:
```
cons of [1,..cons of [2,..cons of []]]
```
However, we are not permitted to use both of `,..` and `..,` in the same expression:
```
[F..,2,3,..B]
```
is not permitted (since it amounts to a concatenation of two sequences which implies a non-deterministic decomposition when used as a pattern).

The major benefit of general sequence notation is that it allows us to construct programs involving collections that are independent of type *and* to do so in a syntax which is concise: the only constraint is the sequence contract.

For example, we can use sequence notation to write functions over sequences; such as the concat function that concatenates two sequences:
```
concat:all c,e ~~ stream[c->>e] |: (c,c)=>c.
concat([],X) => X.
concat([E,..X],Y) => [E,..concat(X,Y)].
```
This function will work equally well with `cons` lists, lists, strings, even your own collection types. All that is required is that there is an implementation of the sequence contract for the actual type being concatenated.

### The stream contract
Underlying the sequence notation is the `stream` contract. This contract contains type signatures that can be used to construct and to match against stream values. The stream notation is realized by the compiler translating stream terms to a series of calls to those functions.

The actual `stream` contract is
```
contract stream[t->>e] ::= {
  _nil:t.     -- empty stream
  _cons:(e,t)=>t. -- add to front
  _apnd:(t,e)=>e. -- add to back
  _eof:()=>boolean.   -- is stream empty
  _hdtl:(t)=>option[(e,t)]. -- match front of stream
  _back:(t)=>option[(t,e)]. -- match back of stream
}
```

The first three entries in this contract should be fairly self-evident:

* _nil is the empty stream;
* _cons is a function that ‘glues’ a new element to the front of the collection; and
* _apnd appends elements to the *back* of the collection.

The compiler uses these three functions to transform stream expressions into function calls.

For example, the sequence expression:
```
[1,2,3]
```
is transformed into
```
_cons(1,_cons(2,_cons(3,_nil())))
```
If a stream expression has an explicit type marker on it, then its translation is slightly different -- to allow the type checker to make use of the type information. For example,
```
cons of [1,2]
```
is translated as:
```
_cons(1,_cons(2,_nil())):cons[_]
```
This annotation is all that is needed to force the compiler to treat the result as a concrete cons list. Type inference does the rest of the hard work.[^fn2]

[^fn2]: The type expression `_` is a special type that denotes an anonymous type: each occurrence of the type expression denotes a different unknown type. It is useful in situations, like this one, where only some of the type information is known.

### Sequence patterns
The complete stream contract has six signatures in it -- the latter three signatures play an analogous role to the first three but for sequence *patterns* rather than sequence *expressions*. They also introduce a new form of type expression -- the *pattern type*. For example, the signature for _pair -- which is used to decompose sequences into a head and tail -- is:
```
_hdtl:(t)=>option[(e,t)].
```
This function will be applied to a sequence in the attempt to split it into a head and remainder. The question is how can a function be used in a pattern?

The term `[1,2,..X]` _as a pattern_ is rewritten as:
```
_hdtl?(1,_hdtl?(2,X))
```
where the `?` is syntactic sugar for the more elaborate form:
```
S0 where (1,S1) ?= _hdtl(S0) && (2,X)?=_hdtl(S1)
```
which is, in turn, syntactic sugar for:
```
S0 where some((1,S1)) .= _hdtl(S0) && some((2,X)) .= _hdtl(S1)
```
I.e., the sequence pattern becomes a series of progressive decompositions of the stream; at each stage an `option`-valued function is applied to peel off elements from the stream.

**Pattern Abstraction**



*A pattern abstraction is an expression that denotes a pattern.*

Pattern abstractions are exactly analogous to functions -- another name for which is *expression abstraction*. Pattern abstractions allow patterns to be encapsulated and reused in the same way that functions allow expressions to be encapsulated and reused.

In this case, the pattern abstraction is critical because general sequence notation is independent of the types of the collections involved -- and so we have no way of knowing what concrete patterns to apply.

Pattern abstractions are applied using the same application notation as for function application; for example, the _pair pattern in

first:all c,e ~~ stream[c->>e] |: (c)=>e.

first(_pair(H,T)) => H.

is a pattern abstraction that is applied to the argument of first. What may be a little surprising initially is that the arguments to a pattern application are also patterns! So, here, the variables H and T in the call to _pair will be bound to the first element of the collection and the remainder respectively.

For example, applying _pair(H,T) to [1,2] binds H to the value 1 and binds T to the sequence [2]. The value returned by first will be 1. We can combine pattern applications in an exactly analogous manner to the way we combine function calls.

Obviously, there must also be a way of defining pattern abstractions. We can define a cPair pattern abstraction that applies to cons lists thus:

cPair:all e ~~ (e,cons[e])<=cons[e].

cPair(H,T) <= cons(H,T).

This takes a little careful reading, but is ultimately straightforward: the right hand side is the pattern that is being abstracted, the left hand side is an application template.

The general form of a pattern abstraction is:

*name*(*E1*,..,*En*) <= *Pattern*

where the various *Ei* are *expressions* that represent the values ‘read off’ the *Pattern* -- should the pattern be satisfied. This is quite analogous to the situation for rewrite equations -- except that the roles of patterns and expressions are reversed.

Like functions, pattern abstractions may be defined with multiple pattern rules; and the pattern abstraction is satisfiable exactly when one of its pattern rules is.

Pattern abstractions are not as ubiquitous as functions; however, they certainly play a vital role in the overall design of **Star**; and are indispensable in the right circumstances.

One special use for pattern abstractions is to give higher-level names to particular patterns. This mimics the use of functions naming expressions, and has a similar importance for program design.

Given that we have seen how sequence expressions are transformed into function calls from the stream contract, we can now straightforwardly give the equivalent translation for sequence patterns. Syntactically, there is no distinction between sequence expressions and sequence patterns -- what distinguishes them is context: sequence patterns show up as patterns in functions and sequence expressions show up in the expression context.

A sequence pattern, as in the pattern [E,..X] for the non-empty case in concat:

concat([E,..X],Y) => [E,..concat(X,Y)]

is transformed into the pattern:

_pair(E,X)

and the entire rewrite equation becomes:

concat(_pair(E,X),Y) => _cons(E,concat(X,Y))

We can combine multiple pattern abstraction applications; for example, the function:

single:all c,e ~~ sequence[c->>e] |: (c)=>e.

single([H]) => H

which is a function that only matches singleton sequences requires two pattern applications from the sequence contract:

single(_pair(H,_empty())) => H

The sequence contract is one of the most important and commonly used contracts in the **Star** library. As we shall see further, many of the standard collections functions are built on top of it.

### Notation and contracts
One of the distinctive features of the sequence notation is that it is an example of *syntax* that is underwritten by a semantics expressed as a *contract*. This is part of a widespread pattern in **Star**.

This has a parallel in modern OO languages like Java and C# where important contracts are expressed as interfaces rather than concrete types. However, **Star** extends the concept by permitting special notation as well as abstract interfaces -- as many mathematicians understand, a good notation can make a hard problem easy. In **Star** we further separate interfaces from types by separating the type definition from any contracts that may be implemented by it.

This is part of a general pattern in **Star**: there are many *sub-languages* that are actually underwritten by contracts for their realization. For example, the *indexing* notation has the same pattern: of a special notation backed by contract.

The merit of this combination of special syntax and contracts is that we can have the special notation expressing a salient concept -- in this case the sequence -- and we can realize the notation without undue commitment in its lower-level details. In the case of sequence notation, we can have a notation of sequences without having to commit to the type of the sequence itself.

## Indexing
Accessing collections conveniently is arguably more important than a good notation for representing them. There is a long standing traditional notation for accessing arrays:
```
L[ix]
```
where L is some array or other collection and ix is an integer offset into the array. **Star** uses a notation based on this for accessing collections with random indices; suitably generalized to include dictionaries (collections accessed with non-numeric indices) and *slices* (contiguous sub-regions of collections).

Before we explore **Star**’s indexing notation it is worth looking at the contract that underlies it -- the indexable contract.

### The indexable contract
The indexable contract captures the essence of accessing a collection in a random-access fashion. There are functions in the contract to access a directly accessed element, to replace and to delete elements from the collection:

```
contract all s,k,v ~~ indexable[s->>k,v] ::= {
  _index:(s,k)=>option[v].
  _set_indexed:(s,k,v)=>s.
  _delete_indexed:(s,k)=>s.
}
```
There are several noteworthy points here:

* the form of the contract itself; the signature for _index which accesses elements; and
* the signatures for _set_indexed and _delete_indexed which return new collections rather than modifying in-place.

Recall that the stream contract had the form:
```
contract all s, e ~~ stream[s->>e] ::= ...
```
the s->>e clause allows the implementation of the contract to functionally determine (sic) the type of the elements of the collection.

In the case of indexable, the contract form determines *two* types denoted by k and v. The type k denotes the type of the key used to access the collection and v denotes the type of the elements of the collection. Each individual implementation of indexable is free to specify these types; usually in a way that best reflects the natural structure of the collection.

For example, the implementation of indexable for strings starts:
```
implementation indexable[string ->> integer,char] => ...
```
reflecting the fact that the natural index for strings is integer and the natural element type is char (neither being explicitly part of the string type name).  Note that char is a synonym for integer; but that is not relevant here.

On the other hand, the implementation of indexable for the concrete type `map` starts:
```
implementation all k,v ~~
  indexable[map[k,v] ->> k,v] => ...
```
reflecting the fact that dictionaries are naturally generic over both the key and value types.

### Tentative computing
If we look at the signature for _index we can see that this function does not directly return a value from the collection, but instead returns an option value. This bears further explanation.

The great unknown of accessing elements of a collection is ‘is it there?’. Its not guaranteed of course, and we need to be able to handle failure.

This is where the concept of ‘tentative computation’ becomes important.

**Tentative Computation**
: A tentative computation is denoted by an expression that is inherently plausible to not have a value.

When we want to open a file, access an element of a `map`, parse a string with a regular expression we need to be able to express the possibility of failure as well as of success. There are also times when ‘no answer’ is a legitimate response.

We encode this tentativeness (sic) in the option type. The type definition for option is straightforward:
```
all t ~~ option[t] ::= none | some(t).
```
where `none` is intended to denote the non-existence of a value and some denotes an actual value.

The option type is intended to be used in cases where functions are known to be partial.[^fn1] The option return type signals that the function may not always have a value.

In the case of the _index function, its responsibility is to either return a value wrapped as a some value -- if the index lookup is successful -- or the signal none if the index lookup fails. Just to be clear, _index can act both as a lookup *and* as a test for membership in the collection.

In addition to the option type, there are a series of operators that make tentative computations easier to express: these are the optional field access operator -- `?.` -- the option default operator -- `?|` -- and the `?=` optional binding predicate.

The `?|` operator allows one to unpack an optional value but to give a default in the case that the optional value is none.

We can see where the latter may be useful when accessing dictionaries. For example, the fillIn function accesses a `map` for a key but uses a default value when it is not there:
```
fillIn:all k,v ~~ (map[k,v],k,v)=>v.
fillIn(Tr,Ky,Def) where Vl ?= _index(Tr,Ky) => Vl.
fillIn(Tr,_,Def) => Def.
```
The condition `Vl ?= _index(Tr,Ky)` is satisfied if the `_index` call returns a proper value and it also binds the variable `Vl` to that value (specifically, it *matches* the value against the variable `Vl`.

While the `?=` operator[^fn2] is very useful in unpacking an optional value, the `?|` operator allows us to handle cases where we always need to be able to give some kind of value. For example, normally a `map` returns none if an entry is not present. However, a *cache* is structured differently: if a value is not present in a cache then we must go fetch it:

```
cacheValue(K) => cache[K] ?| fetch(K)
```
There is, clearly, a strong relationship between `?=` and `?|` -- each can be expressed in terms of the other.

The option type -- and the some and none values -- play some of the same roles as NULL does in other languages. For someone approaching a functional language from most imperative languages they will be struck -- and maybe upset -- by the lack of a null (or nil or NULL or undefined). After all, if it’s good enough for Java, why can’t **Star** have it too?

Perhaps the biggest single reasons for not having a universal NULL value are that it corrupts the type system and that it makes reasoning about programs harder.

In a language which has a universal NULL value, the programmer (and the compiler) must be ever vigilant about references: is the value actually a NULL value? This is true even in those cases where the programmer knows values cannot be NULL. By isolating nullability (sic) into a single concept it allows the programmer to use the feature where it is actually needed.

Overall, the option type is part of an elegant approach to nullability that is easily incorporated into **Star**’s (and similar) type system.

[^fn1]: A partial function does not have a value across the whole range of its arguments.

[^fn2]: Read as ‘has a value’.

### Adding and removing elements
The function _set_indexed is used to add an element to a collection associating it with a particular index position; and the function _delete_indexed removes an identified element from the collection.

Both of these functions have a property often seen in functional programming languages and not often seen elsewhere: they are defined to return a complete new collection rather than simply side-effecting the collection. This is inline with an emphasis on *persistent data structures*[^fn1] and on *declarative programming*.

One might believe that this is a bit wasteful and expensive -- returning new collections instead of side-effecting the collection. However, that is something of a misconception: modern functional data structures have excellent computational properties and approach the best side-effecting structures in efficiency. At the same time, persistent data structures have many advantages -- including substantially better correctness properties and behavior in parallel execution contexts.

It should also be stressed that the indexable contract allows and encourages persistence but does not *enforce* it. It is quite possible to implement indexing for data structures that are not persistent.

[^fn1]: A persistent structure is one which is never modified.

### The index notation
Given the indexable contract we can now show the specific notation that **Star** has for accessing elements of a collection. Accessing a collection by index follows conventional notation:

C[ix]

will access the collection C with element identified by ix. For example, given a `map` D of strings to strings, we can access the entry associated with “alpha” using:

D["alpha"]

Similarly, we can access the third character in a string S using:

S[2]

As might be expected, given the discussion above, the type of an index expression is optional. This is because the element may not be there; i.e., it is an example of a *tentative computation*.

The most natural way of making use of an index expression is to use it in combination with a ?.= condition or an ?| expression -- which allows for smooth handling of the case where the index fails. For example, we might have:

nameOf(F) where N ?.= names[F] => N.

nameOf(F) => ...

We will take a deeper look at exceptions and more elaborate management of tentative computation in the section on [Computation Expressions](file:///Users/fgm/Projects/Cafe/Docs/Book/computation-expression).

**Star** also has specific notation to represent modified collections. For example, the expression

D["beta"->"three"]

denotes the `map` D with the entry associated with "beta" replaced by the value "three". Note that the value of this expression is the updated map.

For familiarity’s sake, we also suppose a form of assignment for the case where the collection is part of a read-write variable. The action:

D["beta"] := "three"

is entirely equivalent to:

D := D["beta"->"three"]

always assuming that the type of D permits assignment.

Similarly, the expression:

D[+"gamma"]

which denotes the `map` D where the value associated with the key "gamma" has been removed.

In addition to these forms, there is also a test expression:

present D["delta"]

which is a predicate that is true if the `map` D contains an entry for "delta".

Although, in these examples, we have assumed that D is a `map` value (which is a standard type in **Star**); in fact the index notation does not specify the type. As with the sequence notation, the only requirement is that the indexable contract is implemented for the collection being indexed.

In particular, index notation is supported for the built-in list types, and is even supported for the string type.

In addition to the indexed access notation described so far, **Star** also allows a variant of the sequence notation for constructing indexable literals (aka dictionaries). In particular, an expression of the form:

["alpha"->1, "beta"->2, "gamma"->3]

is equivalent to a sequence of tuples, or to:

_cons(("alpha",1),_cons(("beta",2),_cons(("gamma",3),_nil())))

which is understood by indexable types as denoting the contruction of a literal.

Note that there are two levels of domain-specific notation here: the representation of indexed literals in terms of a sequence of two-tuples and the implicit rule governing indexable types: they should implement a specific form of sequence contract. Both are actually part of the semantics of representing indexable literals.

### Implementing indexing
Of course, this includes our own types. For example, before, when looking at generic types we saw the tree type:

all t ~~ tree[t] ::= tEmpty | tNode(tree[t],t,tree[t]).

We can define an implementation for the indexable contract for this type -- if we arrange for the tree to be a tree of key-value pairs:

implementation all k,v ~~

comparable[k], equality[k] |:

indexable[tree[(k,v)]->>k,v] => {

_index(T,K) => findInTree(T,K).

_set_indexed(T,K,V) => setKinTree(T,K,V).

_delete_indexed(T,K) => removeKfromTree(T,K).

}

The form of the type expression tree[(k,v)] is required to avoid confusion -- tree takes a single type argument that, in this case, is a tuple type. The extra set of parentheses ensures that tree is not interpreted (incorrectly) as a type that takes two type arguments.

With this statement in scope, we can treat appropriate tree expressions as though they were regular arrays or dictionaries:

T = tNode(tEmpty,("alpha","one"),tEmpty)

assert T["alpha"]=="one".

U = T["beta"->"two"]. -- Add in "beta"

assert U["alpha"]=="one".

assert present U["beta"].

assert + present U["gamma"].

The implementation statement relies on another feature of **Star**’s type system -- we need to constrain the implementation of indexable to a certain subset of possible instances of tree types -- namely, where the element type of the tree is a *pair* -- a two-tuple -- and secondly we require that the first element of the pair is comparable -- i.e., it has the comparable contract defined for it.

This is captured in the contract clause of the implementation statement:

implementation all k,v ~~

comparable[k], equality[k] |:

indexable[tree[(k,v)]->>k,v] => ...

This implementation statement is fairly long, and the type constraints are fairly complex; but it is exquisitely targeted at precisely the right kind of tree without us having to make any unnecessary assumptions.[^fn1]

Implementing the indexable contract requires us to implement three functions: findInTree, setKinTree and removeKfromTree. The findInTree function is quite straightforward:

findInTree:all k,v ~~ (tree[(k,v)],k)=>option[v].

findInTree(tEmpty,_) => none.

findInTree(tNode(_,(K,V),_),K) => some(V).

findInTree(tNode(L,(K1,_),_),K) where K1>K => findInTree(L,K).

findInTree(tNode(_,(K1,_),R),K) where K1<K => findInTree(R,K).

Notice that each ‘label’ in the tree is a 2-tuple -- consisting of the key and the value. This function is also where we need the key type to be both comparable and supporting equality. The comparable constraint has an obvious source: we perform inequality tests on the key.

The equality constraint comes from a slightly less obvious source: the repeated occurrence of the K variable in the second equation. This equation is actually equivalent to:

findInTree(tNode(_,(K,V),_),K1) where K==K1 => some(V)

We leave the implementations of setKinTree and removeKfromTree as an exercise for the reader.

Along with the implementation of indexable, we should also implement stream for our trees:

implementation all k,v ~~ comparable[k], equality[k] |:

stream[tree[(k,v)]->(k,v)] => {

_nil() => tEmpty.

_cons((K,V),T) => setKinTree(T,K,V).

...

}

where, again, we leave the implementation of the remaining part of the contract to the reader. The reason for implementing stream is that that will allow users of the tree to use the `map` variant of the sequence notation for writing tree literals; as in:

tree of [1->”alpha”, 2->”beta”]

The stream contract includes a pattern that can be used to match sequences. It is not completely obvious what one should do when matching the head of a tree. The standard library chooses to match against the left-most leaf of the tree and return a new tree containing all the other elements for the tail of the match. Since the tree is ordered by key value, the order in which elements of a tree are written in a tree literal does not affect the finally constructed tree value.[^fn2]

[^fn1]: It is also true that most programmers will not be constructing new implementations of the indexable contract very frequently.

[^fn2]: With the possible exception of tree balance.

### Index slices
Related to accessing and manipulating individual elements of collections are the *indexed slice* operators. An indexed slice of a collection refers to a bounded subset of the collection. The expression:

C[fx:tx]

denotes the subsequence of C starting with -- and including -- the element indexed at fx and ending -- but *not* including the element indexed at tx.

As might be expected, the index slice notation is also governed by a contract -- the sliceable contract. This contract defines the core functions for slicing collections and for updating subsequences of collections:

contract all s,k ~~ sliceable[s->>k] ::= {

_slice:(s,k,k)=>s.

_tail:(s,k)=>s.

_splice:(s,k,k,s)=>s.

}

The _slice function is used extract a slice from the collection, _tail is a variant that returns the ‘rest’ of the collection, and _splice is used to replace a subset of the collection with another collection.

Like the indexing notation, there is notation for each of the three cases:

C[fx:]

denotes the tail of the collection -- all the elements in C that come after fx (including fx itself); and

C[fx:tx->D]

denotes the result of splicing D into C. This last form has an additional incarnation -- in the form of an assignment statement:

C[fx:tx] := D

This action is equivalent to the assignment:

C := _splice(C,fx,tx,D)

which, of course, assumes that C is correctly defined as a read/write variable.

**Star** encourages declarative programming but we fully recognize that side-effecting behavioral code is often the most effective solution to the problem.

The slice notation is an interesting edge case in domain specific languages. It is arguably a little obscure, and, furthermore, the use case it represents is not all that common. On the other hand, without specific support, the functionality of slicing is hard to duplicate with the standard indexing functions.

## Doing stuff with collections
One of the most powerful features of collections is the ability to treat a collection as a whole. We have already seen a little of this in our analysis of the [visitor pattern](file:///Users/fgm/Projects/Cafe/Docs/Book/going-even-further). Of course, the point of collections is to be able to operate over them as entities in their own right. As should now be obvious, most of the features we discuss are governed by contracts and it is paradigmatic to focus on contract specifications rather than specific implementations.

The number of things that people want to do with collections is only limited by our imagination; however, we can summarize a class of operations in terms several patterns:

* Filtering
* Transforming into new collections
* Summarizing collections
* Querying collections

Each of these patterns has some support from **Star**’s standard repertoire of functions.

### Filtering with filter
The simplest operation on a collection is to subset it. The standard function filter allows us to do this with some elegance. Using filter is fairly straightforward; for example, to remove all odd numbers from a collection we can use the expression:

filter((X)=>X%2==0,Nums)

For example, if Nums were the list:

list of [1,2,3,4,5,6,7,8,9]

then the value of the filter expression would be

list of [2,4,6,8]

The first argument to filter is a *predicate*: a function that returns a boolean value. The filter function (which is part of a standard contract) is required to apply the predicate to every element of its second argument and return a *new* collection of every element that satisfies the predicate.[^fn1]

Note that the % function is arithmetic remainder, and the expression X%2==0 amounts to a test that X is even (its remainder modulo 2 is 0).

By using a function argument to represent the predicate it is possible to construct many filtering algorithms whilst not making any recursion explicit. However, not all filters are easily handled in this way; for example, a prime number filter *can* be written

filter(isPrime,N)

but such an expression is likely to be very expensive (the isPrime test is difficult to do well).

[^fn1]: The original collection is unaffected by the filter.

#### The filterable contract
As noted above, the filter function is governed by a contract, the filterable contract:

contract all c,e ~~ filterable[c->e] ::= {

filter:((e)=>boolean,c) => c.

}

### The sieve of Erastosthneses
One of the classic algorithms for finding primes can be expressed using filters -- the so-called sieve of Eratosthenes. This algorithm works by repeatedly removing multiples of primes from the list of natural numbers. We cannot (yet) show how to deal with infinite lists of numbers but we can capture the essence of this algorithm using a cascading sequence of filter operations.

The core of the sieve algorithm involves taking a list of numbers and removing multiples of a given number from the list. This is very similar to our even-number finding task, and we can easily define a function that achieves this:

filterMultiples(K,N) => filter((X)=>X%K=!=0,N).

The overall Eratosthenes algorithm works by taking the first element of a candidate list of numbers as the first prime, removing multiples of that number from the rest, and recursing on the result:

sieve([N,..rest]) => [N,..sieve(filterMultiples(N,rest))].

There is a base case of course, when the list of numbers is exhausted then we have no more primes:

sieve([]) => [].

The complete prime finding program is hardly larger than the original filter specification:

primes(Max) => let{

sieve([]) => [].

sieve([N,..rest]) => [N,..sieve(filterMultiples(N,rest))].

filterMultiples(K,N) => filter((X)=>X%K=!=0,N).

iota(Mx,St) where Mx>Max => [].

iota(Cx,St) => [Cx,..iota(Cx+St,St)].

} in [2,..sieve(iota(3,2))]

The iota function is used to construct a list of numbers, in this case the integer range from 3 through to Max with an increment of 2. We start the sieve with 2 and the list of integers with 3 since we are making use of our prior knowledge that 2 is prime.

It should be emphasized that the sieve of Eratosthenes hardly counts as an efficient algorithm for finding primes. For one thing, it requires that we start with a list of integers; most of which will be discarded. In fact, each ‘sweep’ of the list of numbers results in a new list of numbers; many of which too will eventually be discarded. Furthermore, the filterMultiples function examines every integer in the list; it does not make effective use of the fact that successive multiples occupy predictable slots in the list of integers. In fact, building a highly optimized version of the sieve of Eratosthenes is not actually the main point here -- it is to illustrate the power of **Star**’s collections processing functions.

We might ask whether the sieve function can also be expressed as a filter. The straightforward answer is that it cannot: the sieve *is* a kind of filter, but the predicate being applied depends on the entire collection; not on each element. The standard filter function does not expose the entire collection to the predicate. However, we will see at least one way of achieving the sieve without any explicit recursion below when we look at folding operations.

### Mapping to make new collections
One of the limitations of the filter function is that it does not create new elements: we can use it to subset collections but we cannot transform them into new ones. The fmap function -- part of the functor contract -- can be used to perform many transformations of collections.

For example, to compute the lengths of strings in a list we can use the expression:

fmap(size,list of ["alpha","beta","gamma"])

which results in the list:

list of [5,4,5]

The fmap function is defined via the functor contract -- thus allowing different implementations for different collection types:

contract all c/1 ~~ functor[c] is {

fmap:all e,f ~~ ((e)=>f,c[e])=>c[f]

}

Notice how the contract specifies the collection type -- c -- without specifying the type of the collection’s element type. We are using a different technique here than we used for the stream and filterable contracts. Instead of using a functional dependency to connect the type of the collection to the type of the element, we denote the type of the input and output collections using a *type constructor* variable as in c[e] and c[f].[^fn1]

We are also using a variant of the quantifier. A quantified type variable of the form c/1 denotes a type constructor variable rather than a regular type variable. In this case, c/1 means that the variable c must be a type constructor that takes one argument.

The reason for this form of contract is that functor implies creating a new collection from an old collection; with a possibly different element type. This is only possible if the collection is generic and hence the type expressions c[e] for the second argument type of fmap and c[f] for its return type.

One might ask whether we could not have used functional dependencies in a similar way to stream and filterable; for example, a contract of the form:

contract all c,e,f ~~ mappable[c->>e,f] ::=  {

mmap:((e)=>f,c)=>c.

}

However, *this* contract forces the types of the result of the mmap to be identical to its input type, it also allows the implementer of the mappable contract to fix the types of the collection elements -- not at all what we want from a fmap.

It is not all that common that we need to construct a list of sizes of strings. A much more realistic use of fmap is for *projection*. For example, if we wanted to compute the average age of a collection of people, which is characterized by the type definition:

person ::= someOne{

name:string.

age:()=>float.

}

Suppose that we already had a function average that could average a collection of numbers; but which (of course) does not understand people. We can use our average by first of all projecting out the ages and then applying the average function:

average(fmap((X)=>X.age(),People))

In this expression we project out from the People collection the ages of the people and then use that as input to the average function.

There is something a little magic about the lambda function in this expression: how does the type checker ‘know’ that X can have a field age in it? How much does the type checker know about types anyway?

In this particular situation the type checker could infer the type of the lambda via the linking between the type of the fmap function and the type of the People variable. However, the type checker is actually capable of giving a type to the lambda even without this context. Consider the function:

nameOf(R) => R.name

This function takes an arbitrary record as input and returns the value of the name field. The nameOf function *is* well typed, its type annotation just needs a slightly different form than that we have seen so far:

nameOf:all r,n ~~ r <~ {name:n} |: (r)=>n

This is another example of a *constrained type*: in this case, the constraint on r is that it has a field called name whose type is the same as that returned by nameOf itself.

The type constraint:

r <~ {name:n}

means that any type bound to r must have a name field whose type is denoted by the type variable n in this case.

With this type signature, we can use nameOf with any type that that a name field. This can be a record type; it can also be a type defined with an algebraic type definition that includes a record constructor.

[^fn1]: This also means that the collection type in fmap must be generic: it is not possible to implement functor for strings.

### Compressing collections
Another way of using collections is to summarize or aggregate over them. For example, the average function computes a single number from an entire collection of numbers -- as do many of the other statistical functions. We can define average using the standard leftFold1 function, which is part of the standard folding contract:

average(C) is leftFold1((+),C)::float/size(C)::float

Notice the use of coercion here -- coercing both the result of the leftFold1 and size to float. The reason for doing this is that functions like average are ‘naturally’ real functions.[^fn1] Without the explicit coercion, averaging a list of integers will also result in an integer value -- which is likely to be inaccurate.

Of course, in our definition of average we need to coerce *both* the numerator and denominator of the division because **Star** does not have implicit coercion.

The leftFold1 function applies a left-associative binary operator to a collection: starting from the first element and successively ‘adding up’ each of the elements in the collection using the supplied operator.

As we noted above, leftFold1 is part of the foldable contract. Like the functor contract, this uses some more subtle type constraints:

contract all c/1 ~~ foldable[c] ::= {

leftFold1:all e ~~ ((e,e)=>e,c[e])=>e.

...

}

Notice that this contract -- which we have not fully written down here -- uses quantifiers in two places: once in the contract specification and once in the type signature for leftFold1. What we are trying to express here is that any implementation of foldable must allow for a generic accumulator function.

![][folding]

Left Folding a Collection

Our definition of the average function is therefore about as close to a specification of average as is possible in a programming language!

While it is convenient for computing averages, there are several restrictions in the signature for leftFold1: the most egregious is that the result type and the types of the elements of the collection must be identical. A more refined function is possible that liberates us from this:

contract all c/1 ~~ folding[c] ::= {

...

leftFold:all e,ac~~((ac,e)=>ac,ac,c[e])=>ac

...

}

leftFold is also part of the foldable contract; as are the analogous rightFold and rightFold1 functions.

In this variant of fold, we separate out an ‘accumulator’ (of type ac) from the type of the elements of the collection. We saw something similar with the visitor pattern. The leftFold function applies its argument function in a similar way to leftFold1, except that it does not require that the type of the accumulator is the same as the elements of the collection. Furthermore, it does not use the first element of the collection as a ‘seed’ of the aggregation -- instead, the initial seed is explicitly given.

![][leftfold]

Left Folding a Collection With a Seed

We can still use leftFold in situations where we would use leftFold1, for example our average function is equally well written as:

average(C) => leftFold((+),0,C)::float/size(C)::float

But we can do much more than computing averages with a fold. Recall that when we realized the sieve of Eratosthenes, we still had a recursive structure to the program. Furthermore, the way our original program was written each filter results in a new list of numbers being produced. Instead of doing this, we can construct a cascade of filter functions.

Consider the task of adding a filter to an existing filter. What is needed is a new function that combines the effect of the new filter with the old one. The cascade function takes a filter function and a prime as arguments and constructs a new function that checks both the prime *and* the existing filter:

cascade:((integer)=>boolean,integer)=>((integer)=>boolean).

cascade(F,K) => (X)=>F(X) && X%K=!=0.

This is a truly higher-order function: it takes a function as argument and returns a another function.

Given cascade, we can reformulate the sieve function itself as a leftFold -- at each new prime step we ‘accumulate’ a new cascaded filter function:

step:((integer)=>boolean,integer)=>((integer)=>boolean).

step(F,X) where F(X) => cascade(F,X).

step(F,X) => F.

At each step in the fold we want to know whether to continue to propagate the existing filter or whether to construct a new filter.

The sieve function itself is now very short: we simply invoke leftFold using step and an initial ‘state’ consisting of a function that checks for odd numbers:

sieve(C) => leftFold(step,(K)=>K%2=!=0,C).

This version of sieve is not quite satisfactory as, while it does find prime numbers, it does not report them. A more complete version has to also accumulate a list of primes that are found. We can do this by expanding the accumulated state to include both the cascaded filter function and the list of found primes. The main alteration is to the step function:

prStep:((list[integer],(integer)=>boolean),integer) =>

(list[integer],(integer)=>boolean).

prStep((P,F),X) where F(X) => ([P..,X],cascade(F,X))

prStep((P,F),_) => (P,F)

and the initial state has an empty list:

sieve(C) is fst(leftFold(prStep,([],(K)=>K%2=!=0),C)).

where fst and snd pick the left and right hand sides of a tuple pair:

fst:all s,t ~~ ((s,t))=>s.

fst((L,R)) => L

snd:all s,t ~~ ((s,t))=>t.

snd((L,R)) => R

There is one final step we can make before leaving our sieve of Eratosthenes -- we can do something about the initial list of integers. As it stands, while the sieve program does not construct any intermediate lists of integers, it still requires an initial list of integers to filter. However, this particular sequence can be represented in a very compact form -- as a range term.

range terms are special forms of collections that denote ranges of numeric values. For example, the expression

range(0,100,2)

denotes the sequence of integers starting at zero, not including 100, each succesive integer being incremented by 2.

Using a similar range term, we can denote the list of primes less than 1000 with

primes(Max) => let{

cascade:((integer)=>boolean,integer)=>((integer)=>boolean).

cascade(F,K) => (X)=>(F(X) && X%K=!=0).

prStep:((list[integer],(integer)=>boolean),integer) =>

(list[integer],(integer)=>boolean).

prStep((P,F),X) where F(X) => ([P..,X],cascade(F,X)).

prStep((P,F),_) => (P,F).

sieve:(list[integer])=>list[integer].

sieve(C) is fst(leftFold(prStep,([],(K)=>K%2=!=0),C)).

} in sieve(range(3,Max,2)).

show primes(1000)

This final program has an important property: there are no explicit recursions in it -- in addition, apart from the leftFold function, there are no recursive programs at all in the definition of primes.

[folding]: folding.png width=332px height=204px

[leftfold]: leftfold.png width=376px height=173px

[^fn1]: Real as in the Real numbers.

## Different types of collection
Just as there are many uses of collections, so there are different performance requirements for collections themselves. The most challenging aspects of implementing collections revolves around the cost of adding to the collection, the cost of *accessing* elements of the collection and the cost of *modifying* elements in the collection.

There is a strong emphasis on *persistent* semantics for the types and functions that make up **Star**’s collections architecture. This is manifest in the fact, for example, that functions that add and remove elements from collections *do not* modify the original collection.

However, even without that constraint, different implementation techniques for collections tend to favor some operations at the cost of others. Hence, there are different types of collection that favor different patterns of use.

### The list type
The list type offers a different trade-off to the cons type: where the latter is optimal for ease of constructing and for traversing complete lists, the list type offers constant-time access to random elements within the array -- at the potential cost of more expensive construction of lists.

Unlike the cons type, the list type does not have a straightforward definition as an algebraic type. Internally, an list structure consists of an array of locations with a ‘control pointer’ giving the portion of the array block that represents a given list value.

The list type is optimized for random access and for shared storage -- recall that **Star** collection types are persistent: that means that different values can share some or all of their internal structure. The diagram below shows two list values that overlap in their elements and consequently share some of their structure.

![][twoarrays]

Two lists Sharing Structure

[twoarrays]: twoarrays.pdf width=311px height=143px

### The cons type

This is the simplest collection type; and is perhaps the original collection type used in functional programming languages. It is defined by the type declaration:

all t ~~ cons[t] ::= nil | cons(t,cons[t]).

Cons lists have the property that adding an element to the front of a list is a constant-time operation; similarly, splitting a cons list into its head and tail is also a constant time operation. However, almost every other operation is significantly more expensive: putting an element on to the end of a cons list is linear in the length of the list.

The main merit of the cons list is the sheer simplicity of its definition. Also, for small collections, its simple implementation outweighs the advantages that more complex collections offer.

### The map type
Unlike the cons or list type, the `map` type is oriented for access by arbitrary keys. The `map` is also quite different to hash trees as found in Java (say), the `map` type is *persistent*: the functions that access dictionaries such as by adding or removing elements return new dictionaries rather than modifying a single shared structure. However, the efficiency of the map is quite comparable to Java’s HashMap.

The template for the `map` type is:

all k,v ~~ equality[k] |: map[k,v]

Notice that there is an implied constraint here: the `map` assumes that the keys in the map can be compared for equality.

A `map` value can be written using the sequence notation, using tuple pairs for the key-value pairs:

map of [(1,"alpha"),(2,"beta”)]

Dictionaries also have a special variant of the sequence notation; instead of writing the pairs as tuples we can use an arrow notation for `map` terms:

map of [1->"alpha", 2->"beta"]

Dictionaries also have their own special variant of a *query search condition*. A condition of the form

K->V in D

where D is a `map` will be satisfied if there is a key/value pair in D corresponding to K and V. For example, the condition:

K->V in map of [1->"alpha", 2->"beta"] && V=="alpha"

is satisfied for only one pair of K and V: namely 1 and "alpha" respectively.

For the curious, dictionaries are implemented using techniques similar to Ideal Hash Trees, as described by Bagwell [#Bagwell01idealhash]. This results in a structure with an effective O(1) cost for accessing elements *and* for modifying the `map` -- all the while offering an applicative data structure.

### The set type

There are many instances where a programmer needs a collection but does not wish specify any ordering or mapping relationship. The standard set type allows you to construct such entities.

Using a set type offers the programmer a signal that minimizes assumptions about the structures: the set type is not ordered, and offers no ordering guarantees. It does, however, offer a guarantee that operations such as element insertion, search and set operations like set union are implemented efficiently.

Like `map`, the set type is not publicly defined using an algebraic type definition: its implementation is private. It’s type is given by the template:

all t ~~ equality[t] |: set[t]

### The range type

The range type is a very particular form of collection type: a range denotes a range of numbers. Its type description says it all:

all t ~~ arithmetic[t],comparable[t] |: range[t] ::= range(t,t,t).

This type description has some special features; in particular it is a constrained type: a type expression of the form:

range[T]

is only valid if T is an arithmetic type; specifically a type that supports arithmetic and is comparable. Thus a type expression such as range[integer] is fine, but range[list[integer]] will result in a syntax error!

Range terms are used to compactly represent regular ranges of numbers; for example the term

range(0,100,1)

denotes the first 100 integers. But, of course, we have already seen the range collection in our exploration of the [sieve of Eratosthenes](file:///Users/fgm/Projects/Cafe/Docs/Book/original-sieve).

There is a specific property of the range type that is difficult to capture with this type definition -- specifically, we rely in many places on ranges *half closed* property: that is, the range of numbers in the range include the first number but does not include the second. This property makes combining ranges much smoother than either a fully closed range (includes both ends) or an open range (includes neither end).

For example the following assertion is expected to hold for range terms:

range(F,I,Ix)++range(I,T,Ix)=range(F,T,Ix)

where ++ is the standard function -- i.e., is part of the concatenate contract -- for expressing sequence concatenation.

## Queries

Consider, if you will, the problem of finding a set of grandparent-grandchild pairs -- given information about parent-child relationships. For example, suppose that we had a list of pairs - each pair indicating a parent and child:

parent:list[(string,string)].

parent = [("john","peter), ("peter","jane"), ... ].

and that we wanted to construct a result list -- also pairs -- along the lines of:

GC:list[(string,string)].

GC = [("john","jane"),...].

Computing the list grandparent/grandchildren pairs involves searching the parents for pairs  that satisfy the grandparent relationship. This involves a double iteration: each pair in the parents list might represent the upper or lower half of a grandparent/grandchild relationship. Based on the collection operators we have seen so far, we can build such a search using two leftFold operations:

leftFold(

(SoFar,(X,Z)) => leftFold(

let {

acc:(list[(string,string)],(string,string))=>list[(string,string)].

acc(gp1,(ZZ,Y)) where Z==ZZ => [gp1..,(X,Y)].

acc(gp1,_) => gp1.

} in acc,

SoFar,parent),

list of [],

parent)

This, rather intimidating,[^fn1] expression uses one leftFold to look for the grandparent, for each candidate grandparent a second leftFold finds all the grand-children. All without any explicit recursion.

The acc function defined above in the let expression implements the logic of deciding what to accumulate depending on whether we had found a grandparent or not.

The various filter, fmap and leftFold functions *are* powerful ways of processing entire collections. However, as we can see, they can be difficult to construct and harder to follow; something that is not helped by the occasional need to construct complex functions in the middle, as in this case.

It turns out that **Star** has a special notation that makes this kind of complex computation significantly easier to write and comprehend. **Star**’s query notation is a very high level way of expressing combinatorial combinations of collections. We can write the equivalent of the previous grandparent expression in this query notation as:

GC = list of { all (X,Y) where

(X,Z) in parent && (Z,Y) in parent }

It may not be obvious, but these expressions compute the same values. What should be obvious is that the query is much easier to read and easier to verify that it is correct.

The syntax and style of **Star**’s query notation is similar to SQL’s syntax -- deliberately so.

Specifically, we take SQL’s *relational calculus* subset -- the language of wheres and of boolean combinations. **Star**’s query expressions do not have the equivalent of explicit relational join operators.

There are several variations of query expression, but the most common form is:

*SequenceType* of { *QuantifierTerm* where *Condition* *Modifier* }

where *SequenceType* is any type name that implements the stream contract, *QuantifierTerm* is a form that indicates the form of the result of the query, *Condition* is a condition and the optional *Modifier* is used to signal properties of the result -- such as whether the result is grouped or sorted.

[^fn1]: There are, unfortunately, some functional programmers that revel in complex code expressions like this one. We are not one of them!

### Satisfaction semantics
The foundation for the query notation is the notation for conditions. Conditions are boolean valued -- but they are not always expressions. For example, the first condition for grandparent is that there is a parent; this was expressed using the condition:

(X,Z) in parent

This condition is not evaluated in the way that expressions are normally evaluated -- by *testing* to see if a given pair of X and Z are in the parent collection. Instead, the condition is evaluated by trying to find X and Z that are in the collection. In effect, the condition becomes a search for suitable candidate pairs.

Technically this is called *satisfying* the condition -- to distinguish what is going on from *evaluating* the condition. Of course, satisfying and evaluating are close cousins of each other and amount to the same thing when there is no search involved.

In addition to individual *search conditions* like this, it is also possible to use logical operators -- called *connectives* -- to combine conditions. In the case of our grand-parent query, there is a conjunction; which involves a variable Z that acts as a kind of glue to the two search conditions.

In database parlance the conjunction amounts to an inner join operation; however, it is also simply logical conjunction.

The available connectives include the usual favorites: &&, ||, and +. They also include some less familiar connectives: *> (read as *implies*) and otherwise.

The implies connective is a way of testing complete compliance with a condition; for example, we can define a query capturing the situation that a manager earns more than his/her members by requiring that anyone who works for the manager earns less than they do:

managerOk:(string)=>boolean.

managerOk(M) => (X,M) in worksFor *> X.salary=<M.salary.

Notice that we can use conditions’ satisfaction-oriented semantics outside of query expressions.

### Query quantifiers

The *QuantifierTerm* in a query specifies ‘how many’ answers we want. There are essentially three forms of *QuantifierTerm* -- if we want all the answers then we use a term of the form:

{ all (X,Y) where ... }

On the other hand, if we want a fixed number, then we use:

{ 5 of (X,Y) where ... }

Of course, there might not be five answers, and so this is called a bounded *QuantifierTerm*.

We have only scratched the surface of possibilities of query expressions here. They are, in fact, one of **Star**’s most powerful high-level features.

### Querying for statistical purposes

We wrap up our exposition on collections with an example that highlights how we can combine many of the collection manipulation features; with a specific goal of statistical processing of data.

Statistics is, of course, one of the key application areas of computers in general. However, there is often a substantial gap between the theoretical aspects of statistical processing and the pragmatics of collecting and processing data. We aim to demonstrate **Star**’s power in both areas.

One fecund source of statistics is the web; we can even get statistics about the web. The on-line tool [Web Page Test](http://www.webpagetest.org/) can be used to generate a lot of data about how a browser responds to a website. Furthermore, we can down this data as a file of comma separated values (CSV). The first few lines of this file shows the detail of the data collected:

Date,...,URL,Response Code,Time to Load (ms),...,Bytes In,...

9/12/14,...,/,302,397,...1272,...

There are over 70 columns there. Suppose that we wanted to process this to find out how much time is spent loading Javascript data. Our first step is to introduce the CSV file to **Star** which we can do with a special macro:

import metaCSV.

worksheet{

generateCSV Wpt from "file:WPT_Sample.csv".

...

}

The generateCSV line is a macro that parses the sample CSV file, constructs a type (Wpt) that reflects the entries in the file and a parser that can parse similar CSV files into Wpt entries. The generated Wpt type looks like:

Wpt ::= Wpt{

Date:string.

...

URL:string.

'Response Code':integer.

'Time to Load (ms)':integer.

...

'Bytes In':integer.

...

};

One immediate feature to notice is that some of the field names are quoted. **Star** allows a variable (and by extension a field) identifier to have any characters in them -- so long as the identifier is quoted. This allows us to access ‘foreign’ data structures like this CSV file in a straightforward way.

Given the implicit generation of the type, and of a parser, we can use this to process real data files. For example, we can extract out from the CSV file records containing just the URLs, the file sizes and the load times using a query:

import metaCSV.

worksheet{

generateCSV Wpt from "file:WPT_Sample.csv".

wptData = WptParser("http:www.webpagetool.org/...csv").

extracted = list of { all

{ URL=D.URL

size=D.'Bytes In'

loadTime=D.'Time to Load (ms)'} where

D in wptData}

...

}

We used the generated parser to reach out to the website for the data and to parse the resulting content -- in a single high-powered statement.

A single run of the web page test may involve many separate browser actions -- the purpose of the tool is to test the performance of a website, which needs many trials to achieve statistical significance. So a better organization of the data is to categorize the raw data by URL. We can do this using a group by query:

import metaCSV.

worksheet{

generateCSV Wpt from "file:WPT_Sample.csv".

wptData = WptParser("http:www.webpagetool.org/...csv").

extracted = list of { all

{ URL=D.URL.

size=D.'Bytes In'.

loadTime=D.'Time to Load (ms)'} where

D in wptData} group by ((X)=>X.URL)

...

}

The group by operator takes a collection, and a categorization function, are produces a `map` of collections.

We want to produce average load times and standard deviations of the load times -- to smooth out the vagaries of the Internet. For now, we will assume that we have average and stddev functions that have type signatures:

average:all c,e ~~ stream[c->>e] |:(c,(e=>float))=>float.

stddev:all c,e ~~ stream[c->>e] |:(c,(e=>float))=>float.

I.e., we are assuming functions that take collections, and an ‘accessor’ function, and return the average and standard deviation respectively.

Given these functions, we can compute our statistics using:

import metaCSV.

import stats.

worksheet{

generateCSV Wpt from "file:WPT_Sample.csv".

wptData = WptParser("http:www.webpagetool.org/...csv").

extracted = list of { all

{ URL=D.URL.

size=D.'Bytes In'.

loadTime=D.'Time to Load (ms)'} where

D in wptData} group by ((X)=>X.URL)

ldTime:(Wpt)=>float.

ldTime(D) => D.loadTime::float.

show list of { all (K,average(V,ldTime),stddev(V,ldTime)) where

K->V in extracted }

}

The query condition:

K->V in extracted

is analogous to the regular search condition:

```
D in wptData
```
except that it is used to search within a `map`-based collection. The pattern K->V matches the successive key/value pairs in the `map`.

That is it! The final query should get output along the lines of:

```
...
("/o/oauth2/auth?...", 337.0, 0.0),
("/main-thumb-58380181-100-yqpttun...", 101.18518518519, 24.41468441456),
("/main-thumb-49759239-100-pcgldxn...",93.48148148148, 19.03285636867),
...
```

Notice that most of the remaining complexity in this example related to selecting the right parts of the data to pick up and process.[^fn1]

[^fn1]: A standard deviation of 0.0 is likely a signal that the indicated URL only occurred once in the sample data.

## The 'M' word

### Monads for collections processing

### Parser expression grammars

### A different kind of sequence

## Collecting it together

Collections form an important part of any modern programming language. The suite of features that make up the collections architecture in **Star** consists of a number of data types, contracts and special syntax that combine to significantly reduce the burden of the programmer.

The collections facility amounts to a form of DSL -- Domain Specific Language -- that is, in this case, built-in to the language. We shall see later on that, like many DSLs, this results in a pattern where there is a syntactic extension to the language that is backed by a suite of contracts that define the semantics of the DSL.
