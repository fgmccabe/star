This is guide.info, produced by makeinfo version 6.3 from book.texi.

Copyright (C) 2019 and beyond, Francis G. McCabe

   All rights reserved.
INFO-DIR-SECTION Programming
START-INFO-DIR-ENTRY
* star-guide: (star).		Programming in Star
END-INFO-DIR-ENTRY


File: guide.info,  Node: Top,  Next: Why be a Star programmer?,  Up: (dir)

Star Guide
**********

This is the on-line guide for the *Star* language.

This book gives an introduction into the form and usage of the *Star*
programming language.  However, it is not a definitive reference on the
language; for which the reader is referred to *Note (star)Top:: by F.G.
McCabe.

* Menu:

* Why be a Star programmer?::
* A tour of Star::
* Functional Programming::
* Collections::
* Making choices::
* Boiling the Ocean::

* Concept index::
* Function index::
* List of Syntax Rules::


File: guide.info,  Node: Why be a Star programmer?,  Next: A tour of Star,  Prev: Top,  Up: Top

1 Why be a Star programmer?
***************************

This book is about the programming language called *Star*.  Why, you may
ask, do you need to learn yet another language?  We hope to answer that
question and more in the course of this book.  We also aim to show you
how to program effectively in *Star* to solve real world problems.

* Menu:

* Programming has changed::
* Is Star for you?::
* Design goals for Star::
* About this book::


File: guide.info,  Node: Programming has changed,  Next: Is Star for you?,  Up: Why be a Star programmer?

1.1 Programming has changed
===========================

Many of the world's most popular languages are quite old: C/C++ date
back to the 1970's and 1980's; Java dates back to 1995.  Although it
sometimes does not seem like it; quite a lot has changed since those
days.

* Menu:

* Programs are huge::
* Planning for change::
* Programming safely and effectively::
* Real-time is normal time.::
* This train is leaving the station::
* Technology::


File: guide.info,  Node: Programs are huge,  Next: Planning for change,  Up: Programming has changed

1.1.1 Programs are huge
-----------------------

If your program is 100 lines long, it does not actually matter what
programming language you use to write it in - unless you are learning
programming for the first time.

   However, in practice, many software systems are huge.  It is normal
to develop and to encounter programs measured in 100 Klocs (thousands of
lines of code) and it is not uncommon to encounter software systems
measured in the millions of lines of code.

     NOTE: To put these numbers into perspective, 500KLocs is comparable
     to the expected output of a programmer over their entire career.
     So, these systems necessarily involve many people over many years:
     a team of 10 programmers would take 10 years to write 1M lines of
     code.(1)

   Software development at this scale does not share much with
developing small systems.  Large scale software is invariably a team
effort, spread over multiple years.  It often involves large numbers of
components and multiple sub-systems interacting with each other.
Overall, this is a scenario of staggering heterogeneity and complexity.

   If there is one theme that is common in large code bases is that
tooling is critical: powerful IDEs can make a difference in productivity
measured in factors of 2-5.  However, it is our opinion that language
tooling starts with a sound programming language design: the sounder the
foundations, the taller the structure that can be built on it.

   ---------- Footnotes ----------

   (1) Not including code thrown away due to changes of requirements
over that time period.


File: guide.info,  Node: Planning for change,  Next: Programming safely and effectively,  Prev: Programs are huge,  Up: Programming has changed

1.1.2 Planning for change
-------------------------

Furthermore, _change_ is the dominant fact of life for large systems.
Software systems evolve, grow, are repurposed to meet new objectives,
and are re-implemented to take advantage of new technologies.

   Managing change in code is often left to Source Code Control Systems;
but these systems only address part of the problem.  For example, _code
re-use_ is also a requirement and a challenge for large systems.  In
fact, one could imagine a _three Rs_(1) for software engineering: Reuse,
Repurpose, and Refactor.

   When the code base is over 500 Kloc, the theoretical probability of
being able to re-use existing code is high, the actual probability of
re-use may be very low - software engineers may simply not be able to
find the needle in the code haystack, or, typically, the cost of
re-using code may outweigh the cost of developing from scratch.

   The need to repurpose reflects the fact that requirements change and
that software written for one purpose may be used for something
different.  Finally, even if the requirements dont change, the context
almost certainly will: increasing workload can lead to a need to
refactor existing code to enable it to meet changing needs.

   There is another kind of change that we need to consider: the
evolution of our programming languages and other tools.  A successful
programming language is part of an ecosystem.  That ecosystem consists
of the language itself, together with compilers and related tools.  It
also contains libraries, frameworks and packages.  In many cases the
libraries that are associated with a language are far more important
than the language compiler (e.g., Java's package ecosystem is enormous).

   A well designed language is capable of supporting a large variety of
libraries without running into limitations.  Furthermore, a well
designed language is structured in a way that supports its own evolution
- new requirements can lead to new features being added to the language.

     Managing this evolution may be the hardest aspect of designing
     programming languages!

   ---------- Footnotes ----------

   (1) Originally, _R_eading, w_R_iting and a_R_ithmetic.


File: guide.info,  Node: Programming safely and effectively,  Next: Real-time is normal time.,  Prev: Planning for change,  Up: Programming has changed

1.1.3 Programming safely and effectively
----------------------------------------

At the same time, safety and security are also critical: no-one likes to
have their private information exposed to the bad guys.  Most
main-stream programming languages were designed in an era when safety
was not uppermost in programmers' minds - usually it was performance.
Some seemingly trivial design choices - such as C's conventions for
laying out strings in memory - turn out to have potentially devastating
security implications.

   In addition, systems that are built assuming a shielded execution
environment, behind closed doors as it were, are often actually expected
to perform in the full glare of the Internet.  Hardening programs so
that they stand up to that glare can often dramatically add to the cost
of development - both in time and in money.


File: guide.info,  Node: Real-time is normal time.,  Next: This train is leaving the station,  Prev: Programming safely and effectively,  Up: Programming has changed

1.1.4 Real-time is normal time.
-------------------------------

Many kinds of business are becoming more and more 'real-time': a small -
over the order of milliseconds - slowdown in loading a web page can mean
the loss of 5% or more of revenue for an e-commerce site; an unrented
car, like an unrented hotel room, represents a permanent loss of
business and a competitive disadvantage.

   For the modern programmer, this means that applications must be
engineered from the start to be responsive and multitasking - aspects
that challenge even the most professional of programmers.


File: guide.info,  Node: This train is leaving the station,  Next: Technology,  Prev: Real-time is normal time.,  Up: Programming has changed

1.1.5 This train is leaving the station
---------------------------------------

Perhaps most importantly, we need to be able to do these things _now_ -
time to market is a critical factor in many if not most modern
applications.  Its no good developing the world's best widget if you run
out of 'runway' trying to build it.

   A major bottleneck is the relative poor productivity of most modern
programming languages.  It is simply too hard to produce correct robust
code in languages like C/C++, Python etc.

   Productivity is an issue for individual programmers but is especially
salient for programmer teams.

     Every successful software project involves a team.

   The requirements for team-based development tend to put certain
aspects of programming language design into sharp focus.  For example,
strong types and clear interfaces may be excellent aids for individual
programmers but they are absolutely paramount for team development.

   More generally, in a competitive environment, the only way to
reliably out-perform the competition in reaching the market is to use
radically more productive technology.


File: guide.info,  Node: Technology,  Prev: This train is leaving the station,  Up: Programming has changed

1.1.6 Technology
----------------

The technology platform that programs are written for is also changing.
Just a few decades ago most computers were single-core; nowadays most
computers are multi-core and are capable of significant parallelism.

   Especially spectacular is the parallelism available in modern GPUs;
where a high end graphics processor may have thousands of cores capable
of processing instructions in parallel.  We expect that the days of
personal computers with thousands of cores is not too far in the future.

   Programming parallel machines with conventional languages is an
exercise in frustration.  This is because programming models that worked
in single core computers do not scale well to highly parallel machines.
One of the primary reasons for this is that state - as represented by
the changing values of variables - is _implicit_ in procedural and
object oriented languages.  The implicitness of state is important
because it makes many programs easier to express.  On the other hand,
that implicitness becomes a liability in multi-threaded and parallel
situations where state is no longer so well behaved.

   However, *Star* has adopted some of the recent innovations in that
make dealing with multi-tasking and parallel execution easier.  These
innovations layer on top of basic features such as threading and provide
simpler models of execution than 'conventional' threaded models.
*Star*'s computation expressions combine the best of fork-join queues
and map-reduce frameworks whilst enabling a more normal style of
programming.


File: guide.info,  Node: Is Star for you?,  Next: Design goals for Star,  Prev: Programming has changed,  Up: Why be a Star programmer?

1.2 Is Star for you?
====================

Choosing a programming language - when you actually have a choice - is
highly personal.  Here are some reasons to think about *Star*.

* Menu:

* If you are already a Java (or C#)::
* If you are already a C++ programmer::
* If you are already a functional programmer::


File: guide.info,  Node: If you are already a Java (or C#),  Next: If you are already a C++ programmer,  Up: Is Star for you?

1.2.1 If you are already a Java (or C#)
---------------------------------------

Most OO languages are embracing some of the simpler features of
functional languages.  Even modern Java with its lambda expressions and
stream features represents a nod to the power of functional programming.
However, at the same time, there is a substantial gap in the
capabilities of most OO languages compared to modern functional
programming languages.

   Fundamentally, OO languages revolve around _nouns_ rather than
_verbs_.  Verbs (methods) are relegated to being inside the scope of
some noun (object): they are not first class.  In functional programming
languages, like *Star*, there is more of an balance between nouns and
verbs.

   It is possible to have functions that are about data; it is also
quite straightforward in functional programming languages to have data
structures with functions embedded in them.  In fact, a simple
definition of a _module_ is a record that contains functions in it.

   While OO languages like Java provide excellent _data abstraction_
tools, the same cannot be said for _control abstractions_.  The result
is that OO languages are 'stuck in the 1970s' when it comes to control
abstractions.  However, concepts such as map/reduce, computation
expressions, and continuations bring a rich suite of new control
possibilities that solve important problems in modern programming.

   Similarly, the type systems of languages like Java (or C# or C/C++)
make are not as expressive or sensitive as modern type systems in
functional languages can be.  Professional programmers will recognize a
typical symptom of insufficiently expressive types: lots of casting and
dynamic meta-programming.  But, while powerful, these techniques amount
to giving up on types and their important advantages.  Furthermore,
contrary to many programmers' expectations, a modern type system is
quite capable of dealing statically with scenarios that require dynamic
programming in languages like Java.


File: guide.info,  Node: If you are already a C++ programmer,  Next: If you are already a functional programmer,  Prev: If you are already a Java (or C#),  Up: Is Star for you?

1.2.2 If you are already a C++ programmer
-----------------------------------------

Moving to a language like *Star* is a radically different programming
experience for the C++ programmer.  However, most of the differences are
not because *Star* is a functional programming language.  Rather, it is
because *Star* does not abide by two of C++'s core principles: zero-cost
abstractions and compatibility with C.

   While *Star*'s compiler does not willfully make programs execute more
slowly, strict CPU performance is not the primary driver for design
features.  This shows up in a desire to avoid the so-called premature
optimization problem: of optimizing the wrong feature.

   In the case of *Star* vs C++, the most salient of these is likely to
be the different ways that generic programs are compiled: C++ compiles
generic programs by constructing special case implementations for all
used versions of the generic program; whereas *Star* constructs a single
- type agnostic - implementation of generic functions.

   This affects many aspects of one's day-to-day usage of the language
and its tooling.  One of which is that, for a compiler that has been in
daily use for over 40 years, C++ compilers are still remarkably slow.
C++ binaries can often be very large - typically outclassed in
performance by the Java compiler.

   The compatibility with C requirement has meant that C++ has to
support many programming paradigms (such as pointer arithmetic) that
most modern programming languages - *Star* included - eschew.

   Pointer arithmetic is known to put applications at risk of severe
security problems; and there are many modern safer alternatives (which
C++ does also support).

   Beyond these, you will find *Star* to be a rich and expressive
language; especilly if your focus is on applications rather than systems
programming.


File: guide.info,  Node: If you are already a functional programmer,  Prev: If you are already a C++ programmer,  Up: Is Star for you?

1.2.3 If you are already a functional programmer
------------------------------------------------

You have many choices for functional programming languages that are
excellent.  The author considers two languages that are principal
sources of inspiration for many of the functional features of *Star*:
Haskell and Standard ML (SML) - both of which are excellent; but not
perfect.

   For the functional programmer, the principal benefits of *Star* are
_readability_, _modernity_ and _predictability_.

   One of the major drivers of the design of Haskell and (to a lesser
extent) SML is conciseness.  However, conciseness is not the same as
readability.  In modern software development environments there are many
stakeholders beyond the developer.  Having a language that is easy to
follow by non-technical readers is a major benefit in mixed skill teams.

   Like Haskell, *Star* has a powerful type system.  *Star*'s type
system has many features in common with Haskell's type system - features
that typically go beyond the capabilities of many OO languages.  In
particular, *Star*'s contract system is reminiscent of Haskell's type
classes; and *Star*'s existential and higher-kinded types give
considerable expressive power to the programmer.

   *Star* does not follow all of Haskell's type features; and some type
concepts are rephrased into terminology that is more familiar to
main-stream (sic) programmers.

   Like SML, *Star* has a powerful module system.  However, unlike SML's
functors, *Star* modules are first class values.  This means that there
is no artificial separation between 'ordinary' programs and 'functor'
programs.

   The result is a balanced set of type features that provides
capabilities that scale well from small programs to large systems.

   *Star*'s evaluation is, like that of SML but unlike Haskell, strict.
We believe that that makes it significantly easier to reason about the
actual behavior and performance of programs.  However, *Star* has a rich
set of features that support productive concurrent and parallel
programming - based on a combination of system threads and the features
of Concurrent ML.

   Like SML, *Star* is not a strictly 'pure' language.  This was neither
an accident nor an afterthought.  Computer systems are built to fulfill
purposeful activity (although there may be many times when the actual
purpose is hard to discern).  For example, if I deposit a check into my
bank account, I require that the bank's state is updated to reflect my
new balance: the world has changed as a result of my action.

   However, the converse does not follow: just because the world is
stateful does not mean that all our programs should be needlessly
stateful.  Much, if not most, of a given application program can and
should be crafted in a mathematical style - the merits of functional
programming are very great.

   Overall, the primary rationale in the design of *Star* is to empower
the programmer in making obviously correct programs.


File: guide.info,  Node: Design goals for Star,  Next: About this book,  Prev: Is Star for you?,  Up: Why be a Star programmer?

1.3 Design goals for Star
=========================

*Star* is a multi-paradigm high-level _symbolic_ language.  It is
designed to be scalable, readable, accurate, high performing and
extensible.

   Paradoxically, scalability in a programming language is always about
large and small chunks of code.  Scalability in *Star* is fostered by a
range of elements that facilitate composition, change and re-use:

   * The language is strongly statically typed.  This encourages both
     safety and documentation.  The type system is strong enough that
     there is very limited need to escape the type system.  For example,
     modules can be given a first-class type semantics.  This is
     important because it facilitates programmatic manipulation of
     modules in a safe manner.

   * Programs are defined in terms of rules; for example, functions are
     defined in terms of equations.  Apart from being more readable,
     rules are also a natural unit of change in an evolving system.

     A meta-language based on logical annotations makes it possible to
     build meaningfully connected documentation and facilitates
     processes such as code re-use, issue tracking, and code lifetime
     management.
   * The package system is intrinsically versioned and abstracted away
     from any underlying storage system.

   The syntax of *Star* is oriented towards readability rather than
strict conciseness.  The reason for this is that the programmer is only
one of the stake holders in a given program.  A readable program is one
that is more easily trusted by non-programmers.

   Experience also suggests that readability enhances programmer
productivity also: much of team-based development involves comprehending
and modifying other programmers' code.

   *Star* is a strongly, statically typed language.  The purpose of a
strong type system is to facilitate the communication of intent of the
programmer.  The purpose of static typing is to ensure that the compiler
can rapidly 'fail' incorrect programs without requiring the program to
be run.  Furthermore, static type checking minimizes any run-time
penalty for imposing type constraints.

   Although *Star* is strongly typed, it uses _type inference_ to
eliminate much of the clutter that some type systems impose on the
programmer - which itself is a productivity sink of course.

   Generally, the _stronger_ the type system, the more the language
system can detect errors before programs are run.  In addition, the more
_expressive_ the type system is, the less the temptation to try to
subvert or bypass the type system.

   However, even though it is technically feasible to completely
eliminate type declarations of functions; doing so is in conflict with
some of the other goals behind *Star*.  For example, type declarations
act as a form of documentation; and when there is a type error in your
program, having _no_ explicit type declarations can make tracking the
culprit of the error very difficult.  So all top-level variable
definitions (typically functions) are required in *Star* to have
explicit type _annotations_.(1)

   *Star* has a range of features that make exploiting parallelism
easier to manage.  For example, it has support for _computation
expressions_ and _actors_.  Partitioning an application into different
_agents_ allows programming to follow a more human approach.
Computation expressions allow the programmer to manipulate computations
as easily as they do data values; that in turns greatly eases the
development of parallel and concurrent applications.

   There is no one technology that can solve all problems.  This is as
true for programming as for other domains.  *Star* supports a range of
programming paradigms that allows the developer to 'use the best tool
for the job'.  However, we go beyond this 'swiss army knife' stance and
make it straightforward to extend the language.

   Virtually every non-trivial program can be factored into a
combination of general purpose mechanism and specific policy for
applying the mechanism.  *Star* has powerful self-extension features
that allow programmers to design their own policy structures (a.k.a.
domain specific languages).

   Many of *Star*'s own features - such as its query notation and its
actor notation - are built using these extension mechanisms.

   ---------- Footnotes ----------

   (1) The term _type declaration_ is reserved for defining a new type.
Variable types are defined through _type annotations_.


File: guide.info,  Node: About this book,  Prev: Design goals for Star,  Up: Why be a Star programmer?

1.4 About this book
===================

This book acts as an introduction to the language and to its use.  The
basic features of the language are introduced; however, this is not a
reference manual: it is not intended to be a complete description of the
language.

   That can be found in the *Star* Language Definition.

   Introducing a programming language like *Star* can be a challenge in
presentation.  This is because there is a significant amount of mutual
support between elements of the language.

   Our strategy is to take a layered approach - we start with simple
examples, occasionally skipping over certain aspects of the language
without explanation.  Later chapters focus on deeper, more complex
topics.

   For the most part, examples in the text of the book are executable.
You are encouraged to try to get them running on your own system.

* Menu:

* Getting hold of Star::
* Typographical conventions::
* Acknowledgements::


File: guide.info,  Node: Getting hold of Star,  Next: Typographical conventions,  Up: About this book

1.4.1 Getting hold of Star
--------------------------

The *Star* compiler and run-time is being developed as an open source
project on GitHub.  You can access the source by cloning or downloading
the repository at

     github.com/frankmccabe/star


File: guide.info,  Node: Typographical conventions,  Next: Acknowledgements,  Prev: Getting hold of Star,  Up: About this book

1.4.2 Typographical conventions
-------------------------------

Any text on a programming language often has a significant number of
examples of programs and program fragments.  We show these using a
typewriter-like font, often broken out in a display form:

     P:integer;
     ...

   We use the ... ellipsis to explicitly indicate a fragment of a
program that may not be syntactically correct as it stands.

   As we noted above, *Star* is a rich language with many features.  As
a result, some parts of the text may require more careful reading, or
represent comments about potential implications of the main text.  These
notes are highlighted the way this note is.


File: guide.info,  Node: Acknowledgements,  Prev: Typographical conventions,  Up: About this book

1.4.3 Acknowledgements
----------------------

No-one is an island, and no project of this scale is one person's work.
I have had the great fortune to be able to develop *Star* in the context
of real world applications solving hard problems.  Individuals have also
played a large role; and it can be hard to ensure that all are properly
acknowledged: please forgive any omissions.

   Of particular significance, I would like to thank Michael Sperber for
our many discussions on the finer topics of language design; and for his
not insignificant contributions to the implementation itself.

   I would also like to thank my old colleagues at Starview inc., in
particular Steve Baunach and Bob Riemenschneider who were the world's
first *Star* programmers!  In addition, I would like to thank Michael
Sperber, David Frese and Andreas Bernauer who helped with crucial parts
of the implementation of the concurrency features.  I would also like to
thank Keith Clark, Kevin Cory, Prasenjit Dey, Chris Gray, Mack
Mackenzie, and Kevin Twidle for their help and advice.  I would like to
acknowledge the support of Thomas Sulzbacher who originated the project
and Jerry Meerkatz for keeping the faith.

   Last, but definitely not least, I would like to acknowledge the love
and support of my family; without whom none of this makes sense.


File: guide.info,  Node: A tour of Star,  Next: Functional Programming,  Prev: Why be a Star programmer?,  Up: Top

2 A tour of Star
****************

Our first task is to introduce you to the *Star* language.  It is
difficult to find the right order in which to present a programming
language like *Star*; this is because many of the features are highly
inter-related.  For example, types depend on values which depend on
functions which depend on types!

   Instead, our approach in this book is to take a series of horizontal
slices through the whole language; going progressively deeper as you
become more comfortable with the language.  Each layer represents a
reasonably workable subset of the complete language.

   Since a layered approach means that any given description may be
incomplete or slightly inaccurate, there is a temptation to use footnote
annotations which declare '... but there is also(1) ...'.

* Menu:

* A first Star program::
* Texture::
* Types more types and even more types::
* A tale of three loops::
* A functional loop::
* Contracts and constrained types::
* Actions::
* There is more::

   ---------- Footnotes ----------

   (1) Please forgive these pedantic notes when you see them.


File: guide.info,  Node: A first Star program,  Next: Texture,  Up: A tour of Star

2.1 A first Star program
========================

It is traditional to introduce a new programming language with something
like the hello world example.  Which we will do in a moment.  However,
the factorial function often makes a better first example for functional
programming languages:

     sample.factorial{
       import star.               -- Access standard language features

       public fact : (integer)=>integer.
       fact(0) => 1.              -- base case
       fact(N) where N>0 => N*fact(N-1).
     }

   This is not an executable program per se.; however, it does represent
a more typical source code unit - most programs are built from many
smaller modules which come together to make the application.  This
program is small, but already introduces many of the key elements of the
*Star* language.

   In this module, we see the name of the module - 'sample.factorial' -
an import statement and a function definition - of the 'fact' function.

   Source code can be in any form of textual container.  There is, for
example, no specific requirement that this 'sample.factorial' package be
in a file called 'factorial.star'; although that may be good advice.
Instead, the *Star* system relies on a _catalog based_ system that maps
package names to text entities.  The catalog system also serves as an
anchor point in the version management of *Star* programs.  We will
cover this, and the related repository system for generated artifacts,
in Chapter 4.


File: guide.info,  Node: Texture,  Next: Types more types and even more types,  Prev: A first Star program,  Up: A tour of Star

2.2 Texture
===========

All programming languages can be said to have a particular _style_ or
_texture_.  This is often so strong that it is often possible to
identify a programming language from a single line of source code.  In
the case of *Star*, this line might be:

     public fact : (integer)=>integer.
   which is a _type annotation statement_ declaring the type of the
function 'fact'.

   The 'public' annotation means that the function is exported by this
module and will be available in other modules where the
'sample.factorial' module is imported.

* Menu:

* Lexical style::
* Types::
* Rules::
* Patterns::
* Packages::
* Worksheets::
* String interpolation::


File: guide.info,  Node: Lexical style,  Next: Types,  Up: Texture

2.2.1 Lexical style
-------------------

*Star*’s lexical syntax uses a combination of special operators and
keywords.

   It can be difficult for language designers to decide when to use a
keyword and when to use a special operator.  In the case of *Star* we
use special operators for common elements and keywords when either a
graphical operator would be obscure and/or is not common.

   For example, in the factorial module, we use braces for grouping; but
we also use the 'import' and the 'where' keywords.  The rationale here
is that programmers have become used to seeing braces for grouping
statements;(1) whereas the import and where elements are somewhat rarer.

   Notice that every statement is terminated with a period.  This is one
of those places where a little redundancy can help when building large
programs: the statement terminator is not technically necessary; but it
helps to reduce the scope of error messages.(2)

     NOTE: The precise rule is slightly more nuanced: a period is
     required to terminate a statement; unless the last character of the
     statement is a closing brace - or unless the statement is itself
     the last statement in a brace sequence.(3)

   Another aspect of *Star*’s texture that may not be visible at this
stage is the reliance on an underlying meta-grammar - specifically on an
_operator precedence grammar_.  OPGs are likely already familiar to you:
it is the almost universally used grammar that underpins arithmetic
expressions.  We take the OPG and stretch its use to include the whole
language.  The relevance of this will be apparrent when we look at
_extending_ *Star* with new language features.

   ---------- Footnotes ----------

   (1) Those who remember Algol 60 will understand that braces are not
the only way of grouping statements.

   (2) We use the period rather than the commonly used semi-colon
because *Star* statements are statements, not instructions to perform in
sequence.

   (3) This is one of those somewhat pedantic notes!


File: guide.info,  Node: Types,  Next: Rules,  Prev: Lexical style,  Up: Texture

2.2.2 Types
-----------

*Star* is a strongly, statically typed language.  This means that all
variables and expressions have a single type; and that all type
constraints are enforceable at compile-time.  This is a fairly strong
(sic) statement but it is a key aspect of *Star*’s design - we need
everything to be well typed and we also want to guarantee completeness
of the type system.

   The type annotation statement:
     fact : (integer)=>integer.

   is a statement that declares that the type of 'fact' is a function of
one integer argument and which returns an integer result.

   *Star* encourages, but does not require, most programs to have
explicit type annotations.  The precise rule is a little subtle:
variables whose type are not quantified _may_ have their type
automatically inferred.  For top-level functions, that annotation is
often contiguous in the text; but in other cases that may not be the
case.

   Other variables - like the variable 'N' which is part of the second
recursive equation - do not need type annotations.  This is possible
because underlying the type system is a powerful _type inference_ system
that can actually infer all types.

   The result is that a lot of the 'clutter' that can pervade a strongly
typed language is just not necessary; but the use of explicit type
annotations for top-level definitions provides useful structure and
documentation.

   Note that the requirement is that quantified _definitions_ have
explicit type annotations.  We don’t distinguish functions in any way
here.  In particular, functions which are _not_ generic - for example
lambda functions - do not need type annotations.


File: guide.info,  Node: Rules,  Next: Patterns,  Prev: Types,  Up: Texture

2.2.3 Rules
-----------

In *Star*, most programs are defined using _rules_.  In this case,
'fact' is defined using _equations_.  The equations that make up a
function definition (or any program definition for that matter) are
statements that are written in order.

   Rule-based programs support a _case driven_ approach to programming:
a program is defined in terms of the different cases or situations that
apply.  Using rules to cover different cases allows the programmer to
focus on each case in relative isolation.

   In addition, as we shall see later on, the partitioning of programs
into cases like this is very helpful in supporting large-scale issues
such as code annotations, versioning and life-cycle management.

   *Star* has various kinds of rules, including function definitions,
grammar definitions, variable definitions and type definitions.


File: guide.info,  Node: Patterns,  Next: Packages,  Prev: Rules,  Up: Texture

2.2.4 Patterns
--------------

Patterns are ubiquitous in *Star*: they form the basis of many rules:
including, most importantly, to define equations.  In fact, _all_
variables are introduced by means of patterns.

   A pattern can be viewed as a combination of a test -- does a value
match a particular pattern -- and as a way ( _the_ way in *Star*) of
binding a variable to a value.

   An equation’s pattern defines when the equation is applicable.  The
first equation for 'fact' above:

     fact(0) => 1.
   has a literal pattern on the left hand side of the '=>' operator.
This equation only applies when 'fact' is called with zero as its
argument.

   The pattern in the second equation:
     fact(N) where N>0 => N*fact(N-1).

   has a guard on it -- after the 'where' keyword.  Guards are
additional conditions that constrain patterns.  In this case, the
equation only applies if the argument is greater than zero.

   Any pattern may be qualified with a guard; we could have written the
guard _inside_ the argument pattern:

     fact(N where N>0) => N*fact(N-1).

   We did not because having the guard outside the arguments is neater.

     NOTE: The fact function’s equations are not fully covering: there
     are no cases for fact for negative numbers.  This means that the
     function is _partial_; and if called with a negative number will
     result in a run-time exception.


File: guide.info,  Node: Packages,  Next: Worksheets,  Prev: Patterns,  Up: Texture

2.2.5 Packages
--------------

The normal compilation unit is a _package_.  The sample.factorial
package contains just the function fact, but packages can contain
functions, type definitions, import statements and many other elements
that we will encounter.

   Package names and references to packages do not refer to file names;
package names are symbolic - in general a package name consists of a
sequence of identifiers separated by periods.

   The _catalog_ and _repository_ system explored in Chapter 7 that
supports the language ensures a proper connection between files and
packages.


File: guide.info,  Node: Worksheets,  Next: String interpolation,  Prev: Packages,  Up: Texture

2.2.6 Worksheets
----------------

The other main kind of compilation unit is the _worksheet_.  Worksheets
are a modern replacement for the REPL(1) that you see in many functional
programming languages.

   We say a _modern_ replacement for REPLs because worksheets fit much
better in the typical environment of an IDE.

   A worksheet can be used to implement the infamous hello world example
in just a few lines:

     worksheet{
       show "hello world".
     }

   We can also use a worksheet to display the results of using and
testing our fact function:

     worksheet{
       import sample.factorial.
       show "fact(10) is $(fact(10))".
       assert fact(5) == 120.
     }

   Worksheets are like a combination of a module and the transcript of a
session.  In an IDE, the ideal mode of displaying a worksheet is via an
interactive editor that responds to edit changes by recomputing the
transcript and displaying the results in-line.

   The key features of a worksheet that we will use are the ability to
import packages, define elements, show the results of computations and
define assertions.

   ---------- Footnotes ----------

   (1) Read-Eval-Print-Loop


File: guide.info,  Node: String interpolation,  Prev: Worksheets,  Up: Texture

2.2.7 String interpolation
--------------------------

The expression
     "fact(10) is $(fact(10))"
   is an _interpolated string_ expression.  It means the string
'"fact(10) is $(fact(10))"' with the substring '(fact(10)' replaced by
the value of the expression embedded within.  Interpolated string
expressions are a convenient way of constructing string values; and, via
the use of contracts, are also type safe.


File: guide.info,  Node: Types more types and even more types,  Next: A tale of three loops,  Prev: Texture,  Up: A tour of Star

2.3 Types, more types and even more types
=========================================

In many ways, the defining characteristic of a programming language is
the approach to types.  As we shall see, *Star*’s type system is quite
extensive and powerful; however, simple types are quite straightforward.

   The most basic question to ask about types is

     What is a type?

   There is some surprising variability to the answer to this question;
for example, in many OO languages, types are conflated with classes.
*Star* types are terms - i.e., names - that denote different kinds of
values.

     Type: A type is a term that denotes a collection of values.(1)

   *Star*’s type system can be broken down into a number of dimensions:

   * How legal values of various kinds can be identified with a type;
   * the treatment of type variables and quantifiers; and
   * constraints on types, particularly type variables

   *Star* distinguishes two basic styles of type: so-called _structural_
or transparent types and _nominative_ or opaque types.  A structural
type term echoes the values it models, whereas a nominative type
typically does not.

   For example, the standard type 'integer' is nominative -- its name
gives no hint as to the representation, structure or kinds of values
that are modeled by integer.(2)  However, a nominative type often
indicates some actual entity being modeled - in this case integer
values.  Two nominative types which have different names always denote
distinct values, whereas two structural types that look the same are
actually identical.

* Menu:

* Nominative types::
* Reference Type::
* Structural types::
* Optional values::
* The flavors of equality::

   ---------- Footnotes ----------

   (1) Not a set of values because not all collections of values are
mathematical sets.

   (2) I.e., everything you thought you knew about integers may or may
not apply to the values denoted by integer.


File: guide.info,  Node: Nominative types,  Next: Reference Type,  Up: Types more types and even more types

2.3.1 Nominative types
----------------------

A nominative type is normally defined using an _algebraic type
definition_.  This both introduces a type and defines all the legal
values that belong to the type.  For example, we might introduce a
Person type with the type definition:

     Person ::= noOne
              | someOne{
                  name : string.
                  dob : date.
                }

   This statement tells us that there are two kinds of 'Person': a
'someOne' who has a 'name' and date of birth ('dob') associated with
them; and a distinguished individual we identify as 'noOne'.  The no-one
individual _does not_ have a name or date of birth.

     Notice how the type annotation statement we saw for declaring the
     type of 'fact' is also used for defining the types of fields in the
     'someOne' record.

   We can _make_ a Person value with a labeled record expression:

     S = someOne{
       name = "fred".
       dob = today()
     }

   The equality symbol is used to introduce a new single-assignment
variable.  In this case the variable 'S' is defined to be a 'someOne'
record.

   Recall that names do not always require an explicit type annotation.
In this case we can infer that 'S' is a 'Person' (because 'someOne'
marks it).  Furthermore, we do not need to explicitly give types to the
'name' and 'dob' fields because their type is constrained by the type
declaration for 'Person'.


File: guide.info,  Node: Reference Type,  Next: Structural types,  Prev: Nominative types,  Up: Types more types and even more types

2.3.2 Reference Type
--------------------

An important detail about the 'someOne' record defined above is that the
fields within it are not re-assignable.  If we want to make a variable
reassignable, or if we want to make a field of a record reassignable, we
use a special 'ref' type to denote that.  For example, the type
definition

     employee ::= employee{
       dept : ref string.
       name : string
     }

   allows the 'dept' field within the employee record to be modifiable -
although the employee's name is still fixed.

   Only fields that have a 'ref' type are modifiable in records.  This
is even true when a record is assigned to a reassignable variable.

   A reassignable variable is declared using the ':=' operator:

     N := employee{
       dept := "your department".
       name = "Anon. Y. Mouse"
     }

   Since the variable 'N' is declared as being reassignable, we can give
it a new value:

     N := employee{
       dept := "another".
       name = "some one"
     }

   We can also modify the 'dept' field of 'N':

     N.dept := "new department".
   However, we cannot modify the 'name' field - because it is not
re-assignable within the 'Person' type.

   Notice that the re-assignability of variables and fields does not
inherit: each field or variable is separate.  So, for example, if we
declared a single-assignment variable 'D' to be an employee:

     D = employee{
       dept := "his department".
       name = "Your Name Here"
     }

   then, even though 'D' itself cannot be re-assigned to, the 'dept'
field of 'D' _can_ be altered:

     D.dept := "my department"

* Menu:

* Accessing Reference Variables::


File: guide.info,  Node: Accessing Reference Variables,  Up: Reference Type

2.3.2.1 Accessing Reference Variables
.....................................

The value of a re-assignable variable is accessed using the '!'
operator.  For example:
     D.dept!
   will retrieve the actual department D is assigned to.  The expression
'D.dept' actually means something different - it denotes the _container_
for the department.


File: guide.info,  Node: Structural types,  Next: Optional values,  Prev: Reference Type,  Up: Types more types and even more types

2.3.3 Structural types
----------------------

A structural type is, informally, a type that looks like a value.  For
example, the type

     (integer,string,employee)
   is a _tuple type_ - it denotes the type of a triple of values,
consisting of an 'integer', a 'string' and an 'employee' in this case.
Values of this tuple type are also tuples; for example:
     (3,"fred",employee{name="peter". dept:="sales"})

   *Star* has several forms of structural type, the tuple type is one of
them; others include _record types_ and _function types_.

   We shall see more of these as we introduce the rest of the language.
However, it is worth pausing to ask the question _Why_?

   Briefly, nominative types help the programmer focus on what a value
_denotes_; whereas structural types tend to expose what a value can
_do_.

   For example, the 'employee' type clearly points to what an employee
value is intended to denote (an employee!), but does not help if we want
to know what an employee can do.  On the other hand, the function type
in the annotation:

     f : (integer)=>string

   clearly indicates what one can use 'f' for, but it does not indicate
anything about why you would want to (except, perhaps, to convert an
'integer' to a 'string').

   In summary, use nominative types when you are modeling real world
entities and structural types when the focus is on operations and
structure more than on what the intention is.  In practice, of course,
you will use both in some combination.


File: guide.info,  Node: Optional values,  Next: The flavors of equality,  Prev: Structural types,  Up: Types more types and even more types

2.3.4 Optional values
---------------------

Notice that we identified a special case of 'noOne' in our 'Person'
type.  One reason for including this in a type is to be able to cope
with non-existent people.  However, this approach is not always the most
effective one when modeling situations where a variable or field may not
have a value.

   Explicit null values, as found in Java and similar languages, cause a
great number of problems: for example, null must have a special
universal type; there are many scenarios where it is not possible for a
variable to be null but the compiler must discover those for itself; and
there is often a consequent tendency in defensive programming to test
for null.

   *Star* has no direct equivalent of a global null value.  However, the
standard 'option' type allows the equivalent of selective nullability.
Any variable that might not have a proper value can be marked with the
option type rather than the underlying type.  And you can use 'none' in
those cases to indicate the equivalent of no value.

   So, for example, suppose that a 'Person' might have a 'spouse' -- who
is also a 'Person' -- but is not guaranteed to have one.  Such a type
can be described using:

     Person ::= someOne{
       name : string.
       dob : date.
       spouse : ref option[Person].
     }

   Here we have done two things: we have eliminated the 'noOne' case for
'Person' and we have marked the 'spouse' as being both read-write and
'option'al.

   Someone with no spouse would be written:

     freddy = someOne{
       name = "Freddy".
       dob = today().
       spouse := none
     }

   whereas someone who has a spouse would be written:

     someOne{
       name = "Lisa".
       dob = lastYear.
       spouse := some(johnny)
     }

   Of course, we can record 'freddy'’s marriage to 'lisa' using an
assignment:

     freddy.spouse := some(lisa)

     lisa.spouse := some(freddy)
   although such circular structures should be avoided where possible.


File: guide.info,  Node: The flavors of equality,  Prev: Optional values,  Up: Types more types and even more types

2.3.5 The flavors of equality
-----------------------------

Equality in programming languages is typically a very subtle topic.  The
issues can range from the nature of floating point numbers, the
difference between integers and long values and the multiple potential
concepts of equality for objects.

   Equality in *Star* is always between values of the _same type_ and it
is always _semantic_.  So, for example, an equality condition such as:

     3==3.0
   is not considered type safe -- because '3' is an 'integer' literal
and '3.0' is a 'float' literal.  If you need to compare an integer and a
floating point number for equality you will need to first of all decide
in which type the comparison will be made (integer or floating point
equality) and then _coerce_ the other value into that type:
     3 :: float == 3.0
   is valid(1) excepting, of course, that exact comparison between
floating point numbers is not _stable_.

   This is an important issue because not all integer values can be
represented in a float value and vice-versa.  So, comparing an integer
and a floating point value raises the possibility of spurious accuracy
as a result of losing information.  The intended effect of the coercion
is to make explicit the nature of equality being relied on.

   The second principle is that equality is semantic.  What that means
is that the '==' symbol is the name of a boolean-valued function.  The
precise type of '==' is quite interesting, we shall, however, leave it
to later when we have covered some of the core type features around
contracts.

   In effect, equality is _not_ considered to be privileged; and it is
definable by the programmer -- albeit with some important useful default
implementations.

   ---------- Footnotes ----------

   (1) The expression '3::float' is a coercion expression that converts
the integer '3' into a float value.


File: guide.info,  Node: A tale of three loops,  Next: A functional loop,  Prev: Types more types and even more types,  Up: A tour of Star

2.4 A tale of three loops
=========================

Imagine that your task is to add up a list of numbers.  Sounds simple
enough: in most procedural or OO languages (such as Java) one would
write a fragment of code that looks like:

     int total = 0;

     for(Integer ix:L)
       total += ix;
   However, this code is also full of pitfalls.  For one thing we have a
lot of extra detail in this code that represents additional commitments
beyond those we might be comfortable with:

   * we have had to fix on the type of the number being totaled;
   * we had to know about Java’s boxed v.s.  unboxed types; and
   * we had to construct an explicit loop, with the result that we
     sequentialized the process of adding up the numbers.

   We can also write an equivalent loop in *Star*:

     total = valof do{
       tot := 0;
       for ix in L do
         tot := tot+ix;
       return tot
     }

   The valof/return combination is a neat way of segueing from the
'world of expressions' into the 'world of actions'.

   This program is essentially equivalent to the Java loop; although
there are some subtleties about the nature of valof/valis that go beyond
Java.  As a result, it has similar architectural issues.

   While one loop is not going to hurt anyone; real code in languages
like Java typically has many such loops.  Especially when nesting loops
to any depth, such code quickly becomes impossible to follow.


File: guide.info,  Node: A functional loop,  Next: Contracts and constrained types,  Prev: A tale of three loops,  Up: A tour of Star

2.5 A functional loop
=====================

A more idiomatic way of expressing a computation like the totalizer is
to use a function.  For example, we can write:

     let{
       total:(cons[integer])=>integer.
       total(nil) => 0.
       total(cons(E,L)) => total(L)+E
     } in total(L)
   while short, this code too has some of the same drawbacks as the for
iteration.

   The type expression 'cons[integer]' refers to the standard type of
'cons lists'.  Similarly, 'nil' refers to the empty list and 'cons(E,L)'
refers to the list obtained by prepending 'E' to the list 'L'.  We will
explore this in more detail in *note Functional Programming::.

   Even if it is more declarative, there is still a lot of extra detail
and architectural commitments here -- like the commitment to 'cons'
lists and the commitment to integers.  These result in a function that
is needlessly restricted.

   Like other functional languages, *Star* has a range of higher-order
operators that may come to the rescue.  For example, we can avoid the
explicit recursion altogether by using 'leftFold':

     leftFold((+),0,L)
   where 'leftFold' means

     apply a left associative accumulating function to the elements of
     the data, assuming that the applied operator is left associative.

   This expression is clearly both more concise and higher-level than
either the explicit loop or the explicit recursion; and it begins to
illustrate the productivity gains that are potentially available to the
functional programmer.

   Using 'leftFold' means that we can often abstract away the machinery
of loops and recursion completely -- instead we can solve the problem at
a more holistic level.  This is one of the hallmarks of functional
programming - it is possible to eliminate many instances of explicit
loops and recursions.

* Menu:

* A totalizer query::
* The homunculus in the machine::


File: guide.info,  Node: A totalizer query,  Next: The homunculus in the machine,  Up: A functional loop

2.5.1 A totalizer query
-----------------------

While concise, expressions involving much use of 'leftFold' (and the
analogous 'rightFold') can be difficult to follow.  An even clearer way
of adding up numbers is to use a _query expression_:

     {fold X with (+) | X in L}
   This query expression frees us from most of the commitments we
endured before: it can add up the elements of any kind of collection --
not just 'cons' lists -- and it can add up floating point numbers just
as easily as integers.  Finally, we have not had to say exactly how the
numbers should be added up: the language system is free to use a
parallel algorithm for the computation should it be more optimal.

   The query expression is also very close to the natural specification:

     Add up the numbers in L

   *Star*’s query expressions -- which are similar to but also more
expressive than LINQ -- can be used to encapsulate a wide range of such
computations.  We shall look deeper into them when we look at query
rules in *Star*.

     Of course, SQL programmers have long had access to this kind of
     conciseness and declarative expressiveness.  However, SQL is
     constrained by the fact that it is intended to represent queries
     and actions over a very particular form of data -- the relational
     table.


File: guide.info,  Node: The homunculus in the machine,  Prev: A totalizer query,  Up: A functional loop

2.5.2 The homunculus in the machine
-----------------------------------

Programming is often taught in terms of constructing sequences of steps
that must be followed.  What does that imply for the programmer?  It
means that the programmer has to be able to imagine what it is like to
be a computer following instructions.

   It is like imagining a little person -- a homunculus -- in the
machine that is listening to your instructions and following them
literally.  You, the programmer, have to imagine yourself in the
position of the homunculus if you want to write effective programs in
most languages today.

   Not everyone finds such feats of imagination easy.  It is certainly
often tedious to do so.  Using query expressions and other higher-order
abstractions significantly reduces the programmer’s burden -- precisely
by being able to take a more declarative approach to programming.


File: guide.info,  Node: Contracts and constrained types,  Next: Actions,  Prev: A functional loop,  Up: A tour of Star

2.6 Contracts and constrained types
===================================

The concepts of interface and contract are foundational in modern
software engineering.  This is because explicit interfaces make it
substantially easier to develop and evolve systems.  A *Star* contract
goes beyond the traditional concept of interface in important ways: we
do not mark the definition of a type with its implemented contracts and
we allow contracts to involve multiple types.

   A contract defines a collection of signatures and an implementation
provides specific implementations for those functions for a specific
type (or type combination).

   For example, we can imagine a contract for simple four function
calculator arithmetic containing definitions for the basic four
functions of addition, subtraction, multiplication and division:

     contract all t ~~ four[t] ::= {
       plus : (t,t)=>t.
       sub : (t,t)=>t.
       mul : (t,t)=>t.
       div : (t,t)=>t.
     }

   This contract defines -- but does not implement -- the four
calculator functions 'plus', 'sub', 'mul' and 'div'.  All these
functions have a similar type, the type for 'plus' is:

     plus :  all t ~~ four[t] |: (t,t)=>t.

   The clause 'four[t] |:' is a _type constraint_, specifically a
_contract constraint_.  So, these functions are generic (universally
quantified) but the bound type ('t') has the additional constraint that
there must be an implementation for 'four' for 't'.

   The 'four' contract defines a set of functions that can be used
without necessarily knowing the type(s) that are involved.  For example,
we can define the 'double' function in terms of 'plus':

     double(X) => plus(X,X).

The type of 'double' reflects the fact that we are using elements from
the 'four' contract:
     double : all t ~~ four[t] |: (t)=>t.
   I.e., it inherits the same constraint as the function 'plus' has.
There are several kinds of type constraint in *Star*’s type system; but
the _contract constraint_ is the most significant of them.

     Notice that we have to give an explicit type annotation for
     'double'.  The reason is that we want to have it have a quantified
     type.

* Menu:

* Implementing contracts::
* Coercion not casting::


File: guide.info,  Node: Implementing contracts,  Next: Coercion not casting,  Up: Contracts and constrained types

2.6.1 Implementing contracts
----------------------------

Defining a contract is a big step, but it is not generally sufficient to
produce working programs.  If we had a 'worksheet' containing only:

     worksheet{
       contract all t ~~ four[t] ::= {
         plus : (t,t)=>t.
         sub : (t,t)=>t.
         mul : (t,t)=>t.
         div : (t,t)=>t.
       }

       double : all t ~~ four[t] |: (t)=>t.
       double(X) => plus(X,X).

       show double(2)
     }

   we would get a compiler error along the lines of:

     2:integer
       which is not consistent with
       display[t_12] , four[t_12] |: t_12
       because four[integer] not known to be implemented

   This error message is effectively warning us that we have defined the
'four' contract but we have not implemented it.  Until we do, the
program is not complete.  However, if we do supply an implementation of
four over 'integer's:
     worksheet{
       contract all t ~~ four[t] ::= {
         plus : (t,t)=>t.
         sub : (t,t)=>t.
         mul : (t,t)=>t.
         div : (t,t)=>t.
       }

       double : all t ~~ four[t] |: (t)=>t.
       double(X) => plus(X,X).

       implementation four[integer] => {
         plus(x,y) => x+y.
         sub(x,y) => x-y.
         mul(x,y) => x*y.
         div(x,y) => x/y.
       }

       show double(2)
     }

   then everything works as expected.

   Notice that the error message above shows that type t_12 actually has
two type constraints:

     display[t_12] , four[t_12] |: t_12

   This is because the 'show' action also results in a type constraint
being involved.  The 'display' contract is used to display values in a
number of circumstances; including the string formatting we saw above.

   As may be expected, arithmetic itself is also mediated via the
arithmetic contract in *Star*.  This is how we can support multiple
numeric types using a common set of operators: there are standard
implementations of arithmetic for integers, and floating point numbers.


File: guide.info,  Node: Coercion not casting,  Prev: Implementing contracts,  Up: Contracts and constrained types

2.6.2 Coercion, not casting
---------------------------

*Star* does not support automatic type casting, as found in languages
like Java and C/C++.  This is for many reasons, not the least of which
is safety and predictability of code.

   Casting in many languages is really two kinds of operations-in-one
which we can refer to as _casting_ and _coercion_.  Casting is mapping
of a value from one type to another without changing the value itself;
and coercion involves converting a value from one type to another.

   For example, the Java cast expression:
     (Person)X
   amounts to a request to verify that 'X' is actually a 'Person'
object.  In particular, this only checks the value of 'X' to see if it
is a 'Person'.  On the other hand, casting an integer to a floating
point number involves changing the value to conform to the floating
point representation.

   *Star* does not support casting, but does support coercion.  However,
coercion in *Star* is never silent or implicit - as it can be in Java
and C/C++.  An expression of the form:
     3+4.5
   will fail to type in *Star* - because there is an attempt to add an
integer to a floating point number.

   The reason for signaling an error is strongly related to safety and
predictability: automatic conversion of integers to floating point can
be a common source of errors in languages like C - because such
coercions are not always guaranteed to be semantics preserving (not all
integers can be represented as floating point values).  The implicit
coercion of numeric values is easy to miss when reading arithmetic
expressions.

   *Star* provides a coercion notation that allows programmers to be
precise in their expectations:

     (3 :: float)+4.5
   denotes the explicit coercion of the integer '3' to a 'float' and
type checks as expected.

   In fact, type coercion in *Star* is mediated via a contract and this
expression is equivalent to
     (_coerce(3):float)+4.5
   where '_coerce' is defined in the 'coercion' contract involving two
types:

     contract all s,t ~~ coercion[s,t] ::= {
       _coerce :: (s)=>t
     }

   The 'coercion' contract is an interface, but has no analog in most OO
languages: it involves two types - the source type and the destination
type.  Each implementation of coercion specifies both types.  For
example, the implementation of coercion between integers and floating
point is explicitly given:
     implementation coercion[integer,float] => { ... }
   This statement gives the implementation for coercing integers to
floats.  Other implementation statements give the definitions for other
forms of coercion.

   Having coercion as a contract makes it straightforward to add new
forms of coercion.  This is used quite extensively within *Star* itself:
for example, parsing JSON can be viewed as coercion from string values
to 'json' values.  Thus the interface to parsers can be standard across
all types and parsers.


File: guide.info,  Node: Actions,  Next: There is more,  Prev: Contracts and constrained types,  Up: A tour of Star

2.7 Actions
===========

*Star* is primarily intended to be a functional programming language.
The preferred phrase is _functional first_.  However, even in functional
programming, sequence can be important.

   The way that we handle sequential computation is a little
idiosynctratic: actions are entities that are created separately from
being performed.  This is based on the standard 'execution' contract(1)

   The basic concept is that one uses expressions to build up an action
entity and then evaluates that entity - and often also extract a
returned value from the computation.

   Actions can be constructed from primitive elements, but the normal
method is to use the 'do' notation.  For example, this 'do' expression
can be used to compute factorial (somewhat laboriously):

     AA = do{
         Cx := 1;
         Fx := 1;
         while Cx!=<Lx do {
           Fx := Fx! * Cx!;
           Cx := Cx! + 1
         };
         return Fx!
       }

   Not all 'do' expressions will involve using re-assignable variables,
but nearly all use of re-assignable variables is in the context of 'do'
expressions.

   It is important to note that a 'do' expression does not directly
compute its value: instead, it is an expression that _denotes_ a
computation.  So, 'AA' above is not an 'integer' but a computation that
will produce an integer.

   To run a computation and to get its value we use the 'valof'
operator:

     FF = valof AA

   There are actually several variants of computation expressions, all
based on the same fundamental structure; two of the most important are
the 'action' expression and the 'task' expression.

   'action' expressions are used to represent normal sequential
computations and 'task' expressions are used to represent concurrent
computations; both of which we will explore later on in the book.

   ---------- Footnotes ----------

   (1) Which is actually a richer version of the 'monad' contract.


File: guide.info,  Node: There is more,  Prev: Actions,  Up: A tour of Star

2.8 There is more
=================

As we have noted, *Star* is a rich language and it would be impossible
to try to cover it in a short introduction.  Later chapters will look at
some of the other features such as a deeper look at contracts, queries &
query rules, actors, concurrency, existential types, and extending
*Star* with domain specific languages.  The next chapter (*Note
Functional Programming::) starts this process by looking at functional
programming in *Star*.


File: guide.info,  Node: Functional Programming,  Next: Collections,  Prev: A tour of Star,  Up: Top

3 Functional Programming
************************

There is a perception of functional programming that it is _weird_ and
_difficult_.  This is unfortunate for a number of reasons; the most
important being that functional programming is _not_ weirder than
procedural programming and that all programmers can benefit by
programming functionally.

   As for being difficult, a more accurate description would be that
there is a deeper _range of features_ in functional programming than in
most modern programming languages: so a perception of complexity can
arise simply because there is more to say about functional programming
languages.  However, the simplest aspects of functional programming are
very simple and the ramp need not be steep.

   What may be surprising to the reader who is not familiar with
functional programming is that it is _old_: predating the origins of
modern computing itself, that there is a huge amount that can be
expressed functionally, and that functional programming is often at
least as efficient and sometimes more efficient than procedural
programming.

   In this chapter we will show how we can utilize *Star* as a vehicle
for functional programming.  As a side-goal, we also hope to demystify
some of the language and ideas found in functional programming.

* Menu:

* What is functional programming?::
* Basics::
* Another look at types::
* Functions as values::
* Going further::
* Polymorphic arithmetic::
* A word about type inference::
* Are we there yet?::


File: guide.info,  Node: What is functional programming?,  Next: Basics,  Up: Functional Programming

3.1 What is functional programming?
===================================

The foundations of functional programming rest on two principles:

  1. Programs are expressed in terms of functions, where a somewhat
     mathematical view of functions is taken; i.e., functions always
     produce the same output for the same input.

     This is what people mean when they say that functional programs are
     _declarative_.(1)

  2. Functions are values: they can be passed as arguments to functions,
     returned from functions, and they can be put into and retrieved
     from data structures.

   This last principle is what people mean when they refer to functions
as being _first class_ values.  It is also what is meant when we say
that functional programming languages are _higher order_.

   It is worth asking why these two principles are so important.
Although most early programmers were mathematicians; the extreme
constraints imposed by early machines meant that one might not have seen
much evidence of mathematical thinking in those early programs.
However, almost by accident, programming and mathematics share some
attributes: the importance of _composition_ and the power of
_abstraction_.  Functional programming is almost ideally placed to
exploit both of these to the full.

   *Star* is not actually a _pure_ functional programming language;
i.e., it is possible to write programs that violate the declarative
principle.  However, it is a _functional-first_ programming language: a
declarative style is strongly encouraged.  In this book we shall mostly
focus on writing pure functional programs in *Star*.

   ---------- Footnotes ----------

   (1) The term declarative has a technical definition.  But this
captures much of the essence of declarativeness (sic).


File: guide.info,  Node: Basics,  Next: Another look at types,  Prev: What is functional programming?,  Up: Functional Programming

3.2 Basics
==========

It is often easier to introduce functional programming using numerical
examples.  Last chapter we saw, for example, the factorial program.
This is mostly because most programmers are already familiar with
numbers.  Continuing that tradition, here is a function that returns the
sign of a number:

     sign(X) where X<0 => -1.
     sign(0) => 0.
     sign(X) where X>0 => 1.

Each of these equations applies to different situations: the first
equation applies when the input argument is negative, the second when it
is exactly zero and the third when it is strictly positive.  These
represent the three possible cases in the definition of the sign
function.

   A *Star* function may be built from any number of rewrite equations;
however, they must all be contiguous within the same group of
statements.

   Although it is good practice to ensure that equations in a function
definition do not overlap, *Star* will try the equations in a function
definition in the order they are written in.(1)  We could have relied on
this and written 'sign' using:

     sign(X) where X<0 => -1.
     sign(0) => 0.
     sign(X) => 1.

   Sometimes it is important to mark a particular equation as the
_default_ case: i.e., an equation that should be used if none of the
other cases apply:

     sign(X) where X<0 => -1.
     sign(0) => 0.
     sign(X) default => 1.
   An explicitly marked 'default' equation does not need to be the last
equation; but, wherever it is written, default equations are only
attempted after all other equations have failed to apply.

* Menu:

* Functions::
* Order of evaluation::

   ---------- Footnotes ----------

   (1) The _Church Rosser Theorem_ guarantees some independence on the
order of the rewrite equations provided that the different rewrite
equations that make up function definitions do not overlap.  Usually,
however, it is too fussy to _require_ programmers to ensure that their
equations do not overlap; hence the reliance on ordering of equations.


File: guide.info,  Node: Functions,  Next: Order of evaluation,  Up: Basics

3.2.1 Functions
---------------

A function is defined as a sequence of _rewrite equations_ each of which
consist of a _pattern_ and an _expression_.  There are three general
forms of rewrite equations:

     PATTERN => EXPRESSION

   or
     PATTERN where CONDITION => EXPRESSION

   or
     PATTERN default => EXPRESSION

The left hand side of a rewrite equation consists of a pattern which
determines the applicability of the equation; and the right hand side
represents the value of the function if the pattern matches.

     Pattern: A pattern represents a test or guard on an (implicit)
     value.  Patterns can be said to succeed or fail depending on
     whether the value being tested matches the pattern.

   We also refer to a pattern being _satisfied_ when matching a
value.(1)

   The pattern in a rewrite equation is a guard on the arguments of the
function call.  For example, given a call
     sign(34)
   the patterns in the different equations of the 'sign' function will
be applied to the integer value '34'.

   When the pattern on the left hand side of a rewrite equation succeeds
then the equation _fires_ and the value of the expression on the right
hand side of the equation becomes the value of the function.

   There are many kinds of pattern in *Star*; here, we look at three of
the most common kinds of pattern, and in later sections, we look at
additional forms of patterns.

_Variable Pattern_
     A variable pattern is denoted by an identifier; specifically by the
     first occurrence of an identifier.

   A variable pattern always succeeds and has the additional effect of
_binding_ the variable to the value being matched.

   For example, the 'X' in the left hand side of
     double(X) => X+X
   is a variable pattern.  Binding 'X' means that it is available for
use in the right hand side of the equation - here to participate in the
expression 'X+X'.

   The part of the program that a variable has value is called its
_scope_.

   * Variables in rewrite equations always have scope ranging from the
     initial occurrence of the variable through to the whole of the
     right hand side of the equation.
   * Variable patterns are the _only_ way that a variable can get a
     value in *Star*.

   Subsequent occurrences of variables in a pattern are semantically
equivalent to an equality test; specifically a call to the '=='
predicate.  For example, in the equation:

     same(X,X) => true.
   the second occurrence of 'X' is regarded as an equality test; i.e.,
this equation is equivalent to:

     same(X,X1 where X==X1) => true.

   Sometimes, the earlier occurrence of a variable is not in the pattern
itself but in an outer scope.

_Literal Pattern_
     A literal pattern - such as a numeric literal or a string literal -
     only matches the identical number or string.

   Clearly, a literal match amounts to a comparison of two values: the
pattern match succeeds if they are identical and fails otherwise.

   Equality is based on _semantic equality_ rather than _reference
equality_.  What this means, for example, is that two strings are equal
if they have the same sequence of characters in them, not just if they
are the same object in memory.

     NOTE: not all types admit equality: for example, functions are not
     comparable; similarly, implementing equality for circular
     structures is problematic.(2)

   There is no automatic coercion of values to see if they _might_
match.  In particular, an integer pattern will only match an integer
value and will not match a float value - even if the numerical values
are the same.  I.e., there will be no attempt made to coerce either the
pattern or the value to fit.

   This, too, is based on the desire to avoid hard-to-detect bugs from
leaking into a program.

_Guard Pattern_
     Sometimes known as a _semantic guard_, a guard pattern consists of
     a pair of a pattern and a condition:

          PATTERN where CONDITION

   Conditions are boolean-valued and a guard succeeds if both its
pattern matches and if its condition is _satisfied_.

     Note that conditions _may_ bind variables.  This is why we do not
     state that conditions are boolean-valued expressions.

   *Star* has a normal complement of special conditional forms which we
shall explore as we encounter the need.  In the case of the equation:
     sign(X) where X>0
   the guard pattern is equivalent to:
     X where X>0
   We can put guard pattern anywhere that a pattern is valid; and, for
convenience, we can also put them immediately to the left of the rewrite
equation's '=>' operator.

     Any variables that are bound by the pattern part of a guarded
     pattern are _in scope_ in the condition part of the guard.
     Furthermore, any variables that are bound by the condition part of
     the guarded pattern have the same scope as variables introduced in
     the pattern.

   In the pattern above, the variable 'X' will be bound in the variable
pattern 'X' and will then be tested by evaluating the condition 'X>0'.

   ---------- Footnotes ----------

   (1) This terminology originates from Logic - where a formula can be
satisfied (made true) by observations from the world.

   (2) None of the standard *Star* data types are circular in nature.


File: guide.info,  Node: Order of evaluation,  Prev: Functions,  Up: Basics

3.2.2 Order of evaluation
-------------------------

*Star* is a so-called _strict_ language.  What that means is that
arguments to functions are evaluated prior to calling the function.

     We do not specify the order of evaluation of the arguments of the
     function; except that:
        * Arguments are evaluated before entry to the function.
        * The function is evaluated before entry to the function.

   Most programming languages are strict; for two main reasons:

  1. It is significantly easier for programmers to predict the
     evaluation characteristics of a strict language.
  2. It is also easier to implement a strict language efficiently on
     modern hardware.  Suffice it to say that modern hardware was
     designed for evaluating strict languages, so this argument is
     somewhat circular.

   There many possible styles of evaluation order; one of the great
merits of programming declaratively is that the order of evaluation does
not affect the actual results of the computation.

   It may, however, affect whether you get a result.  Different
strategies for evaluating expressions can easily lead to differences in
which programs terminate and which do not.

   One other kind of evaluation that is often considered is _lazy_
evaluation.  Lazy evaluation means simply that expressions are only
evaluated _when needed_.  Lazy evaluation has many potential benefits:
it certainly enables some very elegant programming techniques.

   Essentially for the reasons noted above, *Star* does not use lazy
evaluation; however, as we shall see, there are features of *Star* that
allow us to recover some of the power of lazy evaluation.(1)

   The other dimension in evaluation order relates to the rewrite
equations used to define functions.  Here, *Star* uses an in-order
evaluation strategy: the equations that make up the definition of a
function are tried in the order that they are written - with the one
exception being any default equation which is always tried last.

   ---------- Footnotes ----------

   (1) Even predominantly lazy languages like Haskell have features
which enforce strict evaluation.  It reduces to a question of which is
the _default_ evaluation style.


File: guide.info,  Node: Another look at types,  Next: Functions as values,  Prev: Basics,  Up: Functional Programming

3.3 Another look at types
=========================

Organizing data is fundamental to any programming language.  *Star*'s
data types are organized around the algebraic data type.  In addition,
*Star*'s supports quantified types of several varieties.

* Menu:

* Quantifier types::
* Contract constrained types::
* Algebraic data types::


File: guide.info,  Node: Quantifier types,  Next: Contract constrained types,  Up: Another look at types

3.3.1 Quantifier types
----------------------

A _generic_ type is one which has one or more type variables in it.  For
example, the type expression:
     (x,x)=>boolean
   is such a generic type (assuming that 'x' is a type variable - see
below).

   All type variables must be bound by a quantifier in some enclosing
scope.  If a type variable is not bound within a particular type
expression, it is considered _free_ in that type expression.

   A _quantified type_ is a type that introduces (i.e., binds) a type
variable.  There are two quantifiers in *Star*: a universal quantifier
and an existential quantifier.

   The most common quantifier is the _universal quantifier_ and
universally quantified types correspond closely to generic types in
other languages.

   Universally quantified types are often used to denote function types
and collection types.  For example, the type
     all x ~~ (x,x)=>boolean
   denotes the type of a generic binary function that returns a boolean
value.  The standard type for the equality predicate '==' is similar to
this type.

   A universally quantified type should be read as 'for all possible
values' of the bound variable.  For example, this function type should
be read as denoting functions that:

     for any possible type - 'x' - the function takes two such 'x''s and
     returns a 'boolean'.

   *Star* also supports _existentially_ quantified types - these are
useful for denoting the types of modules and/or abstract data types.
For example, the type expression:

     exists e ~~ { f1 : e }
   denotes the type of a record with a field - 'f1' - which has a type.
The analogous reading for this type expression would be:

     there is a type - 'x' - such that the record has a field - called
     'f1' - of this type.

   As may be guessed, this is kind of obscure: 'f1' has a type, but we
don't know much else about it!

   However, existential types can be very useful, especially in
combination with record types like this one.  We will leave our more
detailed exploration of existential types for later.


File: guide.info,  Node: Contract constrained types,  Next: Algebraic data types,  Prev: Quantifier types,  Up: Another look at types

3.3.2 Contract constrained types
--------------------------------

We noted above that the type of the standard equality predicate was
almost the same as:
     all x ~~ (x,x)=>boolean
   This type denotes a universally quantified function type that can be
applied to arguments of any given type.  However, equality in a normal
programming language is _not_ universal: not all values admit to being
reliably tested for equality.  A great example of such a limitation are
functions - equality between functions is not well defined.(1)

   To capture this, we need to be able to constrain the scope of the
quantifier; specifically to those argument types that do admit equality.

   We can do this by adding a _contract constraint_ to the type - the
constraint states that equality must be defined for the type.  We do
this by prepending an 'equality' constraint clause to the type:
     all x ~~ equality[x] |: (x,x)=>boolean
   The 'equality[x] |:' clause states that the type variable 'x' must
satisfy the 'equality' contract.

   What is implied when we state this?  This is captured in the
definition of the contract itself, in this case:
     contract all t ~~ equality[t] ::= {
       (==) : (t,t) => boolean.
     }
   In effect, the 'equality' contract states that there must be a single
function defined - '==' - that is defined for any type that claims to
satisfy the 'equality' contract.

   If this seems a little circular, it is not.  The 'equality' contract
is effectively saying that the '==' function must be defined for the
type; and we constrain function (and other) types with the 'equality'
contract constraint when we need to ensure that '==' is defined!

   We provide evidence for contracts through the 'implementation'
statement.  This declares that a given type satisfies a contract by
providing implementations for the functions in the contract.

   For example, we can provide evidence that the 'equality' contract
applies to strings using a built-in primitive to actually implement
equality for strings:
     implementation equality[string] => {
       s1 == s2 => _string_eq(s1,s2).
     }
   We shall explore more fully the power of this form of type constraint
in later sections and chapters.  For now, the core concept is that
quantified types can be constrained to allow very precise formulations
of types.

   ---------- Footnotes ----------

   (1) Strictly, not decidable.


File: guide.info,  Node: Algebraic data types,  Prev: Contract constrained types,  Up: Another look at types

3.3.3 Algebraic data types
--------------------------

An algebraic data type definition achieves several things
simultaneously: it introduces a new type into scope, it gives an
enumeration of the legal values of the new type and it defines both
constructors for the values and it defines patterns for decomposing
values.  This is a lot for a single statement to do!

   For example, we can define a type that denotes a point in a
two-dimensional space:
     point ::= cart(float,float).
   This kind of statement is called a _type definition statement_ and is
legal in the same places that a function definition is legal.

   The new type that is named by this statement is point; so, a variable
may have point type, we can pass point values in functions and so on.

   The constructor cart allows us to have expression that allow new
point structures to be made:
     cart(3.4,2.1)
   'cart' is also the name of a _pattern_ operator that we can use to
take apart point values.  For example, the euclid function computes the
Euclidian distance associated with a point:
     euclid:(point)=>float.
     euclid(cart(X,Y)) => sqrt(X*X+Y*Y).
   Of course, this particular 'point' type is based on the assumption
that point values are represented in a cartesian coordinate system.  One
of the more powerful aspects of algebraic data types is that it is easy
to introduce multiple alternate forms of data.  For example, we might
want to support two forms of point: in cartesian coordinates and in
polar coordinates.  We can do this by introducing another case in the
type definition statement:
     point ::= cart(float,float)
             | polar(float,float).
   Of course, our 'euclid' function also needs updating with the new
case:
     euclid:(point)=>float.
     euclid(cart(X,Y)) => sqrt(X*X+Y*Y).
     euclid(polar(R,T)) => R.
   'cart' and 'polar' are called _constructor functions_.  The term
_constructor_ refers to the common programming concept of constructing
data structures.  They are called functions because, logically, they
_are_ functions.

   For example, we might give a type to 'polar':
     polar : (float,float)=>point
   In fact, constructor functions are one-to-one functions.  Variously
known as _free functions_ (in logic), _bijections_ (in Math), one-to-one
functions are guaranteed to have an inverse.  This is the logical
property that makes constructor functions useful for representing data.

   Of course, we are talking of a _logical_ property of constructor
functions.  Internally, when implementing functional languages like
*Star*, the values returned by constructor functions are represented
using data laid out in memory - just like in any other programming
language.

   *Star* actually uses a special type for constructor functions; so the
correct type of 'polar' is given by:
     polar : (float,float)<=>point
   The double arrow representing the fact that constructor functions are
bijections.

   In addition to constructor functions, an algebraic type definition
can introduce two other forms of data: _enumerated symbols_ and _record
functions_.  Enumerated symbols are quite useful in representing
symbolic alternatives.  The classic example of an enumerated type is
'daysOfWeek':
     daysOfWeek ::= monday
                  | tuesday
                  | wednesday
                  | thursday
                  | friday
                  | saturday
                  | sunday.
   Another example is the standard 'boolean' type which is defined:
     boolean ::= true | false.
   Unlike enumerated symbols in some languages, there is no numeric
value associated with an enumeration symbol: an enumerated symbol
'stands for' itself only.  The reason for this will become clear in our
next type definition which mixes enumerated symbols with constructor
functions:
     sTree ::= sEmpty | sNode(sTree,string,sTree)
   In addition to mixing the enumerated symbol ('sEmpty') with the
'sNode' constructor, this type is _recursive_: in fact, this is a
classic binary tree type where the labels of the non-empty nodes are
strings.  (We shall see shortly how to generalize this).

   Whenever you have a recursive type, its definition must always
include one or more cases that are not recursive and which can form the
base case(s).  In that sense, an enumerated symbol like 'sEmpty' plays a
similar role in *Star* as null does in other languages; except that
'sEmpty' is only associated with the sTree type.

   We can use 'sTree' to construct binary trees of string value; for
example:
     sNode(sNode(sEmpty,"alpha",sEmpty),
           "beta",
           sNode(sEmpty,"gamma",sEmpty))
   denotes the tree in:

 [image src="images/sTree.jpg" ]

Figure 3.1: A Binary string Tree

     One of the hallmarks of languages like *Star* is that _every_ value
     has a legal syntax: it is possible to construct an expression that
     denotes a literal value of any type.

   Just as we can define 'sTree' values, so we can also define functions
over 'sTree's.  For example, the 'check' function returns 'true' if a
given tree contains a particular search term:
     check(sEmpty,_) => false.
     check(sNode(L,Lb,R),S) => Lb==S || check(L,S) || check(R,S).
   Here we see several new aspects of *Star* syntax:

   * An empty pattern - marked by '_' - matches anything.  It is called
     the _anonymous pattern_ and is used whenever we don't care about
     the actual content of the data.
   * The '||' disjunction is a _short-circuit_ disjunction; much like
     '||' in languages like Java.  Similarly, conjunction ('&&') is also
     short-circuiting.
   * Functions can be recursive.  *Star* permits _mutual recursion_ just
     as easily: there is no special requirement to order function
     definitions in a program.

   We can use 'sTest' to check for the occurrence of particular strings:
     T = sNode(sNode(sEmpty,"alpha",sEmpty),
               "beta",
               sNode(sEmpty,"gamma",sEmpty)).

     show check(T,"alpha").          -- results in true
     show check(T,"delta").          -- results in false


File: guide.info,  Node: Functions as values,  Next: Going further,  Prev: Another look at types,  Up: Functional Programming

3.4 Functions as values
=======================

The second principle of functional programming is that functions are
first class.  What that means is that we can have functions that are
bound to variables, passed into functions and returned as the values of
functions.  In effect, a function is a legal _expression_ in the
language.  It also means that we can have _function types_ in addition
to having types about data.

   We can see this best by looking at a few examples.  One of the
benefits of passing functions as arguments to other functions is that it
makes certain kinds of parameterization easy.  For example, suppose that
you wanted to generalize 'check' to apply an arbitrary test to each node
- rather than just looking for a particular string.

   We will first of all define our 'fTest' function itself:
     fTest:(sTree,(string)=>boolean)=>boolean.
     fTest(sEmpty,_) => false.
     fTest(sNode(L,Lb,R),F) => F(Lb) || fTest(L,F) || fTest(R,F).
   The substantial change here is that, rather than passing a string to
look for, we pass 'fTest' a boolean-valued function to apply; within
'fTest' we replace the equality test 'Lb==S' with a call 'F(Lb)'.

   Notice that the type annotation for 'fTest' shows that the type of
the second argument is a function type - from 'string' to 'boolean'.

   Given 'fTest', we can redefine our earlier 'check' function with:
     check(T,S) => fTest(T,(X)=>X==S).
   We have a new form of expression here: the _anonymous function_ or
_lambda expression_.  The expression
     (X)=>X==S
   denotes a function of one argument, which returns 'true' if its
argument is the same value as 'S'.

   Interestingly, it would be difficult to define a top-level function
that is equivalent to this lambda because of the occurrence of the
variable 'S' in the body of the lambda.  This is an example of a _free
variable_: a variable that is mentioned in the body of a function but
which is defined outside the function.  Because 'S' is free, because it
is not mentioned in the arguments, one cannot simply have a function
which is equivalent to the lambda as a top-level function.

   Free variables are a powerful feature of functional programming
languages because they have an _encapsulating_ effect: in this case the
lambda encapsulates the free variable so that the 'fTest' function does
not need knowledge of 'S'.

* Menu:

* Functions and closures::
* Let binding::
* Generic types::
* Generic functions::


File: guide.info,  Node: Functions and closures,  Next: Let binding,  Up: Functions as values

3.4.1 Functions and closures
----------------------------

If a function is an expression, what is the value of the function
expression?  The conventional name for this value is _closure_:

_Closure_
     A closure is a structure that is the value of a function expression
     and which may be applied to arguments.

   It is important to note that, as a programmer, you will never 'see' a
closure in your program.  It is an implementation artifact in the same
way that the representation of floating point numbers is an
implementation artifact that allow computers to represent fractional
numbers but which programmers (almost) never see explicitly in programs.

   Pragmatically, one of the important roles of closures is to capture
any free variables that occur in the function.  Most functional
programming languages implement functions using closure structures.
Most functional programming languages (including *Star*) do not permit
direct manipulation of the closure structure: the only thing that you
can _do_ with a closure structure is to use it as a function.

     In the world of programming languages, there is a lot of confusion
     about closures.  Sometimes you will see a closure referring to a
     function that captures one or more free variables.


File: guide.info,  Node: Let binding,  Next: Generic types,  Prev: Functions and closures,  Up: Functions as values

3.4.2 Let binding
-----------------

We noted that it is difficult to achieve the effect of the '(X)=>X==S'
lambda expression with named functions.  The reason is that the lambda
is _not_ defined in the same way that named functions are defined -
because it occurs as an expression not as a statement.  If we wanted to
define a named function which also captures 'S', we would have to be
able to define functions inside expressions.

   There is an expression that allows us to do this: the 'let'
expression.  A 'let' expression allows us to introduce local definitions
anywhere within an expression.  We can define our lambda as the named
function 'isS' using the 'let' expression:
     let{
       isS(X) => X==S
     } in isS
   The region between the braces is a _definition environment_ and
*Star* allows _any_ definition statement to be in such an environment.
We can define check using a let expression:
     check(T,S) => fTest(T,
         let{
           isS(X) => X==S.
         } in isS)
   This is a somewhat long-winded way of achieving what we did with the
anonymous lambda function - we would not normally recommend this way of
writing the 'check' function as it is significantly more complicated
than our earlier version.  However, there is a strong inter-relationship
between anonymous lambdas, let expressions and variable definitions.  In
particular, these are equivalent:
     let{
       isS = (X) => X==S
     } in isS
   and
     (X)=>X==S
   Apart from being long-winded, the 'let' expression is significantly
more flexible than a simple lambda.  It is much easier within a 'let'
expression to define functions with more than one rewrite equation; or
to define multiple functions.  We can even define types within the let
binding environment.

   Conversely, lambda functions are so compact because they have strong
limitations: you cannot easily define a multi-rewrite equation function
with a lambda and you cannot easily define a recursive function as a
lambda.

   In short, we would use a 'let' expression when the function being
defined is at all complex; and we would use a lambda when the function
being defined is simple and small.

   Assembling functions in this way, either by using anonymous lambdas
or by using 'let' expressions, is one of the hallmarks of functional
programming.


File: guide.info,  Node: Generic types,  Next: Generic functions,  Prev: Let binding,  Up: Functions as values

3.4.3 Generic types
-------------------

What actually makes 'fTest' more constrained than it could be is the
type definition of 'sTree' itself.  It too is unnecessarily restrictive:
why not allow trees of any type?  We can, using the type definition for
'tree':
     all t ~~ tree[t] ::= tEmpty | tNode(tree[t],t,tree[t]).
   Like the original 'sTree' type definition, this statement introduces
a new type: 'tree[t]' which can be read as _tree of something_.  The
name 'tree' is not actually a type identifier - although we often refer
to the tree type - but it is a _type constructor_.

   In an analogous fashion to constructor functions, a type constructor
constructs types from other types.  Type constructors are even
bijections - one-to-one functions from types to types.

     Unlike constructor functions though, type functions play no part in
     the run-time evaluation of programs.

   The identifier 't' in the type definition for 'tree' denotes a _type
variable_.  Again, similarly to regular variables and parameters, a type
variable denotes a single unspecified type.  The role of the type
variable 't' is like a parameter in a function: it identifies the
unknown type and its role.

   The 'tree' type is a universally quantified type.  What that means is
that instead of defining a single type it defines a family of related
types: for example:
     tree[string]
     tree[integer]
     ...

   are 'tree' types.  We can even have trees of trees:
     tree[tree[string]]
   We capture this genericity of the tree type by using a _universal
quantifier_:
     all t ~~ tree[t]
   What this type expression denotes is a set of possible types: for any
type 't', 'tree[t]' is also a type.  There are infinitely many such
types of course.

   The 'all' quantifier is important: as in logic, there are two kinds
of quantifiers in *Star*'s type system: the _universal_ quantifier all
and the _existential_ quantifier exists.

     *Star* uses context to determine whether any given identifier is a
     type variable or a type name.  Specifically, if an identifier is
     bound by a quantifier then it must refer to a type variable.

   The types of the two constructors introduced in the 'tree' type
definition are similarly quantified:
     tEmpty:all t ~~ tree[t].

     tNode:all t ~~ (tree[t],t,tree[t)]<=>tree[t].
   The type 'tree[t]' on the right hand side of 'tEmpty''s type
annotation raises a couple of interesting points:

  1. This looks like a type annotation with no associated definition.
     The fact that the 'tEmpty' symbol was originally introduced in a
     type definition is enough of a signal for the compiler to avoid
     looking for a definition for the name.

  2. The type of a literal 'tEmpty' expression - assuming that no
     further information is available - will be of the form 'tree[t34]'
     where 't34' is a 'new' type variable not occurring anywhere else in
     the program.  In effect, the type of 'tEmpty' is tree of _some
     type_ 't34' where we don't know anything more about 't34'.


File: guide.info,  Node: Generic functions,  Prev: Generic types,  Up: Functions as values

3.4.4 Generic functions
-----------------------

Given this definition of the tree type, we can construct a more general
form of the tree test function; which is almost identical to fTest:(1)
     test:all t ~~ (tree[t],(t)=>boolean)=>boolean.
     test(tEmpty,_) => false.
     test(tNode(L,Lb,R),F) => F(Lb) || test(L,F) || test(R,F).
   and our original string check function becomes:
     check(T,S) => test(T,(X) => X==S)
   The type of check is also more generic:
     check:all t ~~ (tree[t],t)=>boolean
   I.e., 'check' can be used to find any type of element in a tree -
providing that the types align of course.

   Actually, this is not the correct the type for check.  This is
because we do not, in general, know that the type can support equality.
The precise type for check should take this into account:
     check:all t ~~ equality[t] |: (tree[t],t) => boolean

   ---------- Footnotes ----------

   (1) This time too, we must use an explicit type annotation.


File: guide.info,  Node: Going further,  Next: Polymorphic arithmetic,  Prev: Functions as values,  Up: Functional Programming

3.5 Going further
=================

Although better than the original 'sTest' program there is still one
major sense in which the test program is not general enough.  We can see
this by looking at another example: a function that counts elements in
the tree:
     count:all t ~~ (tree[t]) => integer.
     count(tEmpty) => 0.
     count(tNode(L,_,R)) => count(L)+count(R)+1
   This code is very similar, but not identical, to the 'test' function.

   The issue is that 'test' is trying to do two things simultaneously:
in order to apply its test predicate to a binary tree it has to
implement a walk over the tree, and it also encodes the fact that the
function we are computing over the tree is a boolean-value function.

   We often need to do all kinds of things to our data structures and
writing this kind of recursion over and over again is tedious and error
prone.  What we would like to do is to write a single _visitor_ function
and specialize it appropriately when we want to perform a specific
function.

   This principle of separating out the different aspects of a system is
one of the core foundations of good software engineering.  It usually
goes under the label _separation of concerns_.  One of the beautiful
things about functional programming is that it directly supports such
good architectural practices.

   Since this visitor may be asked to perform any kind of computation on
the labels in the tree we will need to slightly generalize the type of
function that is passed to the visitor.  Specifically, the type of
function should look like:
     F : (a,t)=>a
   where the 'a' input represents accumulated state, 't' represents an
element of the tree and the result is another accumulation.

   Using this, we can write a 'tVisit' function that implements tree
walking as:
     tVisit:all a,t ~~ (tree[t],(a,t)=>a,a)=>a.
     tVisit(tEmpty,_,A) => A.
     tVisit(tNode(L,Lb,R),F,A) => tVisit(R,F,F(tVisit(L,F,A),Lb)).
   Just as the accumulating function acquires a new 'state' parameter,
so the 'tVisit' function also does.  The 'A' parameter in the two
equations represents this accumulated state.

   The second rewrite equation for 'tVisit' is a bit dense so let us
open it out and look more closely.  A more expanded way of writing the
'tVisit' function would be:
     tVisit(tEmpty,_,A) => A.
     tVisit(tNode(L,Lb,R),F,A) => let{
       A1 = tVisit(L,F,A).
       A2 = F(A1,Lb).
       } in tVisit(R,F,A2)
   where 'A1' and 'A2' are two local variables that represent the result
of visiting the left sub-tree and applying the accumulator function
respectively.  We have used the let expression form to make the program
more obvious, rather than to introduce new functions locally; but this
is a legitimate role for let expressions.

   The 'tVisit' function knows almost nothing about the computation
being performed, all it knows about is how to walk the tree and it knows
to apply functions to labels in the tree.

   Given 'tVisit', we can implement our original 'check' and 'count'
functions as one-liners:
     check(T,S) => tVisit(T,(A,X)=>(A || X==S),false).
     count(T) => tVisit(T,(A,X)=>A+1,0).
   The lambda that is embedded in the definition of check bears a little
closer scrutiny:
     (A,X)=>(A || X==S)
   In this lambda, we return 'A' - if it is 'true' - or we return the
result of the test 'X==S'.  This is a common pattern in such programs:
the accumulator 'A' acts as a kind of state parameter that keeps track
of whether we have already found the value.

     Functional programs are not actually _state-free_; often quite the
     opposite.  However, the state in a functional program is never
     _hidden_.  This is the true distinction between functional and
     regular procedural programs.

   Notice that we have effectively hidden the recursion in the function
definitions of 'check' and 'count' - all the recursion is encapsulated
within the 'tVisit' function.

     One of the unofficial mantras of functional programming is _hide
     the recursion_.

   The reason we want to hide recursions that this allows the designer
of functions to focus on _what_ is being computed rather than focusing
on the structure of the data and, furthermore, this allows the
implementation of the visitor to be _shared_ by all users of the tree
type.

   Notice that, while 'a' and 't' are type variables, we did not put an
explicit quantifier on the type of 'F'.  This is because the quantifier
is actually put on the type of 'tVisit' instead:
     tVisit:all a,t ~~ (tree[t],(a,t)=>a,a)=>a
   Just like regular variables, type variables have scope and points of
introduction.  Also like regular variables, a type variable may be
_free_ in a given type expression; although it must ultimately be
_bound_ by a quantifier.

* Menu:

* Going even further::


File: guide.info,  Node: Going even further,  Up: Going further

3.5.1 Going even further
------------------------

We have focused so far on generalizing the visitor from the perspective
of the tree type.  But there is another sense in which we are still
_architecturally entangled_: from the perspective of the 'check' and
'count' functions themselves.

   In short, they are both tied to our 'tree' type.  However, there are
many possible collection data types; *Star* for instance has some 5 or 6
different standard collection types.  We would prefer not to have to
re-implement the 'check' and 'count' functions for each type.

   The good news is that, using contracts, we can write a single
definition of 'check' and 'count' that will work for a range of
collection types.

   Let us start by defining a contract that encapsulates what it means
to visit a collection:
     contract all c,t ~~ visitor[c->>t] ::= {
       visit:all a ~~ (c,(a,t)=>a,a)=>a
     }
   This 'visitor' contract defines a single function that embodies what
it means to _visit_ a collection structure.  There are quite a few
pieces here, and it is worth examining them carefully.

   A contract header has a template that defines a form of _contract
constraint_.  The clause
     visitor[c ->> t]
   is such a constraint.  The sub-clause
     c ->> t
   refers to two types: 'c' and 't'.  The presence of the '->>' term
identifies the fact that 't' _depends_ on 'c'.

   The 'visitor' contract itself is about the collection type 'c'.  But,
within the contract, we need to refer to both the collection type and to
the type of elements in the collection: the 'visit' function is over the
collection, it applies a function to elements of the collection.

   Furthermore, as we design the contract, we _do not know_ the exact
relationship between the collection type and the element type.  For
example, the collection type may be generic in one argument type - in
which case the element type is likely that argument type; conversely, if
the type is _not_ generic (like 'string' say), then we have no direct
handle on the element type.

   We _do know_ that within the contract the element type is
_functionally determined_ by the collection type: if you know the
collection type then you should be able to figure out the element type.

   We express this dependency relationship with the the 'c ->> t' form:
whatever type 'c' is, 't' must be based on it.

   The body of the contract contains a single type annotation:
     visit:all a ~~(c,(a,t)=>a,a)=>a
   This type annotation has three type variables: the types 'c' and 't'
come from the contract header and 'a' is local to the signature.  What
the signature means is

     Given the visitor contract, the 'visit' function is from the
     collection type 'c', a function argument and an initial state and
     returns a new accumulation state.

   It is worth comparing the type of 'visit' with the type of 'tVisit':
     tVisit:all t,a ~~(tree[t],(a,t)=>a,a)=>a
   The most significant difference here is that in 'tVisit' the type of
the first argument is fixed to 'tree[t]' whereas in 'visit' it is left
simply as 'c' (our collection type).

   Given this contract, we can re-implement our two 'check' and 'count'
functions even more succinctly:
     check(T,S) => visit(T, (A,X)=>A || X==S,false)
     count(T) => visit(T, (A,X)=>A+1,0)
   These functions will apply to _any_ type that satisfies - or
implements - the 'visitor' contract.  This is made visible in the
revised type signature for 'count':
     count:all c,t ~~ visitor[c->>t] |: (c)=>integer.
   This type is an example of a _constrained type_.  It is generic in
'c' and 't' but that generality is constrained by the requirement that
the 'visitor' contract is appropriately implemented.  The eagled-eyed
reader will notice that 'count' does not actually depend on the type of
the elements in the collection: this is what we should expect since
'count' does not actually care about the elements themselves.

   The type signature for 'check', however, does care about the types of
the elements:
     check:all c,t ~~
       visitor[c->>t], equality[c] |: (c,t)=>boolean
   This type annotation now has two contract constraints associated with
it: the collection must be something that is visitable and the elements
of the collection must support 'equality'.

   Given the work we have done, we can implement the 'visitor' contract
for our 'tree[t]' type quite straightforwardly:
     implementation all t ~~ visitor[tree[t]->>t] => {
       visit = tVisit
     }
   Notice that header of the implementation statement provides the
connection between the collection type (which is 'tree[t]') with the
element type ('t').  The clause
     visitor[tree[t]->>t]
   is effectively a declaration of that connection.

   Now that we have disconnected 'visit' from 'tree' types, we can
extend our program by implementing it for other types.  In particular,
we could also implement the visitor for the 'sTree' type:
     implementation visitor[sTree ->> string] => {
       visit = sVisit
     }
   however, we leave the definition of 'sVisit' as a simple exercise for
the reader.

   Our final versions of 'count' and 'check' are now quite general: they
rely on a generic implementation of the 'visit' function to hide the
recursion and are effectively independent of the actual collection types
involved.

   If we take a second look at our visitor contract we can see something
quite remarkable: it counts as a definition of the famous _visitor
pattern_.  This is remarkable because, although visitor patterns are a
common design pattern in OO languages, it is often hard in those
languages to be crisp about the semantics of visiting; in fact, they are
called patterns because they represent patterns of use which may be
encoded in Java (say) whilst not necessarily being definable in them.

   The combination of contract and implementation represents a quite
formal way of defining patterns like the visitor pattern.

   There is something else here that is quite important too: we are able
to define and implement the visitor contract _without_ having to modify
in any way the type definition of 'tree' or 'sTree'.  From a software
engineering point of view this is quite important: we are able to gain
all the benefits of interfaces without needing to entangle them with our
types.  This becomes critical in situations where we are not able to
modify types - because they don't belong to us and/or we don't have
access to the source.


File: guide.info,  Node: Polymorphic arithmetic,  Next: A word about type inference,  Prev: Going further,  Up: Functional Programming

3.6 Polymorphic arithmetic
==========================

There are other ways in which programs can be polymorphic.  In
particular, let us focus for a while on arithmetic.  One of the issues
in arithmetic functions is that there are many different kinds of
numbers.  Pretty much every programming language distinguishes several
kinds of numbers; for example, Java distinguishes byte, short, int,
long, float, double, BigInteger and BigDecimal - and this does not count
the wrapped versions.  Other languages have even more choice.

   One question that might seem relevant is why?  The basic answer is
that different applications call for different properties of numbers and
no one numeric type seems to fit all needs.  However, that variety comes
at a cost: when we use numbers we tend to have to make too early a
choice for the numeric type.

   For example, consider the 'double' function we saw earlier:
     double(X) => X+X
   What type should 'double' have?  In particular, what should the type
of '+' be?  Most people would be reluctant to use different arithmetic
operators for different types of numbers.(1)  This is resolved in *Star*
by relying on contracts for the arithmetic operations.

   The result is that the most appropriate type signature for 'double'
is exquisitely tuned:
     double:all t ~~ arith[t] |: (t)=>t
   This type is precisely the most general type that 'double' could
have.  Any further constraints result in making a potentially premature
choice for the numeric type.

   If we take another look at our original 'fact' function:
     fact(0) => 1
     fact(N) => N*fact(N-1)
   this is constrained to be a function from 'integer' to 'integer'
because we introduced the literal integers '0' and '1'.  However, the
'arith' contract contains synonyms for these very common literals.
Using 'zero' and 'one' allow us to be abstract in many arithmetic
functions:
     genFact:all a ~~ arith[a] |: (a)=>a.
     genFact(zero) => one.
     genFact(N) => N*genFact(N-one).
   We call out 'zero' and one for special treatment because they occur
very frequently in numerical functions.  We can introduce other numeric
literals without compromising our type by using _coercion_; although it
is more clumsy:
     factorialC:all t ~~
       arith[t],coercion[integer,t] |: (t)=>t.
     factorialC(N) where N==0::t => 1::t.
     factorialC(N) => N*factorialC(N-1::t).
   The expressions '0::t' and '1::t' are coercions from 'integer' to
't'.

   Of course, coercion is also governed by contract, a fact represented
in the type signature by the coercion contract constraints on the type
of 't'.

   In any case, using these techniques, it is possible to write numeric
functions without unnecessarily committing to specific number types.
That in turn helps to make them more useful.

* Menu:

* Optional computing::
* Special syntax for optional values::

   ---------- Footnotes ----------

   (1) Although some languages - such as SML - do require this.


File: guide.info,  Node: Optional computing,  Next: Special syntax for optional values,  Up: Polymorphic arithmetic

3.6.1 Optional computing
------------------------

There are many situations where it is not possible to guarantee that a
computation will succeed.  The simplest examples of this include
scenarios such as accessing external files; but may also apply to
getting the first element of a list or the label of a 'tree' node.  The
great unknown of accessing elements of a collection is 'is it there?'.
Its not guaranteed of course, and we need to be able to handle failure.

   Many languages employ the concept of a special 'null' value to denote
some of these cases - like a 'someOne' not having a 'spouse'.  However,
the special 'null' value brings its own problems: the type of 'null' is
problematic (it is a legal value for every type) and there are many
situations where 'null' is never possible.

   We address this by handling those situations where failure is
possible differently than where it is not.  Specifically, we do this via
the 'option' type.

   The type definition of 'option' is quite straightforward:
     all t ~~ option[t] ::= none | some(t).
   where 'none' is intended to denote the non-existence of a value and
'some' denotes an actual value.

   The 'option' type is intended to be used in cases where functions are
known to be partial.(1)  An 'option' return type signals that the
function may not always have a value.

   Normal pattern matching can be used to access a value wrapped in a
'some'; for example, to access someone's 'spouse' we can use the
condition:
     isMarriedTo(P,J) where some(JJ).=P.spouse => J==JJ.
     isMarriedTo(_,_) default => false.
   The important detail here is that all access to a 'option' wrapped
value is gated by some form of pattern matching and that, normally, this
takes place in a condition.

     The condition
          some(JJ).=P.spouse
     represents a pattern matching condition: it is satisfied if
     'P.spouse' matches 'some(JJ)' and has the additional effect of
     defining and binding the variable 'JJ' to the matched spouse.

   ---------- Footnotes ----------

   (1) A partial function does not have a value across the whole range
of its arguments.


File: guide.info,  Node: Special syntax for optional values,  Prev: Optional computing,  Up: Polymorphic arithmetic

3.6.2 Special syntax for 'option'al values
------------------------------------------

Of course, the code above _is_ kind of clumsy!  There is a range of
operators in *Star* to make using 'option' values more pleasant.

   The most important of these is the '^=' operator which combines the
'.=' with the 'some' match.  Using this, the 'isMarriedTo' function
becomes:
     isMarriedTo(P,J) where JJ^=P.spouse => J==JJ.
     isMarriedTo(_,_) default => false.
   The meaning of '^=' is similar to the pattern match condition '.=';
except that the pattern is assumed to be for a 'some' value.

   While the '^=' operator(1) is very useful in unpacking an optional
value, the '^|' operator allows us to handle cases where we always need
to be able to give some kind of value.  For example, normally a 'map'
returns none if an entry is not present.  However, a _cache_ is
structured differently: if a value is not present in a cache then we
must go fetch it:

     cacheValue(K) => cache[K] ^| fetch(K)

   We can also apply a match _in line_ to an 'option'al value.  The '^'
operator allows a pattern to be formed by applying a 'option' valued
function directly in place.  For example, the equation:
     head(first^(1)) => "alpha"
   is equivalent to:
     head(X) where some(1).=first(X) => "alpha"
   We will see more examples of this when we look more closely at
sequences and collections processing.

   Finally, we can promote an optional expression into an enclosing
expression (which is therefore also optional).  For example, in the
expression:

     nameOf(^P.spouse)
   if 'P.spouse' is 'none', then the value returned by the 'nameOf'
expression is also 'none', otherwise it takes the form:
     some("P_spouse_name")
   (assuming that 'nameOf' is defined for 'Person's.)

   Overall, the 'option' type is part of an elegant approach to
nullability that is easily incorporated into *Star*'s (and similar) type
system.

   ---------- Footnotes ----------

   (1) Read as 'has a value'.


File: guide.info,  Node: A word about type inference,  Next: Are we there yet?,  Prev: Polymorphic arithmetic,  Up: Functional Programming

3.7 A word about type inference
===============================

We have seen some powerful forms of types in this chapter: recursive
types defined using algebraic type definitions, generic types and even
function types.  Recall also that *Star* only requires programmers to
explicitly declare the types of quantified variables and functions.  It
is worth pausing a little to see how this might be done.

   Recall our original 'fact' function:
     fact(0) => 1.
     fact(N) => N*fact(N-1).
   The compiler is able to compute the types of the various variables
automatically through a process known as _type inference_.  Type
inference may seem magical, but is actually (mostly) quite simple.  Let
us take a look at the expression:
     N-1
   which is buried within the recursive call in 'fact'.  Although it
looks like a special operator, *Star* does not treat arithmetic
expressions in a special way; the '-' function is just a function from
numbers to numbers; its type is given by:(1)
     (-) : all t ~~ arith[t] |: (t,t)=>t
   However, we should simplify this type a little in order to make the
explanation of type inference a little simpler.  In what follows, we
assume that the type of ('-') is:
     (-) : (integer,integer)=>integer
   Type inference proceeds by using special _type inference rules_ which
relate expressions to types, in this case the applicable rule is that a
function application is consistent if the function's parameter types are
consistent with the types of the actual arguments.  If they are
consistent, then the type of the function application is the return type
of the function.

   The type inference process initially gives every variable an unknown
type - represented by a new type variable not appearing anywhere else.
For our tiny 'N-1' example, we will give 'N' the type T_{N}.

   The ('-') function has two arguments whose types can be expressed as
a tuple of types:
     (integer,integer)
   and the types of the actual arguments are also a tuple:
     (T_{N},integer)
   In order for the expression to be type correct, the actual types of
the arguments must be consistent with the expected types of the
function; which we can do by making them _the same_.  There is a
particular process used to do this - called _unification_.

 [image src="images/minustype.png" ]

Figure 3.2: Inferring the Type of N-1

_Unification_
     An algorithm that replaces variables with values in such a way as
     to make two terms identical.

   Unification matches equals with equals and handles (type) variables
by substitutions - for example, we can make these two type expressions
equal by _binding_ the type variable T_{N} to 'integer'.

   We initially picked the type of 'N' to be an arbitrary type variable,
but the process of checking consistency leads us to refine this and make
the type concrete.  I.e., the use of 'N' in a context where an integer
is expected is enough to allow the compiler to infer that the type of
'N' is indeed 'integer' and not T_{N}.

   Of course, if there are multiple occurrences of 'N' then each of
those occurrences must also be consistent with integer; and if an
occurrence is not consistent then the compiler will report an error - a
given expression may only have one type!

   The bottom line is that *Star*'s types are based on a combination of
unification for comparing types and a series of type rules that have the
effect of introducing _constraints_ on types based on which language
features are present in the text.  The type checker is really a
constraint solver: if the constraints are not satisfiable (for example
by trying to 'call' a variable and add a number to it) then there is a
type error in the program.

   The magic of type inference arises because it turns out that solving
these constraints is sufficient for robustly type checking programs.

   A sharp-eyed reader will notice that *Star*'s type system is
different in nature to that found (say) in OO languages.  In *Star*'s
type system, types are considered to be consistent in *Star* if they are
_equal_.  This is quite different to the notion of consistency in OO
languages where an argument to a function is consistent if its type is a
_sub-type_ of the expected type.

   However, we would note that the apparent restriction to the type
system imposed by type equality is much less severe in practice than in
theory - and that OO languages' type systems also often incorporate some
of the same restrictions.

* Menu:

* Why is type inference restricted?::

   ---------- Footnotes ----------

   (1) We put the (-) in parentheses to highlight the use of an operator
as a normal symbol.


File: guide.info,  Node: Why is type inference restricted?,  Up: A word about type inference

3.7.1 Why is type inference restricted?
---------------------------------------

We have stated a few times that *Star*'s type system only infers types
of variables that are _not_ quantified.  In fact, it is fairly
straightforward to build a type inference system that can infer
quantified types.  For example, such a complete type inference system
would infer from these equations:

     conc(nil,x)=>x.
     conc(cons(h,tl),x) => cons(h,conc(tl,x))
   the generalized type for 'conc':
     conc:all t ~~ (cons[t],cons[t])=>cons[t].
   However, several technical and non-technical considerations stay our
hand at building such a type inference system:

   * There are still types that cannot be correctly inferred; and would
     therefore require explicit type annotations to correctly type the
     program.
   * Having explicit type annotations is 'good style' in general and
     definitely aids in debugging type errors.
   * Being explicit about quantification makes unification of quantified
     types simpler and more reliable.

   On the other hand, requiring type annotations for _every_ variable
would be extremely tedious and verbose.  An extreme version of this
policy would require the 'conc' program above to be written:

     conc:all t ~~ (cons[t],cons[t])=>cons[t].
     conc(nil,x:list[t])=>x.
     conc(cons(h:t,tl:list[t]),x:list[t]) => cons(h,conc(t,x))
   The design of *Star* strikes a balance between useability and rigor:
most variables do not require explicit type annotations.  We require
them only when something 'special' is being indicated; one of those
special circumstances is when defining a generic function.

   Even there, there are many situations where explicit type annotations
are not needed: for example when defining a field in a record, or a
function in the implementation of a contract, there already is a type
that the type system can use to verify the program.

   So, the precise rule for type inference is:

     If the type of a variable is known from context, then use that type
     to verify the type of any value the variable may be bound to.
     Otherwise, use type inference on the value to infer a type for the
     variable but do not attempt to generalize it by adding quantifiers
     to the inferred type.

   We are only able to scratch the surface of the type system here.  It
is certainly true that - like many modern functional languages -
*Star*'s type system is complex and subtle.  The primary motivation for
this complexity is to reduce the burden for the programmer: by being
able to infer types automatically, and by being able to address many
programming subtleties, the type system comes to be seen as the
programmer's friend rather than as an obstacle to be 'gotten around'.


File: guide.info,  Node: Are we there yet?,  Prev: A word about type inference,  Up: Functional Programming

3.8 Are we there yet?
=====================

The straightforward answer to this is no.  There is a great deal more to
functional programming than can be captured in a few pages.  However, we
have covered some of the key features of functional programming -
particularly as it applies to *Star*.  In subsequent chapters we will
take a closer look at collections, at modular programming, at
concurrency and even take a pot shot at Monads.

   If there is a single idea to take away from this chapter it should be
that functional programming is natural.  If there is a single piece of
advice for the budding functional *Star* programmer, it should be to
_hide the recursion_.  If there is a single bit of comfort to offer
programmers it should be that _Rome was not built in a day_.

   In the next chapter we look at collections, one of the richest topics
in programming.


File: guide.info,  Node: Collections,  Next: Making choices,  Prev: Functional Programming,  Up: Top

4 Collections
*************

Modern programming - whether it is OO programming, functional
programming or just plain C programming - relies on a rich standard
library.  Given that nearly every program needs to be able to manage
collections of _things_, the central pearl of any standard library is
the _collections_ library.  Recalling our mantra of hiding recursion; a
well designed collections library can make a huge difference to the
programmer's productivity, often by hiding many of the recursions and
iterations required to process collections.

   The collections architecture in *Star* has four main components:
  1. a range of standard collection types - including array-like lists,
     cons lists, first-in first-out queues, and ideal hash trees;
  2. a range of standard functions - mostly defined in contracts - that
     define the core capabilities of functions over collections;
  3. special notations that make programming with collections in a type
     independent way more straightforward; and
  4. the final major component of the collections architecture is
     _queries_.  *Star* has a simple yet powerful set of features aimed
     at simplifying querying collections.

   The query component is sufficiently involved to merit a chapter of
its own.

* Menu:

* Sequence notation::
* Indexing::
* Doing stuff with collections::
* Different types of collection::
* Collecting it together::


File: guide.info,  Node: Sequence notation,  Next: Indexing,  Up: Collections

4.1 Sequence notation
=====================

A sequence is an ordered collection; a sequence expression is an
expression involving a complete or partial enumeration of the values in
the collection.  Star has a straightforward notation for expressing
sequences of any underlying type; for example, a 'cons' sequence of
integers from 1 through 5 can be written:
     cons of [1, 2, 3, 4, 5]
   In situations where we do not know or do not wish to specify the
collection type, we can write instead:
     [1, 2, 3, 4, 5]
   This term - it could be either an expression or a pattern - denotes
the sequence _without_ specifying the underlying collection type.  The
difference in the types of the two terms is telling:
     cons[integer]
   and
     sequence[c->>integer] |: c
   respectively - where 'c' is a type variable.  The first is a concrete
type expression, the second is a constrained type - in this case 'c'
must implement the 'sequence' contract.

   Although the second type expression is longer, and a bit more complex
to read, it is also actually less constraining.  The type expression
'cons[integer]' does not allow for variation of the underlying
collection type; the second type expression allows the term to be used
in contexts that require different concrete types.

* Menu:

* Partial sequence notation::
* The stream and sequence contracts::
* Stream patterns::
* Notation and contracts::


File: guide.info,  Node: Partial sequence notation,  Next: The stream and sequence contracts,  Up: Sequence notation

4.1.1 Partial sequence notation
-------------------------------

The sequence notation also allows for the specification of partial
sequences; this is particularly useful in writing functions that
construct and traverse sequences.  The sequence term:
     [1,2,..X]
   denotes the sequence whose first two elements are '1' and '2' and
whose remainder is denoted by the variable 'X' - which must also be a
sequence of the appropriate type.  Similarly, the term:
     [F..,23]
   denotes the sequence obtained by gluing '23' to the back of the
sequence 'F'.

   There is a strong relationship between the normal sequence notation
and the partial sequence notation.  In particular, the sequence
expression
     cons of [1,2]
   is equivalent to:
     cons of [1,..cons of [2,..cons of []]]
   However, we are not permitted to use both of ',..' and '..,' in the
same expression:
     [F..,2,3,..B]
   is not permitted (since it amounts to a concatenation of two
sequences which, in turn, implies a non-deterministic decomposition when
used as a pattern).

   The major benefit of general sequence notation is that it allows us
to construct programs involving collections that are independent of type
_and_ to do so in a syntax which is concise.

   For example, we can use sequence notation to write functions over
sequences; such as the 'concat' function that concatenates two
sequences:
     concat:all c,e ~~ stream[c->>e], sequence[c->>e] |: (c,c)=>c.
     concat([],X) => X.
     concat([E,..X],Y) => [E,..concat(X,Y)].
   This function will work equally well with 'cons' lists, lists,
strings, even your own collection types.  All that is required is that
there is an implementation of the 'stream' and the 'sequence' contracts
for the actual type being concatenated.

     There are two contracts here: the 'stream' contract is used when
     decomposing sequences and the 'sequence' contract is used when
     building them.


File: guide.info,  Node: The stream and sequence contracts,  Next: Stream patterns,  Prev: Partial sequence notation,  Up: Sequence notation

4.1.2 The 'stream' and 'sequence' contracts
-------------------------------------------

Underlying the sequence notation are two contracts: the 'sequence'
contract and the 'stream' contract.  These contracts contains type
signatures that can be used to construct and to match against sequence
values.  The sequence notation is realized by the compiler translating
sequence terms to a series of calls to those functions.

   The standard 'stream' contract is
     contract stream[t->>e] ::= {
       _eof:()=>boolean.   -- is stream empty
       _hdtl:(t)=>option[(e,t)]. -- match front of stream
       _back:(t)=>option[(t,e)]. -- match back of stream
     }
   and the standard 'sequence' contract is:
     contract stream[t->>e] ::= {
       _nil:t.     -- empty stream
       _cons:(e,t)=>t. -- add to front
       _apnd:(t,e)=>e. -- add to back
     }

   The entries in the 'sequence' contract should be fairly self-evident:

   * '_nil' is the empty sequence;
   * '_cons' is a function that 'glues' a new element to the front of
     the sequence; and
   * '_apnd' appends elements to the *back* of the sequence.

   The compiler uses these three functions to transform sequence
expressions into function calls.

   For example, the sequence expression:
     [1,2,3]
   is transformed into
     _cons(1,_cons(2,_cons(3,_nil())))
   If a sequence expression has an explicit type marker on it, then its
translation is slightly different - to allow the type checker to make
use of the type information.  For example,
     cons of [1,2]
   is translated as:
     _cons(1,_cons(2,_nil())):cons[_]
   This annotation is all that is needed to force the compiler to treat
the result as a concrete cons list.  Type inference does the rest of the
hard work.(1)

   ---------- Footnotes ----------

   (1) The type expression '_' is a special type that denotes an
anonymous type: each occurrence of the type expression denotes a
different unknown type.  It is useful in situations, like this one,
where only some of the type information is known.


File: guide.info,  Node: Stream patterns,  Next: Notation and contracts,  Prev: The stream and sequence contracts,  Up: Sequence notation

4.1.3 Stream patterns
---------------------

The the 'stream' contract is used in _patterns_ to match and decompose
sequences.  For example, the signature for 'hdtl' - which is used to
decompose sequences into a head and tail - is:
     _hdtl:(t)=>option[(e,t)].
   This function will be applied to a sequence in the attempt to split
it into a head and remainder.  The question is how can a function be
used in a pattern?

   The term '[1,2,..X]' _as a pattern_ is rewritten as:
     _hdtl^(1,_hdtl^(2,X))
   where the '^' is syntactic sugar for the more elaborate form:
     S0 where (1,S1) ^= _hdtl(S0) && (2,X)^=_hdtl(S1)
   which is, in turn, syntactic sugar for:
     S0 where some((1,S1)) .= _hdtl(S0) && some((2,X)) .= _hdtl(S1)
   I.e., the sequence pattern becomes a series of progressive
decompositions of the stream; at each stage an 'option'-valued function
is applied to peel off elements from the stream.

   We can now straightforwardly give the translation for sequence
patterns.  Syntactically, there is no distinction between sequence
expressions and stream patterns - what distinguishes them is context:
stream patterns show up as patterns in functions and sequence
expressions show up in the expression context.

   A stream pattern, as in the pattern '[E,..X]' for the non-empty case
in 'concat':
     concat([E,..X],Y) => [E,..concat(X,Y)]
   is transformed into the pattern:
     _hdtl^(E,X)
   and the entire 'concat' equation becomes:
     concat(_hdtl^(E,X),Y) => _cons(E,concat(X,Y))
   which, as we noted above, is actually equivalent to:
     concat(S0,Y) where some((E,X)).=S0 =>
       _cons(E,concat(X,Y)).

   The 'sequence' and 'stream' contracts are two of the most important
and commonly used contracts in the *Star* library.  As we shall see
further, many of the standard collections functions are built on top of
it.

     We have two contracts - one for composing and another for
     decomposing sequences - because not all collections are equally
     amenable to decomposing and/or composing.  For example, the 'map'
     type we describe below does not have a natural notion of
     decomposing (because ordering within a 'map' is not preserved);
     even though it does have a natural form of describing actual 'map'
     collections.


File: guide.info,  Node: Notation and contracts,  Prev: Stream patterns,  Up: Sequence notation

4.1.4 Notation and contracts
----------------------------

One of the distinctive features of the sequence notation is that it is
an example of _syntax_ that is underwritten by a semantics expressed as
a _contract_.  This is part of a widespread pattern in *Star*.

   This has a parallel in modern OO languages like Java and C# where
important contracts are expressed as interfaces rather than concrete
types.  However, *Star* extends the concept by permitting special
notation as well as abstract interfaces - as many mathematicians
understand, a good notation can sometimes make a hard problem easy.  In
*Star* we further separate interfaces from types by separating the type
definition from any contracts that may be implemented by it.

   The merit of this combination of special syntax and contracts is that
we can have the special notation expressing a salient concept - in this
case the sequence - and we can realize the notation without undue
commitment in its lower-level details.  In the case of sequence
notation, we can have a notation of sequences without having to commit
to the type of the sequence itself.


File: guide.info,  Node: Indexing,  Next: Doing stuff with collections,  Prev: Sequence notation,  Up: Collections

4.2 Indexing
============

Accessing collections conveniently is arguably more important than a
good notation for representing them.  There is a long standing
traditional notation for accessing arrays:
     L[ix]
   where 'L' is some array or other collection and 'ix' is an integer
offset into the array.  *Star* uses a notation based on this for
accessing collections with random indices; suitably generalized to
include dictionaries (collections accessed with non-numeric indices) and
_slices_ (contiguous sub-regions of collections).

   Before we explore *Star*'s indexing notation it is worth looking at
the contract that underlies it - the 'indexed' contract.

* Menu:

* The indexed contract::
* The index notation::
* Implementing indexing::
* Index slices::


File: guide.info,  Node: The indexed contract,  Next: The index notation,  Up: Indexing

4.2.1 The indexed contract
--------------------------

The indexed contract captures the essence of accessing a collection in a
random-access fashion.  There are functions in the contract to access a
directly accessed element, to replace and to delete elements from the
collection:

     contract all s,k,v ~~ indexed[s->>k,v] ::= {
       _index:(s,k)=>option[v].
       _insert:(s,k,v)=>s.
       _replace:(s,k,v)=>s.
       _remove:(s,k)=>s.
     }
   There are several noteworthy points here:

   * the form of the contract itself; the signature for '_index' which
     accesses elements; and
   * the signatures for '_insert', '_replace' and '_remove' which return
     new collections rather than modifying them in-place.

   Recall that the 'stream' contract had the form:
     contract all s, e ~~ stream[s->>e] ::= ...
   the 's->>e' clause allows the implementation of the contract to
functionally determine (sic) the type of the elements of the collection.

   In the case of 'indexed', the contract form determines _two_ types
denoted by 'k' and 'v'.  The type 'k' denotes the type of the key used
to access the collection and 'v' denotes the type of the elements of the
collection.  Each individual implementation of indexed is free to
specify these types; usually in a way that best reflects the natural
structure of the collection.

   For example, the implementation of 'indexed' for strings starts:
     implementation indexed[string ->> integer,integer] => ...
   reflecting the fact that the natural index for strings is integer and
the natural element type is integer (neither being explicitly part of
the string type name).  (1)

   On the other hand, the implementation of 'indexed' for the concrete
type 'map' starts:
     implementation all k,v ~~
       indexed[map[k,v] ->> k,v] => { ... }
   reflecting the fact that dictionaries are naturally generic over both
the key and value types.

   If we look at the signature for '_index' we can see that this
function does not directly return a value from the collection, but
instead returns an 'option' value.  This bears further explanation.

   The great unknown of accessing elements of a collection is 'is it
there?'.  Its not guaranteed of course, and we need to be able to handle
failure.  In the case of the '_index' function, its responsibility is to
either return a value wrapped as a 'some' value - if the index lookup is
successful - or the signal 'none' if the index lookup fails.  Just to be
clear: '_index' can act both as a lookup _and_ as a test for membership
in the collection.

* Menu:

* Adding and removing elements::

   ---------- Footnotes ----------

   (1) Note that we use 'integer' to denote the type of string
characters because the complexities of Unicode representation make a
consistent character encoding very difficult.


File: guide.info,  Node: Adding and removing elements,  Up: The indexed contract

4.2.1.1 Adding and removing elements
....................................

The function '_insert' is used to add an element to a collection
associating it with a particular index position; and the function
'_remove' removes an identified element from the collection.  The
function '_replace' is similar to '_insert' except that it is expected
that an existing element is replaced rather than a new value being
inserted.

   These functions have a property often seen in functional programming
languages and not often seen elsewhere: they are defined to return a
complete new collection rather than simply side-effecting the
collection.  This is inline with an emphasis on _persistent data
structures_(1) and on _declarative programming_.

   One might believe that this is a bit wasteful and expensive -
returning new collections instead of side-effecting the collection.
However, that is something of a misconception: modern functional data
structures have excellent computational properties and approach the best
side-effecting structures in efficiency.  At the same time, persistent
data structures have many advantages - including substantially better
correctness properties and behavior in parallel execution contexts.

     It should also be stressed that the 'indexed' contract allows and
     encourages persistence but does not _enforce_ it.  It is quite
     possible to implement indexing for data structures that are not
     persistent.

   ---------- Footnotes ----------

   (1) A persistent structure is one which is never modified - changes
are represented by new structures rather than modifiying existing ones.


File: guide.info,  Node: The index notation,  Next: Implementing indexing,  Prev: The indexed contract,  Up: Indexing

4.2.2 The index notation
------------------------

Given the indexed contract we can now show the specific notation that
*Star* has for accessing elements of a collection.  Accessing a
collection by index follows conventional notation:
     C[ix]
   will access the collection 'C' with element identified by 'ix'.  For
example, given a 'map' 'D' of strings to strings, we can access the
entry associated with “alpha” using:
     D["alpha"]
   Similarly, we can access the third character in a string 'S' using:
     S[2]
   As might be expected, given the discussion above, the type of an
index expression is optional.

   The most natural way of making use of an index expression is to use
it in combination with a '^=' condition or an '^|' expression - which
allows for smooth handling of the case where the index fails.  For
example, we might have:

     nameOf(F) where N ^= names[F] => N.
     nameOf(F) default => ...

   We will take a deeper look at exceptions and more elaborate
management of tentative computation in COMPUTATION EXPRESSIONS.

   *Star* also has specific notation to represent modified collections.
For example, the expression
     D["beta"->"three"]
   denotes the map 'D' with the entry associated with '"beta"' replaced
by the value '"three"'.  Note that the value of this expression is the
updated map.

   For familiarity's sake, we also suppose a form of assignment for the
case where the collection is part of a read-write variable.  The action:
     D["beta"] := "three"
   is entirely equivalent to:
     D := D["beta"->"three"]
   always assuming that the type of 'D' permits assignment.

   Similarly, the expression:
     D[\+"gamma"]
   which denotes the map 'D' where the value associated with the key
'"gamma"' has been removed.

   Although, in these examples, we have assumed that 'D' is a map value
(which is a standard type in *Star*); in fact the index notation does
not specify the type.  As with the sequence notation, the only
requirement is that the 'indexed' contract is implemented for the
collection being indexed.

   In particular, as well as the 'map' type, index notation is supported
for the built-in 'list' type, and is even supported for the 'string'
type.

   In addition to the indexed access notation described so far, *Star*
also allows a variant of the sequence notation for constructing indexed
literals (aka dictionaries).  In particular, an expression of the form:
     ["alpha"->1, "beta"->2, "gamma"->3]
   is equivalent to a sequence of tuples, or to:
     _cons(("alpha",1),_cons(("beta",2),_cons(("gamma",3),_nil))
   which is understood by indexed types as denoting the contruction of a
literal.

   Note that there are two levels of domain-specific notation here: the
representation of indexed literals in terms of a sequence of two-tuples
and the implicit rule governing indexed types: they should implement a
specific form of 'sequence' contract.  Both are actually part of the
semantics of representing indexed literals.


File: guide.info,  Node: Implementing indexing,  Next: Index slices,  Prev: The index notation,  Up: Indexing

4.2.3 Implementing indexing
---------------------------

Of course, this includes our own types.  For example, before, when
looking at generic types we saw the tree type:
     all t ~~ tree[t] ::= tEmpty | tNode(tree[t],t,tree[t]).
   We can define an implementation for the indexed contract for this
type - if we arrange for the tree to be a tree of key-value pairs:
     implementation all k,v ~~
         order[k], equality[k] |: indexed[tree[(k,v)]->>k,v] => {
       _index(T,K) => findInTree(T,K).
       _insert(T,K,V) => setKinTree(T,K,V).
       _replace(T,K,V) => setKinTree(T,K,V).
       _remove(T,K) => removeKfromTree(T,K).
     }
   The form of the type expression 'tree[(k,v)]' is required to avoid
confusion - 'tree' takes a single type argument that, in this case, is a
tuple type.  The extra set of parentheses ensures that 'tree' is not
interpreted (incorrectly) as a type that takes two type arguments.

   With this statement in scope, we can treat appropriate 'tree'
expressions as though they were regular arrays or dictionaries:
     T = tNode(tEmpty,("alpha","one"),tEmpty)
     assert "one" ^= T["alpha"].
     U = T["beta"->"two"]. -- Add in "beta"
     assert "one" ^= U["alpha"].
   The implementation statement relies on another feature of *Star*'s
type system - we need to constrain the implementation of indexed to a
certain subset of possible instances of tree types - namely, where the
element type of the tree is a _pair_ - a two-tuple - and secondly we
require that the first element of the pair is comparable - i.e., it has
the 'order' contract defined for it.

   This is captured in the contract clause of the implementation
statement:
     implementation all k,v ~~ order[k], equality[k] |:
           indexed[tree[(k,v)]->>k,v] => ...
   This implementation contract qualifier is fairly long, and the type
constraints are fairly complex; but it is exquisitely targeted at
precisely the right kind of tree without us having to make any
unnecessary assumptions.(1)

   Implementing the indexed contract requires us to implement three
functions: 'findInTree', 'setKinTree' and 'removeKfromTree'.  The
'findInTree' function is quite straightforward:
     findInTree:all k,v ~~ equality[k], order[k] |: (tree[(k,v)],k)=>option[v].
     findInTree(tEmpty,_) => none.
     findInTree(tNode(_,(K,V),_),K) => some(V).
     findInTree(tNode(L,(K1,_),_),K) where K1>K => findInTree(L,K).
     findInTree(tNode(_,(K1,_),R),K) where K1<K => findInTree(R,K).
   Notice that each 'label' in the tree is a 2-tuple - consisting of the
key and the value.  This function is also where we need the key type to
be both comparable and supporting equality.  The comparable constraint
has an obvious source: we perform inequality tests on the key.

   The 'equality' constraint comes from a slightly less obvious source:
the repeated occurrence of the 'K' variable in the second equation.
This repeated occurrence means that the equation is equivalent to:
     findInTree(tNode(_,(K,V),_),K1) where K==K1 => some(V).
   We leave the implementations of 'setKinTree' and 'removeKfromTree' as
an exercise for the reader.

   Along with the implementation of 'indexed', we should also implement
'sequence' for our trees:
     implementation all k,v ~~ order[k], equality[k] |:
       sequence[tree[(k,v)]->>(k,v)] => {
         _nil = tEmpty.
         _cons((K,V),T) => setKinTree(T,K,V).
         _apnd(T,(K,V)) => setKinTree(T,K,V).
     }
     Notice: that adding a pair to the front of a 'tree' is semantically
     the same as adding it to the back - since the 'tree' is ordered.

   The reason for implementing 'sequence' is that that will allow users
of the tree to use the indexed variant of the sequence notation for
writing tree literals; as in:
     tree of [1->”alpha”, 2->”beta”]
   We do not implement the companion 'stream' contract for our 'tree'
because its semantics would be somewhat problematic.  However, there is
nothing preventing the reader from experimenting with an appropriate
tree-ordering.

   ---------- Footnotes ----------

   (1) It is also true that most programmers will not be constructing
new implementations of the indexed contract very frequently.


File: guide.info,  Node: Index slices,  Prev: Implementing indexing,  Up: Indexing

4.2.4 Index slices
------------------

Related to accessing and manipulating individual elements of collections
are the _indexed slice_ operators.  An indexed slice of a collection
refers to a bounded subset of the collection.  The expression:
     C[fx:tx]
   denotes the subsequence of 'C' starting with - and including - the
element indexed at 'fx' and ending - but _not_ including the element
indexed at 'tx'.

   As might be expected, the index slice notation is also governed by a
contract - the 'sliceable' contract.  This contract defines the core
functions for slicing collections and for updating subsequences of
collections:
     contract all s,k ~~ sliceable[s->>k] ::= {
       _slice:(s,k,k)=>s.
       _tail:(s,k)=>s.
       _splice:(s,k,k,s)=>s.
     }
   The '_slice' function is used extract a slice from the collection,
'_tail' is a variant that returns the 'rest' of the collection, and
'_splice' is used to replace a subset of the collection with another
collection.

   Like the indexing notation, there is notation for each of the three
cases:
     C[fx:]
   denotes the tail of the collection - all the elements in 'C' that
come after 'fx' (including 'fx' itself);(1).  and
     C[fx:tx->D]
   denotes the result of splicing 'D' into 'C'.  This last form has an
additional incarnation - in the form of an assignment statement:
     C[fx:tx] := D
   This action is equivalent to the assignment:
     C := _splice(C,fx,tx,D)
   which, of course, assumes that 'C' is defined as a read/write
variable.

   The slice notation is an interesting edge case in domain specific
languages.  It is arguably a little obscure, and, furthermore, the use
case it represents is not all that common.  On the other hand, without
specific support, the functionality of slicing is hard to duplicate with
the standard indexing functions.

   ---------- Footnotes ----------

   (1) The complement of the tail slice is simple: 'C[0:tx]'


File: guide.info,  Node: Doing stuff with collections,  Next: Different types of collection,  Prev: Indexing,  Up: Collections

4.3 Doing stuff with collections
================================

One of the most powerful features of collections is the ability to treat
a collection as a whole.  We have already seen a little of this in our
analysis of the visitor pattern *note Going even further::.  Of course,
the point of collections is to be able to operate over them as entities
in their own right.  As should now be obvious, most of the features we
discuss are governed by contracts and it is paradigmatic to focus on
contract specifications rather than specific implementations.

   The number of things that people want to do with collections is only
limited by our imagination; however, we can summarize a class of
operations in terms several patterns:

   * Filtering
   * Transforming into new collections
   * Summarizing collections
   * Querying collections

   Each of these patterns has some support from *Star*'s standard
repertoire of functions.

* Menu:

* Filtering::
* The sieve of Erastosthneses::
* Mapping to make new collections::
* Compressing collections::


File: guide.info,  Node: Filtering,  Next: The sieve of Erastosthneses,  Up: Doing stuff with collections

4.3.1 Filtering
---------------

The simplest operation on a collection is to subset it.  The standard
filter function - '^/' - allows us to do this with some elegance.  Using
filter is fairly straightforward; for example, to remove all odd numbers
from a collection we can use the expression:
     Nums^/((X)=>X%2==0)
   For example, if 'Nums' were the list:
     list of [1,2,3,4,5,6,7,8,9]
   then the value of the filter expression would be
     list of [2,4,6,8]
   The right hand argument to '^/' is a _predicate_: a function that
returns a 'boolean' value.  The '^/' function (which is part of the
standard 'filter' contract) is required to apply the predicate to every
element of its left hand argument and return a _new_ collection of every
element that satisfies the predicate.(1)

   Note that the '%' function is arithmetic remainder, and the
expression 'X%2==0' amounts to a test that 'X' is even (its remainder
modulo 2 is 0).

   The '^/' operator allows us to represent many filtering algorithms
whilst not making any recursion explicit.  However, not all filters are
easily handled in this way; for example, a prime number filter _can_ be
written
     N^/isPrime
   but such an expression is likely to be very expensive (the 'isPrime'
test is difficult to do well).

* Menu:

* The filter contract::

   ---------- Footnotes ----------

   (1) The original collection is unaffected by the filter.


File: guide.info,  Node: The filter contract,  Up: Filtering

4.3.1.1 The 'filter' contract
.............................

As noted above, the '^/' function is governed by a contract, the
'filter' contract:
     contract all c,e ~~ filter[c->>e] ::= {
       (^/):(c,(e)=>boolean) => c.
     }


File: guide.info,  Node: The sieve of Erastosthneses,  Next: Mapping to make new collections,  Prev: Filtering,  Up: Doing stuff with collections

4.3.2 The sieve of Erastosthneses
---------------------------------

One of the classic algorithms for finding primes that can be expressed
using filters is the so-called _sieve of Eratosthenes_.  This algorithm
works by repeatedly removing multiples of primes from a list of natural
numbers.  We cannot (yet) show how to deal with infinite lists of
numbers but we can capture the essence of this algorithm using a
cascading sequence of filter operations.

   The core of the sieve algorithm involves taking a list of numbers and
removing multiples of a given number from the list.  This is very
similar to our even-number finding task, and we can easily define a
function that achieves this:
     filterMultiples(K,N) => N^/((X)=>X%K=!=0).
   The overall Eratosthenes algorithm works by taking the first element
of a candidate list of numbers as the first prime, removing multiples of
that number from the rest, and recursing on the result:
     sieve([N,..rest]) => [N,..sieve(filterMultiples(N,rest))].
   There is a base case of course, when the list of numbers is exhausted
then we have no more primes:
     sieve([]) => [].
   The complete prime finding program is hardly larger than the original
filter specification:
     primes(Max) => let{
       sieve([]) => [].
       sieve([N,..rest]) => [N,..sieve(filterMultiples(N,rest))].

       filterMultiples(K,N) => N^/((X)=>X%K=!=0).

       iota(Mx,St) where Mx>Max => [].
       iota(Cx,St) => [Cx,..iota(Cx+St,St)].
     } in [2,..sieve(iota(3,2))]
   The 'iota' function is used to construct a list of numbers, in this
case the integer range from '3' through to Max with an increment of '2'.
We start the 'sieve' with '2' and the list of integers with '3' since we
are making use of our prior knowledge that '2' is prime.

   It should be emphasized that the sieve of Eratosthenes hardly counts
as an efficient algorithm for finding primes.  For one thing, it
requires that we start with a list of integers; most of which will be
discarded.  In fact, each 'sweep' of the list of numbers results in a
new list of numbers; many of which too will eventually be discarded.
Furthermore, the 'filterMultiples' function examines every integer in
the list; it does not make effective use of the fact that successive
multiples occupy predictable slots in the list of integers.

     In fact, building a highly optimized version of the sieve of
     Eratosthenes is not actually the main point here - our purpose is
     to illustrate the power of *Star*'s collections processing
     functions.

   We might ask whether the 'sieve' function can also be expressed as a
filter.  The straightforward answer is that it cannot: the sieve _is_ a
kind of filter, but the predicate being applied depends on the entire
collection; not on each element.  The standard filter function does not
expose the entire collection to the predicate.  However, we will see at
least one way of achieving the sieve without any explicit recursion
below when we look at folding operations.


File: guide.info,  Node: Mapping to make new collections,  Next: Compressing collections,  Prev: The sieve of Erastosthneses,  Up: Doing stuff with collections

4.3.3 Mapping to make new collections
-------------------------------------

One of the limitations of the filter function is that it does not create
new elements: we can use it to subset collections but we cannot
transform them into new ones.  The 'fmap' function - part of the
'functor' contract - can be used to perform many transformations of
collections.

   For example, to compute the lengths of strings in a list we can use
the expression:
     fmap(size,list of ["alpha","beta","gamma"])
   which results in the list:
     list of [5,4,5]
   The 'fmap' function is defined via the 'functor' contract - thus
allowing different implementations for different collection types:
     contract all c/1 ~~ functor[c] ::= {
       fmap:all a,b ~~ ((a)=>b,c[a])=>c[b].
     }
   Notice how the contract specifies the collection type - 'c' - without
specifying the type of the collection's element type.  We are using a
different technique here than we used for the 'stream' and 'filter'
contracts.  Instead of using a functional dependency to connect the type
of the collection to the type of the element, we denote the type of the
input and output collections using a _type constructor_ variable as in
'c[a]' and 'c[b]'.(1)

   We are also using a variant of the quantifier.  A quantified type
variable of the form 'c/1' denotes a type constructor variable rather
than a regular type variable.  In this case, 'c/1' means that the
variable 'c' must be a type constructor that takes one argument.

   The reason for this form of contract is that 'functor' implies
creating a new collection from an old collection; with a possibly
different element type.  This is only possible if the collection is
generic and hence the type expressions 'c[a]' for the second argument
type of 'fmap' and 'c[b]' for its return type.

   One might ask whether we could not have used functional dependencies
in a similar way to 'stream' and 'filter'; for example, a contract of
the form:
     contract all c,e,f ~~ mappable[c->>e,f] ::=  {
       mmap:((e)=>f,c)=>c.
     }
   However, _this_ contract forces the types of the result of the 'mmap'
to be identical to its input type, it also allows the implementer of the
'mappable' contract to fix the types of the collection elements - not at
all what we want from a 'fmap'.

   It is not all that common that we need to construct a list of sizes
of strings.  A much more realistic use of 'fmap' is for _projection_.
For example, if we wanted to compute the average age of a collection of
people, which is characterized by the type definition:
     person ::= someOne{
       name:string.
       age:()=>float.
     }
   Suppose that we already had a function 'average' that could average a
collection of numbers; but which (of course) does not understand people.
We can use our average by first of all projecting out the ages and then
applying the average function:
     average(fmap((X)=>X.age(),People))
   In this expression we project out from the 'People' collection the
ages of the people and then use that as input to the average function.

* Menu:

* More Type Inference Magic::

   ---------- Footnotes ----------

   (1) This also means that the collection type in 'fmap' must be
generic: it is not possible to implement 'functor' for strings.


File: guide.info,  Node: More Type Inference Magic,  Up: Mapping to make new collections

4.3.3.1 More Type Inference Magic
.................................

There is something a little magic about the lambda function in this
expression: how does the type checker 'know' that 'X' can have a field
'age' in it?  How much does the type checker know about types anyway?

   In this particular situation the type checker could infer the type of
the lambda via the linking between the type of the 'fmap' function and
the type of the 'People' variable.  However, the type checker is
actually capable of giving a type to the lambda even without this
context.  Consider the function:
     nameOf(R) => R.name
   This function takes an arbitrary record as input and returns the
value of the 'name' field.  The 'nameOf' function _is_ well typed, its
type annotation just needs a slightly different form than that we have
seen so far:(1)
     nameOf:all r,n ~~ r <~ {name:n} |: (r)=>n
   This is another example of a _constrained type_: in this case, the
constraint on 'r' is that it has a field called 'name' whose type is the
same as that returned by 'nameOf' itself.

   The type constraint:
     r <~ {name:n}
   means that any type bound to 'r' must have a 'name' field whose type
is denoted by the type variable 'n' in this case.

   With this type signature, we can use 'nameOf' with any type that that
a 'name' field.  This can be a record type; it can also be a type
defined with an algebraic type definition that includes a record
constructor.

   ---------- Footnotes ----------

   (1) Note that the *Star* type system will not _infer_ this
generalized type.


File: guide.info,  Node: Compressing collections,  Prev: Mapping to make new collections,  Up: Doing stuff with collections

4.3.4 Compressing collections
-----------------------------

Another way of using collections is to summarize or aggregate over them.
For example, the 'average' function computes a single number from an
entire collection of numbers - as do many of the other statistical
functions.  We can define average using the standard 'foldLeft'
function, which is part of the standard 'folding' contract:
     average(C) is foldLeft((+),0,C)::float/size(C)::float

     This definition of the 'average' function is about as close to a
     specification of average as is possible in a programming language!

     Notice: the use of coercion here - coercing both the result of the
     'foldLeft' and 'size' to float.  The reason for doing this is that
     functions like 'average' are 'naturally' real functions.(1)
     Without the explicit coercion, averaging a list of integers will
     also result in an integer value - which is likely to be inaccurate.

   Of course, in our definition of 'average' we need to coerce _both_
the numerator and denominator of the division because *Star* does not
have implicit coercion.

   The 'foldLeft' function applies a binary function to a collection:
starting from the first element and successively 'adding up' each of the
elements in the collection using the supplied operator.

 [image src="images/leftfold.png" ]

Figure 4.1: Left Folding a Collection

   As we noted above, 'foldLeft' is part of the 'folding' contract.
Like the 'functor' contract, this uses some more subtle type
constraints:
     contract all c,e ~~ folding[c->>e] ::= {
       foldRight:all x ~~ (((e,x)=>x),x,c) => x.
       foldLeft:all x ~~ (((x,e)=>x),x,c) => x.
     }
   The 'folding' contract uses quantifiers in two places: once in the
contract specification and once in the type signature for 'foldLeft'
(and 'foldRight').  What we are trying to express here is that any
implementation of 'folding' must allow for a generic function to process
the collection.

   The 'foldLeft' (and 'foldRight') functions have an 'accumulator' (of
type 'x') which need not be the same as the type of the elements of the
collection.(2)  This argument acts as a kind of linking thread during
the entire computation - and represents the returned value when the fold
is complete.

   But we can do much more than computing averages with a fold.  Recall
that when we realized the sieve of Eratosthenes, we still had a
recursive structure to the program.  Furthermore, the way our original
program was written each filter results in a new list of numbers being
produced.  Instead of doing this, we can construct a cascade of filter
functions - each level in the cascade is responsible for eliminating
multiples of a specific prime.

   The complete cascade filters by checking each level of the cascade:
for example, after encountering 3, 5 and 7, there will be a cascade of
three functions that check each incoming number: one to look for
multiples of 3, one for multiples of 5 and one for multiples of 7.  When
we encounter the next prime (11) then we glue on to the cascade a
function to eliminate multiples of 11.

   Consider the task of adding a filter to an existing cascade of
filters.  What is needed is a new function that combines the effect of
the new filter with the old one.  The 'cascade' function takes a filter
function and a prime as arguments and constructs a new function that
checks both the prime _and_ the existing filter:
     cascade:((integer)=>boolean,integer)=>((integer)=>boolean).
     cascade(F,K) => (X)=>F(X) && X%K=!=0.

     This is a truly higher-order function: it takes a function as
     argument and returns another function.

   Given 'cascade', we can reformulate the 'sieve' function itself as a
'foldRight' - at each new prime step we 'accumulate' a new cascaded
filter function:
     stp:(integer,(integer)=>boolean)=>((integer)=>boolean).
     stp(X,F) where F(X) => cascade(F,X).
     stp(X,F) => F.
   At each step in the fold we want to know whether to continue to
propagate the existing filter or whether to construct a new filter.

   The 'sieve' function itself is now very short: we simply invoke
'foldRight' using 'stp' and an initial 'state' consisting of a function
that checks for odd numbers:
     sieve(C) => foldRight(stp,(K)=>K%2=!=0,C).
   This version of 'sieve' is not quite satisfactory as, while it does
find prime numbers, it does not report them.  A more complete version
has to also accumulate a list of primes that are found.  We can do this
by expanding the accumulated state to include both the cascaded filter
function and the list of found primes.  The main alteration is to the
'step' function:
     step:((list[integer],(integer)=>boolean),integer) => (list[integer],(integer)=>boolean).
     step(X,(P,F)) where F(X) => ([P..,X],cascade(F,X)).
     step(_,(P,F)) => (P,F).
   and the initial state has an empty list:
     sieve(C) is fst(foldRight(step,([],(K)=>K%2=!=0),C)).
   where 'fst' and 'snd' are standard functions that pick the left and
right hand sides of a tuple pair:
     fst:all s,t ~~ ((s,t))=>s.
     fst((L,R)) => L
     snd:all s,t ~~ ((s,t))=>t.
     snd((L,R)) => R
   There is one final step we can make before leaving our sieve of
Eratosthenes - we can do something about the initial list of integers.
As it stands, while the sieve program does not construct any
intermediate lists of integers, it still requires an initial list of
integers to filter.  However, this particular sequence can be
represented in a very compact form - as a 'range' term.

   'range' terms are special forms of collections that denote ranges of
numeric values.  For example, the expression
     range(0,100,2)
   denotes the sequence of integers starting at zero, not including 100,
each succesive integer being incremented by 2.

   Using a similar 'range' term, we can denote the list of primes less
than 1000 with
     primes(Max) => let{
       cascade:((integer)=>boolean,integer) => ((integer)=>boolean).
       cascade(F,K) => (X)=>(F(X) && X%K=!=0).

       step(X,(P,F)) where F(X) => ([P..,X],cascade(F,X)).
       step(_,(P,F)) => (P,F).

       sieve:(range[integer])=>list[integer].
       sieve(R) => fst(foldRight(step,([],(K)=>true),R)).
     } in sieve(range(3,Max,2)).

     show primes(1000)
   This final program has an important property: there are no explicit
recursions in it - in addition, apart from the 'foldRight' function,
there are no recursive programs at all in the definition of 'primes'.

     Of course, it still would not count as the _most efficient_ primes
     finding program; but that was not the goal of this discussion.

   ---------- Footnotes ----------

   (1) Real as in the ℝeal numbers.

   (2) We saw something similar with the visitor pattern.


File: guide.info,  Node: Different types of collection,  Next: Collecting it together,  Prev: Doing stuff with collections,  Up: Collections

4.4 Different types of collection
=================================

Just as there are many uses of collections, so there are different
performance requirements for collections themselves.  The most
challenging aspects of implementing collections revolves around the cost
of _adding_ to the collection, the cost of _accessing_ elements of the
collection and the cost of _modifying_ elements in the collection.

   There is a strong emphasis on _persistent_ semantics for the types
and functions that make up *Star*'s collections architecture.  This is
manifest in the fact, for example, that functions that add and remove
elements from collections _do not_ modify the original collection.

   However, even without that constraint, different implementation
techniques for collections tend to favor some operations at the cost of
others.  Hence, there are different types of collection that favor
different patterns of use.

* Menu:

* The cons type::
* The list type::
* The map type::
* The set type::


File: guide.info,  Node: The cons type,  Next: The list type,  Up: Different types of collection

4.4.1 The 'cons' type
---------------------

This is the simplest collection type; and is perhaps the original
collection type used in functional programming languages.  It is defined
by the type declaration:
     all t ~~ cons[t] ::= nil | cons(t,cons[t]).
   Cons lists have the property that adding an element to the front of a
list is a constant-time operation; similarly, splitting a 'cons' list
into its head and tail is also a constant time operation.  However,
almost every other operation is significantly more expensive: putting an
element on to the end of a 'cons' list is linear in the length of the
list.

   The main merit of the 'cons' list is the sheer simplicity of its
definition.  Also, for small collections, its simple implementation may
outweigh the advantages that more complex collections offer.


File: guide.info,  Node: The list type,  Next: The map type,  Prev: The cons type,  Up: Different types of collection

4.4.2 The 'list' type
---------------------

The list type offers a different trade-off to the 'cons' type: where the
latter is optimal for ease of constructing and for traversing complete
lists, the list type offers constant-time access to random elements
within the array - at the potential cost of more expensive construction
of lists.

   Unlike the 'cons' type, the 'list' type does not have a
straightforward definition as an algebraic type.  Internally, a list
structure consists of an array of locations with a 'control pointer'
giving the portion of the array block that represents a given list
value.  This is sometimes called a Copy on Write (COW) structure.

   The 'list' type is optimized for random access and for shared storage
- recall that *Star* collection types are persistent: that means that
different values can share some or all of their internal structure.  The
diagram below shows two list values that overlap in their elements and
consequently share some of their structure.

 [image src="images/twoarrays.png" ]

Figure 4.2: Two lists Sharing Structure


File: guide.info,  Node: The map type,  Next: The set type,  Prev: The list type,  Up: Different types of collection

4.4.3 The 'map' type
--------------------

Unlike the 'cons' or 'list' type, the 'map' type is oriented for access
by arbitrary keys.  The 'map' is also quite different to hash maps as
found in Java (say), the 'map' type is _persistent_: the functions that
access dictionaries such as by adding or removing elements return new
dictionaries rather than modifying a single shared structure.  However,
the efficiency of 'map' is quite comparable to Java's HashMap.

   The template for the 'map' type is:
     all k,v ~~ equality[k], hash[k] |: map[k,v]
   Notice that there is an implied constraint here: the 'map' assumes
that the keys in the map can be compared for equality, and that they are
hashable - have a 'hash' function.

   A 'map' value can be written using the sequence notation, using tuple
pairs for the key-value pairs:
     map of [(1,"alpha"),(2,"beta”)]
   As we saw before, 'map's also have a special variant of the sequence
notation; instead of writing the pairs as tuples we can use an arrow
notation for 'map' terms:
     map of [1->"alpha", 2->"beta"]
   Maps also have their own special variant of a _query search
condition_.  A condition of the form
     K->V in D
   where D is a 'map' will be satisfied if there is a key/value pair in
D corresponding to K and V. For example, the condition:
     K->V in map of [1->"alpha", 2->"beta"] && V=="alpha"
   is satisfied for only one pair of 'K' and 'V': namely '1' and
'"alpha"' respectively.

   For the curious, dictionaries are implemented using techniques
similar to, as described by Bagwell in 'Ideal Hash Trees, P. Bagwell'.
This results in a structure with an effective O(1) cost for accessing
elements _and_ for modifying the 'map' - all the while offering an
applicative data structure.


File: guide.info,  Node: The set type,  Prev: The map type,  Up: Different types of collection

4.4.4 The 'set' type
--------------------

There are many instances where a programmer needs a collection but does
not wish specify any ordering or mapping relationship.  The standard
'set' type allows you to construct such entities.

   Using a 'set' type offers the programmer a signal that minimizes
assumptions about the structures: the set type is not ordered, and
offers no ordering guarantees.  It does, however, offer a guarantee that
operations such as element insertion, search and set operations like set
union are implemented efficiently.

   Like 'map', the 'set' type is not publicly defined using an algebraic
type definition: its implementation is private.  It's type is given by
the template:
     all t ~~ equality[t] |: set[t]


File: guide.info,  Node: Collecting it together,  Prev: Different types of collection,  Up: Collections

4.5 Collecting it together
==========================

Collections form an important part of any modern programming language.
The suite of features that make up the collections architecture in
*Star* consists of a number of data types, contracts and special syntax
that combine to significantly reduce the burden of the programmer.

   The collections facility amounts to a form of DSL - Domain Specific
Language - that is, in this case, built-in to the language.  We shall
see later on that, like many DSLs, this results in a pattern where there
is a syntactic extension to the language that is backed by a suite of
contracts that define the semantics of the DSL.


File: guide.info,  Node: Making choices,  Next: Boiling the Ocean,  Prev: Collections,  Up: Top

5 Making choices
****************

One of the common themes of systems that 'face the world' (aka online
applications) is that they often have to make decisions.  More
specifically, any outward facing system will receive requests that the
system must decide whether they are legitimate or not.  Before acting on
a request, we must decide whether or not to trust it.

   We can draw an extremely simplistic view of a large class of _online_
applications:

 [image src="images/online.png" ]

Figure 5.1: Simplistic Online Application

   This diagram illustrates the four main elements of an online
application: a _source_ of events, a _processor_ which processes events,
a _Sink_ which represents externally visible actions being taken by the
system, and, finally, a _Store_.

   We are being deliberately simplistic here; because we wish to focus
on a part of this picture: the _Store_.

   A _Store_ is part of most online applications because applications
need some form of context: there is typically not enough information in
an event to determine how to react to it: we must also have some history
as well.  For example, if an event represents a log-in attempt, then in
order to know how to respond, we need to know if the originator has an
account, whether the offered password is consistent with our records and
so on.

   The _Store_ concept is simply an abstract way of representing this
context.  However, we can go a little further and claim that the
dominant mode of accessing a store is through _queries_.

   A query is simply an expression that is evaluated in the right
context; however, the natural semantics for query processing is more
like that of database searches than arithmetic evaluation.  The reason
for that is that a typical _Store_ may be quite large and serve multiple
functions.  As a result, *Star* queries have a lot in common with logic
and database programming paradigms.

* Menu:

* Queries::
* Classifying::


File: guide.info,  Node: Queries,  Next: Classifying,  Up: Making choices

5.1 Queries
===========

Consider, if you will, the problem of finding a set of
grandparent-grandchild pairs - given information about parent-child
relationships.  For example, suppose that we had a list of pairs - each
pair indicating a parent and child:
     parents:list[(string,string)].
     parents = [("john","peter), ("peter","jane"), ... ].
   and that we wanted to construct a result list - also of pairs - along
the lines of:
     GC:list[(string,string)].
     GC = [("john","jane"),...].
   Computing the list of grandparent/grandchildren pairs involves
searching the 'parents' for pairs that satisfy the grandparent
relationship.  This, in turn, involves a double iteration: each pair in
the 'parent's list might represent the upper or lower half of a
grandparent/grandchild relationship (or both).

   Based on the collection operators we have seen so far, we can build
such a search using two 'foldLeft' operations:
     foldLeft(
      (SoFar,(X,Z)) => foldLeft(
        let {
          acc(gp1,(ZZ,Y)) where Z==ZZ => [gp1..,(X,Y)].
          acc(gp1,_) => gp1.
        } in acc,
        SoFar,parents),
        list of [],
       parents)
   This, rather intimidating,(1) expression uses one 'foldLeft' to look
for the grandparent, and the second 'foldLeft' finds all the
grand-children.  All without any explicit recursion.

   The 'acc' function defined above in the 'let' expression implements
the logic of deciding what to accumulate depending on whether we had
found a grandparent or not.

   The various filter, 'fmap' and 'foldLeft' functions _are_ powerful
ways of processing entire collections.  However, as we can see,
combination iterations can be difficult to construct and harder to
follow; something that is not helped by the occasional need to construct
complex functions in the middle, as in this case.

   What is needed is a way of expressing such complex query conditions
in a way that can be _implemented_ using 'foldLeft' expressions but
which are easier to read.

   Consider the conjunction:
     (X,Z) in parents && (Z,Y) in parents
   This could be read as specifying what it means to be grandparents:

     For which 'X', 'Y' and 'Z' can we assert that 'X' is the parent of
     'Z', and 'Z' is the parent of 'Y'?.

   This should be significantly easier to follow than the complex
expression above; and it is easier to be sure whether it is a correct
representation of what it means to be a grand parent.

* Menu:

* Satisfaction semantics::
* Abstracting queries::
* A Splash of Inference::
* Anatomy of a Query::

   ---------- Footnotes ----------

   (1) There are, unfortunately, some functional programmers that revel
in complex code expressions like this one.  We are not one of them!


File: guide.info,  Node: Satisfaction semantics,  Next: Abstracting queries,  Up: Queries

5.1.1 Satisfaction semantics
----------------------------

Conditions like the ones above are boolean valued - but they are not
always expressions.  For example, the first condition there being a
parent:
     (X,Z) in parents
   is not evaluated in the way that expressions are normally evaluated -
by _testing_ to see if a given pair of 'X' and 'Z' are in some 'parents'
collection.  Instead, the condition needs to be evaluated by trying to
_find_ 'X' and 'Z' that are in the 'parents' collection.  Only if, and
when, suitable bindings for 'X' and 'Z' are found does the condition
'return' true; otherwise it returns false.  In effect, the condition
becomes a _search_ for suitable candidate pairs.

   Technically this is called _satisfying_ the condition - to
distinguish what is going on in normal expression _evaluation_.  Of
course, satisfying and evaluating are close cousins of each other and
amount to the same thing when there is no search involved.

   In addition to individual _search conditions_ like this, it is also
possible to use logical operators - called _connectives_ - to combine
conditions.  In the case of our grand-parent query, there is a
conjunction; which uses the variable 'Z' to acts as a kind of glue to
the two search conditions.

     In database query terminology, conjunctions like this one amount to
     _inner joins_.  Languages like SQL have many operators and features
     to make expressing queries easier; the fundamental semantics of
     *Star*'s queries are similar in power to those of SQL.

   The available connectives include the usual favorites: '&&', '||',
and '\+'.  We may also see the need for some less familiar connectives:
'implies' and 'otherwise'.

     An 'implies' connective is a way of testing complete compliance
     with a condition; for example, we can define a query capturing the
     situation that a manager earns more than his/her members by
     requiring that anyone who works for the manager earns less than
     they do:
          (X,M) in worksFor implies X.salary=<M.salary.


File: guide.info,  Node: Abstracting queries,  Next: A Splash of Inference,  Prev: Satisfaction semantics,  Up: Queries

5.1.2 Abstracting queries
-------------------------

The grandparent condition shows how to define what grandparent-hood
means; but we also need ways of abstracting and naming such queries.
The most straightforward way of this is to use a _query abstraction_
expression.  For example, we can embody the grandparent situation in:
     { (X,Y) | (X,Z) in parents && (Z,Y) in parents}
   The value of a query abstraction expression is typically a 'set' - in
this case a set of the pairs of strings in the grandparent-grandchild
relation.

   Since it's an expression, we can assign it to a variable:
     gp = { (X,Y) | (X,Z) in parents and (Z,Y) in parents}
   and we can feed this set into another query:
     (X,"f") in gp
   and we can define functions whose values are the results of queries:
     gpOf(GC) => {X | (X,GC) in gp}
   We shall explore query expressions a little more, but first an
editorial:


File: guide.info,  Node: A Splash of Inference,  Next: Anatomy of a Query,  Prev: Abstracting queries,  Up: Queries

5.1.3 A Splash of Inference
---------------------------

Queries have an important role in *Star* for two reasons: queries often
have a much more obvious and transparent semantics than other programs -
even functional programs!  Secondly, queries have a deep connection to
logic.

   There are many reasons why one might be interested in logic; from a
theoretical modeling perspective (does the universe follow rational
rules(1)) to the deeply pragmatic reason that logical programs are often
easier to understand and therefore easier to trust.

   Like programming languages, it turns out that there are many kinds of
logic.  Again, like programming languages, there is a trade-off between
expressivity and complexity.  The primary source of complexity in a rule
language is the machinery needed to realize it - together with
understanding it sufficiently to be able to predict the meaning of a
written rule.

   A somewhat simplified enumeration of the different kinds of logic
might be:

Propositional calculus
     This is characterized by single-letter conditions (sometimes
     confusingly called _predicate variables_) and a guaranteed finite
     evaluation mechanism.
Datalog
     This is characterized by relations with simple unstructured values
     (i.e., strings and numbers).  Execution in Datalog has similar
     performance characteristics as querying databases.
First Order Predicate Calculus
     This is probably the most well known and well understood logic.
     From an expressiveness point of view its focus is on the logical
     relationships amongst individual entities - which includes things
     like trees, lists and so on.  Inference in First Order has many of
     the same characteristics as program evaluation: not decidable in
     general but many effective sub-cases.
Higher Order Predicate Calculus
     There are actually many higher-order logics.  The main expressive
     enhancement over First Order is that one can directly talk about
     relationships between entities as well as entities themselves.  The
     cost of this is that inference becomes problematic - even equality
     is undecidable.

   Each of these levels represents a step both in expressiveness and in
complexity.  In general, the right logic for your application is
something only you can decide; however, in designing a language, we have
to choose for you.

   In our view, there is a sweet spot between Datalog and First Order
Logic.  Datalog allows one to right rules (unlike pure SQL) but is not
capable of handling arbitrary data structures.  On the other hand, it
may be that _recursion_ is something that we can do without - as we have
seen earlier, many well structured functional programs have no explicit
recursion!

   However, we must also be able to _embed_ our logic into our more
regular programs.  The key goal here is to maximize the benefit of
providing a logical formalism whilst minimizing the burden on both the
programmer and on the language implementation.  This also recognizes
that, while important, logical reasoning is typically only a small part
of an overall system.  It also recognizes the fact that gaps in the
reasoning capability of a system can be patched more easily if the logic
is simpler.

   And so, in *Star*, we highlight the _query_ aspect of logical
reasoning and bless queries as first class entities in the language.

   Critically, queries have a _declarative_ semantics as well as a
_programmatic_ one; this dual reading is essential if one is to be able
to understand the reasoning.

     Historical note: in earlier iterations of the design for embedding
     logic into Star, a more-or-less complete rule system was envisaged.
     Such inference rules would have a similar status to functions and
     equations do.  However, a combination of complexities and edge
     cases (such as how to handle a combination of inputs and outputs in
     rules) lead the designers to radically simplify the proposal and
     simply focus on queries.  This gave us 90% of the potential benefit
     of inference rules at 10% of the cost.

   Sometimes, a splash of logic is all we need.  In terms of styles of
logic, our approach is most reminiscent of _answer set programming_.

   ---------- Footnotes ----------

   (1) Surprisingly, yes!  Of course, discovering the rationality may be
hard; but the immense success of Western thought was only possible
because the universe is very rationally constructed.


File: guide.info,  Node: Anatomy of a Query,  Prev: A Splash of Inference,  Up: Queries

5.1.4 Anatomy of a Query
------------------------

A query can be seen as combining two elements: a _condition_ and an
_answer template_.  A query condition may be _satisfied_ in one or more
ways - each time potentially binding variables in the condition to
values - and the answer template encodes how we want to use the result
of a successful satisfaction.  Notice that the variables that are bound
by the condition _are in scope_ within the answer template.

   The syntax and style of *Star*’s query notation has strong echoes
with SQL’s syntax - deliberately so.  Specifically, we take SQL’s
_relational calculus_ subset - the language of wheres and of boolean
combinations.  *Star*’s query expressions do not have the equivalent of
explicit relational join operators.

* Menu:

* Query Conditions::
* The iterable contract::
* Answer Templates::


File: guide.info,  Node: Query Conditions,  Next: The iterable contract,  Up: Anatomy of a Query

5.1.4.1 Query Conditions
........................

The condition takes the form of a boolean combination of _predications_;
a predication is either a normal boolean-valued expression, a _match_
condition, or a _search_ condition.  Various types of boolean
combinations are supported; the most common being conjunction ('&&'),
disjunction ('||') and negation ('\+').

   We have already seen match conditions; for example:
     some(X) .= opValue
   is a match condition that is satisfied if the value of 'opValue'
matches the pattern 'some(X)'.  A successful match has the additional
effect of binding 'X' to the value embedded in the 'some' value.(1)

   Where a match condition has at most one way of being satisfied, a
search condition can potentially have many solutions.  Search conditions
look like:

     PATTERN in EXPRESSION

   We saw an example of this earlier in our grandparent query:
     (X,Z) in parents
   As should be anticipated at this point, search is realized via a
contract.  This allows us to search any type - so long as the 'iterable'
contract is implemented for that type.

   The final form of query condition is simply the boolean-valued
expression.  Note that, unlike the other forms of query condition,
boolean expressions are test-only: they cannot result in bindings for
query variables.

   ---------- Footnotes ----------

   (1) The form 'X^=opValue' is actually a short form of the same
condition.


File: guide.info,  Node: The iterable contract,  Next: Answer Templates,  Prev: Query Conditions,  Up: Anatomy of a Query

5.1.4.2 The 'iterable' contract
...............................

The 'iterable' contract is similar in intention to the 'folding'
contract we saw before.  However, it is more tuned to supporting
different combination of queries.

   The 'iterable' contract looks like:
     contract all s,e ~~ iterable[s->>e] ::= {
       _iterate:all r ~~ (s,(e,iterState[r])=>iterState[r],iterState[r]) => iterState[r].
     }
   This contract states that to search a collection, you have to be able
to iterate over it using the '_iterate' function.  The '_iterate'
function uses the 'iterState' type to help guide the search:
     all t ~~ iterState[t] ::= noneFound      -- no results yet
                           | noMore(t)        -- all needed results found
                           | continueWith(t)  -- keep going
                           | abortIter(string).
   The different cases in the 'iterState' type codify different things
that can happen during a search.  The most commonly used case is the
'continueWith' case - which is used to encapsulate the results found so
far.

   There are two companion contracts to the 'iterable' contract: the
'indexed_iterable' contract supports search over key/value pairs and the
'generator' contract is used to help construct answers.


File: guide.info,  Node: Answer Templates,  Prev: The iterable contract,  Up: Anatomy of a Query

5.1.4.3 Answer Templates
........................

The result of a query is governed by the _answer template_ of the query
abstraction.  There are two main forms of answer template: the
expression template and the fold template.

Expression Template
     A EXPRESSION TEMPLATE is simply an expression.

   The expression template is evaluated for each successful way that the
query condition can be satisfied; typically, there are free variables in
the template expression that refer to variables bound in the query
condition.  That way.  values found during the search can be extracted
and made part of the overall answer.

Fold Template
     A fold template is used when it is desired to aggregate over the
     found solutions.  The form of a fold template is:

     fold Exp with Fn

   We saw this form of query at the beginning of the book where we
looked at nice ways of adding up elements of a list:

     { fold X with (+) | X in L }

Bounded Query
     A bounded query is similar to the regular query except that we
     restrict the search to a fixed number of answers.

   The form of a bounded query template is:

     N of Exp
   For example, since we know that a person can have at most 2 parents
(special gene therapy excepted), we can ask for someone's parents using:

     parentsOf(X) => { 2 of P | (P,X) in parents }


File: guide.info,  Node: Classifying,  Prev: Queries,  Up: Making choices

5.2 Classifying
===============

Recall that we asserted that classification is a key part of any outward
facing system: the system has to decide how to act on an incoming
request.  It may be instructive to see just how the query formalism can
be used to help.

   Let us imagine a system that has some external face, for example, a
server that allows users to fetch and store documents.  We have a
business requirement to allow anyone to store and fetch documents;
however, we also reserve the right to not process bad documents.(1)

   Since we anticipate enormous success for our service, and we also
anticipate that some people will try to game our system in order to
further their own nefarious goals.  So, we have to put in place a
classification system that can decide how to process requests and which
we anticipate will need continuous evolution.

   We have already claimed that the simplest online processing system
can be viewed as having four components: a source of events, a
processor, a sink where the output of the system is targeted and a
knowledge base that is used to inform the processor.

 [image src="images/online.png" ]

Figure 5.2: A Simple Event Processor

   One of the tasks that the processor must perform is to decide how to
react to incoming events; this is an example of a _classification
problem_; in this case we are classifying events into those we choose to
act on and those we will ignore (or complain about).

   Classification always depends on a combination of information gleaned
from the event itself and on global or contextual information.  In our
case, we can usefully breakdown the contextual knowledge into two kinds:
_rules_ that embody our processing policy and _reputation_ data that the
system has collected about external entities.

 [image src="images/classifier.png" ]

Figure 5.3: Rules and Reputations

   We distinguish these two sources of knowledge, partly because the way
that we collect and use them are different; but mainly because they are
about different things.  However, we can use our query formalisms -
together with functions - for both.

   ---------- Footnotes ----------

   (1) However that is defined!


File: guide.info,  Node: Boiling the Ocean,  Next: Concept index,  Prev: Making choices,  Up: Top

6 Boiling the Ocean
*******************

It is a commonplace in software engineering that you should not try to
'boil the ocean'; which is a synonym for 'biting off more than you can
chew'.  However, it _is_ possible to build very large scale systems if
you approach it in the right way.  This is the purview of architecture
and of software development teams.

   The 'right' way to boil an ocean is one cup at a time.  The 'smart'
way to do it is to build a machine that makes cups that boil the ocean.

   Building software in the context of a team is quite different to
writing individual functions in a program.  It is not enough for your
code to compute the correct function; it must also interact properly
with the environment it is in.  As a result, professional programmers
find themselves often more concerned with making sure that all the
'pieces' are in the right place than simply the correctness of an
algorithm.(1)  Interfaces, contracts, APIs and integration issues often
dominate the system builder’s landscape.

   One’s attitude to basic language features like types is also
different: having to deal with the knowledge that is in your co-worker’s
head (and not in yours) should be enough to convince anyone of the
merits of strong static types.

   *Star*'s modularity features are built from the same functional
programming foundation as the other features of the language.  This has
important implications for the programmability of larger systems.

* Menu:

* Packages are records::
* Existential types::
* Abstract data types::
* Phew::

   ---------- Footnotes ----------

   (1) This is not to deny that correctness is important.  It is just
that algorithmic correctness is _not enough._


File: guide.info,  Node: Packages are records,  Next: Existential types,  Up: Boiling the Ocean

6.1 Packages are records
========================

If we take a slightly deeper look at *Star*'s package we will see some
surprising features; features that are sadly not very common in
programming languages.

   Let's start with a super simple source package:
     ss{
       public double:(integer)=>integer.
       double(X) is X*2.
     }
   This package structure is semantically equivalent to a variable
declaration:
     ss:{double:(integer)=>integer}.
     ss = let{
       double:(integer)=>integer.
       double(X) is X*2
     } in {. double=double .}
   In effect, a package reduces to a variable declaration whose type is
a record type and whose value is an anonymous record of all the public
functions defined in the package.

   We have seen this before, but it may be worth taking another look at
the special form:
     {. double=double .}
   This is a record structure; the periods in the braces signify that
the record is not _recursive_: the definitions within the record cannot
reference each other.(1)  We use this slightly complicated way of
defining the package record because not all members of the package are
public -- and this formulation allows us to be precise in what elements
of the package record can be seen by others.

   What about types though?  Like other programming languages *Star*
allows us to export types from packages too:
     simple{
       public all t ~~ foo[t] ::= foo(t) | bar.

       public fooMe:all t ~~ (t)=>foo[t].
       fooMe(X) => foo(X)
     }
   The ability to export types -- and their constructors -- is an
extremely important part of the package functionality.  However, having
types in records is not a common feature in programming languages --
this may be one reason why the correspondence between packages and
records is not more widely understood.

   It turns out that we _can_ account for types like 'too' in our
semantics of packages.  Similarly to our previous unfolding, the simple
package is equivalent to:
     simple: exists foo/1 ~~ {
       type foo/1.

       foo:all t ~~ (t) <=> foo[t].
       bar:all t ~~ foo[t].

       fooMe:all t ~~ (t)=>foo[t].
     }

     simple = {
       all t ~~ foo[t] ::= foo(t) | bar.

       fooMe:all t ~~ (t)=>foo[t].
       fooMe(X) => foo(X).
     }
   This is clearly quite a bit more complex than our super-simple
example; but the basic story is still there: a package consists of a
variable definition whose value is an anonymous record.  In this case,
the anonymous record contains a type, two constructors as well as a
regular function.  It also includes a _type_ statement that, in this
case, informs the type system that the existentially quantified type foo
type should also be viewed as being part of the record.

   Having constructors in a type is only a small extension to the
conventional notion of a record -- while many languages restrict records
to containing just data values, most functional programming languages do
allow functions in records.  A constructor is just a special kind of
function.

   On the other hand, having a type in a record is quite different.

   ---------- Footnotes ----------

   (1) This is somewhat analogous to the difference between a let and a
letrec form in languages like SML.


File: guide.info,  Node: Existential types,  Next: Abstract data types,  Prev: Packages are records,  Up: Boiling the Ocean

6.2 Existential types
=====================

What does it mean to have a type in a record?  From a programmer's point
of view it is actually quite a natural extension of the concept of a
record; there does not seem to be any intrinsic reason why a record
shouldn't have types in it.  However, the logic of this bears a deeper
look.

   The declaration of the 'foo' type involves the use of an
_existential_ quantifier:
     simple:exists foo/1 ~~ {
       ...
     }
   The meaning of an existentially quantified type is complementary to
the universally quantified type.  An existential type quantification is
an assertion that the type exists, in this case that 'foo' exists and is
a type constructor of 1 argument.

   The statement:
     type foo/1.
   is a _type_ statement.  Its role is to 'bridge the gap' between the
quantifier and external views of the record: in effect, it is a reminder
that a _type annotation_ such as
     X:simple.foo
   means to use the type that is embedded in the simple record (actually
package) and assign 'X''s type to that embedded type.

* Menu:

* Use and evidence::
* Using existentially quantified types::
* Wrapping up packages::


File: guide.info,  Node: Use and evidence,  Next: Using existentially quantified types,  Up: Existential types

6.2.1 Use and evidence
----------------------

To get a better grip on type quantification in general and existentially
quantified types in particular it can be helpful to see what the rules
and expectations are for a universally quantified type.  Occurrences of
any type variable can be classified into two forms -- depending on
whether the context is one of _using_ a variable or whether one is
_defining_ it -- or providing evidence for it.

   Consider the simple function 'd' with its type annotation:
     d:all t ~~ arith[t] |: (t)=>t.
     d(X) => X+X.
   In the equation that defines 'd' the rules of type inference lead us
to determine that -- within the equation -- type of 'X' is 't'.  The
_only_ assumption we can make about 't' is that it implements the
'arith' contract; no other constraints on 't' are permitted.

   For example, if we had forgotten the 'arith' constraint, the compiler
would have complained because we try to add 'X' to itself, which implies
that 'X' had an arithmetic type.

   In general, whenever we define a universally quantified type we
cannot make any assumptions about what it can and cannot do - apart from
any explicitly introduced constraints.

   On the other hand, when we _call_ 'd', the rules for type variables
are more generous: calls of the forms 'd(2)' and 'd(2.4)' are both
permitted because we are allowed to _use_ any type -- that implements
the 'arith' contract.

   The reason we can substitute any type for 't' is because 't' is bound
by a universal quantifier: an all quantifier means we can use any type
for 't'.

   We can summarize this by stating that, for a universally quantified
type,

   * we can _use_ it for any type, but
   * we can make no assumptions about it when giving _evidence_ for a
     correct implementation.

   For existentially quantified types the situation is reversed: when
giving evidence in an implementation involving an existential type we
can use what ever type we want -- again, providing that other
constraints have been met -- but _outside_ the defining expression we
can't make any assumptions or additional constraints about the
quantified type.

   For various reasons, which we will explore further, existentially
quantified types are mostly associated with records -- like the package
record we saw earlier.

   As with universally quantified types, there are two kinds of contexts
in which we use existentially quantified type variables: _use_ and
_evidence_ contexts.  In the former we are using the type and in the
latter we are providing evidence that an expression has the right type.

   The existential quantifier means that within an instance of this
record we can instantiate foo to any type that meets the constraints.
The simplest way is to provide a type definition for it:
     simple = {
       all t ~~ foo[t] ::= foo(t) | bar.
       fooMe(X) => foo(X).
     }
   We can do this because, within the implementing expression, we can do
whatever we like for the type 'foo' - so long as it exists.

   Notice that we actually needed to achieve two separate but related
goals when describing the package as a record: we needed to define a
type within the record and we need to be able to have external
references to the 'foo' type field.

   When we use a type from a record, we can make use of it's existence;
but we cannot further constrain it.  For example, we can use simple's
'foo' type, as well as the 'fooMe' function that relies on it; for
example, in:
     m:for all t ~~ (t)=>simple.foo[t].
     m(X) => simple.fooMe(X).
   There is something a little different going on here: the type of 'm'
appears to be dependent on a field of the variable 'simple'; i.e., the
type of 'm' apparently depends on the _value_ of 'simple'.  This is not
something that we would normally sanction in a type system because of
the potential for disaster.

   For example, consider the scenario where ''simple' is not simple';
i.e., suppose that its value were computed; for example its value might
depend on a conditional computation.  In that case the actual type
'simple.foo' might also depend on the conditional computation;
furthermore, different invokations could easily result in having a
different actual type being exported by 'simple'.  Why does this not
cause problems?

   Normally such 'dynamic types' do cause substantial problems.
However, the type rules for existentially quantified variables are
crafted so that _it must not matter_ what the actual type 'simple.foo'
is.  So long as no additional constraints on the simple.foo are
permitted then the program is provably compile-time type-safe.  I.e.,
uses of an existentially quantified type may not further constrain the
type -- the exact complement of the situation with universally
quantified types.

   Note that this also relies on single assignment semantics.  It is not
enough to constrain 'simple.foo' to be effectively unconstrained in its
usage, it must also be that the simple variable cannot itself be
changed.  Luckily for us, this is true for packages even if it may not
be true for all variables.

   Of course, as with universally quantified types, we can constrain the
existential type with a type constraint.  This would mean that, when
implementing it we have to give evidence for the constraint being
satisfied and when using it we could rely on that implementation -
without knowledge of the actual implementation.


File: guide.info,  Node: Using existentially quantified types,  Next: Wrapping up packages,  Prev: Use and evidence,  Up: Existential types

6.2.2 Using existentially quantified types
------------------------------------------

So, what _are_ the rules for existentially quantified types?  The first
is one that we have already been looking at:

   An existential variable may be bound to a type when providing
_evidence_ that a value has a certain type, but may not be constrained
when _using_ a value with an existential quantifier in its type.

   The second is related to this:

     Each occurrence of an existentially quantified type is potentially
     different.

   Think about a function with the type:
     exFn:for all t ~~ (t)=>exists e ~~ R[e,t]
   For the moment, we don't much care about 'R'.  Now, consider how we
might use 'exFn':
     X1 = exFn("alpha")
     ...
     X2 = exFn("alpha")
   An important question is 'what is the relationship between the type
of 'X1' and the type of 'X2'?'.  Unfortunately, the fundamental answer
is 'we cannot know in general' -- because we cannot assume that 'exFn'
is without side-effects in its implementation -- which in type terms
means effectively there is no relationship: they are different.  The
reason is that the internal type used within the implementation of
'exFn' may result in _different_ instantiations for 'e' for each
invocation.  The result is we cannot assume _any_ link between the types
of 'X1' and 'X2': they are different.  This has some serious
consequences for how we use existentially quantified types.

   On the other hand, consider the similar sequence of definitions:
     Y1 = exFn("alpha")
     ...
     Y2 = Y1
   In this case we _do_ know that the type of 'Y1' is identical to the
type of 'Y2'.  This leads us to the third rule:

   Each _use_ of an existential quantification introduces a new type --
called a _Skolem type_(1) -- that follows the normal inference rules for
all types.

   I.e., once a type has been introduced as a Skolem type, it behaves
just like any regular type and the normal rules of inference apply.
This applies equally to the two fragments of code above; but the
additional constraint on the immutable values of 'Y1' and 'Y2' make it
easier to propagate type information.

   We can see this a little clearly by looking at the effective type
annotations of 'Y1' and 'Y2':
     Y1:R[e345,string]
     Y1 = exFn("alpha")
     ...
     Y2:R[e345,string]
     Y2 = Y1
   where 'e345' is the skolemized variant of the existential type 'e'.

   The effective annotations for 'X1' and 'X2' will have different
skolem constants:
     X1:R[e235,string]
     X1 = exFn("alpha")
     ...
     X2:R[e678,string]
     X2 = exFn("alpha")
   If 'Y1' or 'Y2' were declared to be re-assignable variables then,
once again, we would not be able to connect the types of 'Y1' and 'Y2'
together.

   ---------- Footnotes ----------

   (1) Technically, the type is denoted by a Skolem Constant or a Skolem
Function.


File: guide.info,  Node: Wrapping up packages,  Prev: Using existentially quantified types,  Up: Existential types

6.2.3 Wrapping up packages
--------------------------

Our original simple package record had the type

     simple: exists foo/1 ~~ {
       type foo/1.
       foo:all t ~~ (t) <=> foo[t].
       bar:all t ~~ foo[t].

       fooMe:all t ~~ (t)=>foo[t].
     }

   The type signature has a type 'foo' and a constructor 'foo' in it.
This is permitted because types and values have different name spaces in
*Star*.(1)

   Why, one might ask, is it so important for packages to have this kind
of semantics?  After all, few other programming languages make the
effort to give a first class semantics for modules.(2)  The most
straightforward answer is that it likely will not matter unless your
programs because very large.

   In mega-scale applications, programming between modules can easily
become a major headache if not semantized (sic) correctly.  However, we
shall see an application of this for much smaller systems in Chapter 8
when we discuss building platforms rather than simple applications.

   ---------- Footnotes ----------

   (1) Not allowing that would cause significant hardship for
programmers: it would require that program names could not be the same
as type names; including constructors like foo.

   (2) A notable exception being SML.


File: guide.info,  Node: Abstract data types,  Next: Phew,  Prev: Existential types,  Up: Boiling the Ocean

6.3 Abstract data types
=======================

Abstract data types can be viewed as the mathematics behind object
oriented programming.(1)

_Abstract Data Type_
     An abstract data type is a mathematical model of a set of related
     values that share a common set of semantics.

   In programming, it is the _common_ semantics that defines the
structure; but, of course, programming languages are not able to capture
the full semantics of a program or type and hence the stand-in for this
is usually a type specification.

   Perhaps an example is overdue.  In our chapter on Collections we
looked at many operators over collections and not a few example
collection types.  Although programs using the stream contract are
fairly abstract, the type of the collection itself is still visible.
Suppose we wanted to build a set structure where only the fact that
there is a set, and the set-like operators over the set were visible.
The representation type for the set should otherwise be completely
opaque.

   One might start with a type definition that defines some operators
over sets:

     exists coll/1 ~~ genSet ::= genSet{
       type coll/1.
       z:all t ~~ coll[t].
       addElement:all t ~~ (t,coll[t])=>coll[t].
       present:all t ~~ (t,coll[t])=>boolean.
     }

   The essence of this type declaration is a collection of operators
that define set-style operators.  By protecting the coll type with an
existential quantifier, we ensure that the representation of genSet
values is not accessible externally; whilst at the same time we do allow
other programs to _use_ the set operators.

   One example implementation of 'genSet' might use lists to represent
the set structure itself:

     LS = genSet{
       all t ~~ coll[t] <~ list[t].
       z = list of [].
       addElement(X,L) where X in L => L.
       addElement(X,L) => list of [X,..L].
       present(X,L) => X in L
     }

   The statement:
     all t ~~ coll[t] <~ list[t].
   which is a type alias statement, represents one of the ways that we
can give evidence for the existence (sic) of the coll type.  We could
also have simply declared that:

     type coll = list

   Given 'LS', we can use it like a set generator -- 'LS' provides a set
of operators over sets:

     Z = LS.z

     One = LS.addElement(1,Z)

     Two = LS.addElement(2,One)

   The type of 'LS' gives no clue as to the internal type used to
represent sets generated by it:

     LS:genSet

   But 'Z', 'One' and 'Two' have more interesting types:

     Z:collK341[integer]

   where 'collK341' is a Skolem type -- a unique type generated when we
assign a type to LS. In effect, LS is a module that exports the set type
and associated operators; this module is referenced by name and is used
to construct particular sets.

   A reasonable question here is 'where is the Abstract Data Type?'.
What we have is a record with a type and some functions in it.  Recall
that an ADT is a 'model of a set of related values that share a common
set of semantics'.  The semantics in common are the functions in the
record; and the type itself is the existentially quantified type in that
record -- coll.

   Notice how we index off the 'LS' variable to access the operators for
this set; even while passing into it instances of sets created and
modified by LS. This is one of the hallmarks of a module system.

* Menu:

* Opening up::
* Injection::
* Extensible types::

   ---------- Footnotes ----------

   (1) Not to be confused with Algebraic Data Types -- which represent
the mathematical foundation for enumerations and other non-object
values.


File: guide.info,  Node: Opening up,  Next: Injection,  Up: Abstract data types

6.3.1 Opening up
----------------

One of the reasons that we are so interested in establishing a 'normal'
semantics for modules and ADTs is that we can develop systems where the
contents of a module depends on some additional computation; i.e., we
can use _functions over modules_.  For example, we can show that _aspect
oriented programming_ and _dependency injection_ can be realized just
using normal code structuring with functions and let environments.

   Techniques like dependency injection are typically applied to large
programs; unfortunately that makes constructing small examples a little
forced.  So, we'll use a crow-bar to open a soda bottle.  Imagine, if
you will, that we needed to define a new arithmetic type that supported
arbitrary fractions.

   Floating point numbers are fractions.  But they do not permit the
representation of all fractions -- e.g., it is not possible to represent
1/3 exactly in IEEE 754.

   However, while we want to expose the type, and a set of operator
functions, we definitely do not want to expose anything about the
implementation of fractional numbers: as far as users are to be
concerned, the type fraction is to be completely opaque and might be
implemented in any way.

   Let us start with an interface; which in this case will take the form
of a record type:

     fractionals ::= exists fraction ~~ fracts{
       type fraction.

       frPlus:(fraction,fraction)=>fraction.
       frToString:(fraction)=>string.

       frParse:(string)=>fraction.
       fraction:(integer,integer)=>fraction
     }

   One of the first things to note here is that fraction is
existentially quantified; secondly we need to ensure that the set of
operators we expose is complete.  Our interface is not really complete,
but includes two critical operators: a means of constructing fractions -
via the fractions and frParse functions - and a means of escaping from
the world of fractions to other types (in this case 'string' via
'frToString').

   Here we are mostly concerned with _using_ fractions, so we will
assume that we have at least one implementation -- courtesy of the 'FR'
variable:

     FR:fractionals

   One way to use our implementation of fractions would be to reference
the needed operators via the 'FR' variable:

     F0 = FR.frParse("3/4")

     F1 = FR.fraction(1,2)

     F2 = FR.frPlus(F0,F1)

     show FR.frToString(F2)   -- results in 5/4

   However, we can do rather better than this in *Star*.  We have
already encountered the import statement; there is an analogous
statement that allows us to unwrap a record like 'FR' in a binding
environment -- such as:

     let{
       open FR

       F0 = frParse("3/4").
       F1 = fraction(1,2).
       F2 = frPlus(F0,F1).
     } in
       show frToString(F2)   -- results in 5/4

   The 'open' statement has a similar effect to the package import: it
enables the functions, types and other elements that are embedded in a
record to be made available as normal identifiers within the normal
scope of the let action (or expression).

   Of course, this code is still fairly clumsy; since we would like to
use normal arithmetic notation over fractions; which we can do by
implementing the arith contract:

     let{
       open FR.

       implementation arith[fraction] => {
         X+Y => frPlus(X,Y)
         ... -- more operators needed
       }
     } in {
       F0 = frParse("3/4").
       F1 = fraction(1,2).
       F2 = F0+F1.

       show frString(F2)
     }

   We can improve this further by also implementing the coercion
contract between 'strings' and 'fractions':

     let{
       open FR.

       implementation arith[fraction] => {
         X+Y => frPlus(X,Y)
         ... -- more operators needed
       }
       implementation coercion[string,fraction] => {
         _coerce(S) => frParse(S)
       }

       implementation coercion[fraction,string] => {
         _coerce(F) => frToString(F)
       }
     }

   This allows us to use a more natural notation for expressions
involving our fractions:

     let{
       open FR.
       ...
     } in {
       F0 = "3/4" :: fraction.
       F1 = fraction(1,2).

       show F0+F1
     }

   While much better than our original, we still have too much code to
write to use the fraction type: we have to get the type and then
demonstrate the appropriate implementations.  We want to be able to
combine everything that is important about fractions into a single
structure.

   There is a straightforward way we can do this.  Our original
signature for fractionals simply required the presence of the fraction
type.  What we can do is further require that the arith and appropriate
coercion contracts are also implemented; we do this by constraining the
type definition for 'fractionals':

     fractionals ::= exists fraction ~~ arith[fraction], coercion[string,fraction], coercion[fraction,string] |: fracts{
       type fraction.

       frPlus:(fraction,fraction)=>fraction.
       fraction:(integer,integer)=>fraction.
     }

   Since we are using contracts we do not need the explicit 'frParse'
and 'frToString' functions in the signature any more.

   When we instantiate a 'fracts' record we must provide -- within the
record itself -- appropriate implementations of 'arith' and 'coercion':

     FX = fracts{
       fraction <~ myFraction.
       implementation arith[myFraction] => {
         X+Y => frPlus(X,Y).
         ... --- more operators needed
       }
       implementation coercion[string,myFraction] => {
         _coerce(S) => frParse(S)
       }
       implementation coercion[myFraction,string] => {
         _coerce(F) => frToString(F)
       }
       ...
     }

   Notice that we implemented arithmetic for the internal 'myFraction'
type.  We could have equally implemented the contract for 'fraction'
type too; the key requirement is to provide evidence that arithmetic is
implemented for the type.

   The 'FX' record now has everything we want to expose about fractional
numbers.(1)  If we open the structure then indeed we can write programs
like:

     let{
       open FX.
     } in {
       F0 = "3/4" :: fraction.
       F1 = fraction(1,2).
       show F0+F1.
     }

   This is virtually equivalent to the code we might have written if we
were importing a package with the definition of the 'fraction' type in
it.  The difference is that we have access to the full expressive power
of the language in computing 'FX'.

   ---------- Footnotes ----------

   (1) Assuming that we added the missing operators that we would
actually need.


File: guide.info,  Node: Injection,  Next: Extensible types,  Prev: Opening up,  Up: Abstract data types

6.3.2 Injection
---------------

Injection is a technique where we specialize a program with additional
information; especially where that additional information is not part of
the normal argument flow.  Of course, it can be hard to be crisp about
'not part of the normal argument flow'; but injection is an
architectural technique to apply if and when it makes a difference in
the readability of your code.

   Injection is often used to manage _configuration_ of code: the
configuration is injected into the main program; for example, we might
configure an application server with the path name of a particular
application, or with the port on which the app server should be
listening.  Neither of these would normally be considered part of the
normal information flow in an application server.

   There is a standard functional programming style that can be used to
represent injection -- namely functions that return functions.  To take
an extremely simple example, suppose that we wanted to have a function
that counted the percentage of a class that passes an exam.  The
function itself is pretty simple:

     passes(L) => fraction(
       size(list of { X | X in L && X.score>Pass}),
       size(L))

   The configuration parameter here is obviously the 'Pass' value; this
is an important parameter to the function but is not part of the normal
argument flow (think about computing the pass count for an entire
school).

   We can use the function returning approach to inject an appropriate
value of 'Pass':

     passes(Pass) =>
       (L)=>fraction(
         size(list of { X | X in L && X.score>Pass}),
         size(L))
   Using this passes is a two-step process; we first use a specific
passing grade to construct the test function and then use this to
measure performance on groups of students:

     HS = passes(60)

     allPass = list of { C | C in Courses && HS(C)>0.80 }

   The two-step process is a key part of the injection technique.


File: guide.info,  Node: Extensible types,  Prev: Injection,  Up: Abstract data types

6.3.3 Extensible types
----------------------

Sometimes, rather than configuring a program with a numeric value (or
any other value for that matter), we need to configure it with a _type_.
This does not happen that often, and *Star*'s type constraints can
eliminate many cases where it might be needed; but the requirement still
shows up occasionally.  Where it can show up is in situations where you
need to develop customizable applications -- applications that can be
extended further by your customers without you having to change a line
of your own code.

   For example, you might need to build a system that attempts to
predict the behavior of equipment based on historical performance and
current demand.  This kind of software could be very useful in
determining a proper maintenance schedule.  Suppose that you determine
that what is important in predicting potential breakdowns is the number
of units processed and the number of days since the last scheduled
maintenance.  You might keep track of this in a record:

     maint ::= maint{
       date:date.
       units:integer.
     }

   And you will also probably have a description of each piece of
equipment:

     equip ::= equip{
       id:string.
       eqpType:string.
       nextMaint:date.
     }

   Using this, and similar records, together with some clever
algorithms, you design a function that determines the next most likely
piece of equipment to fail -- perhaps together with an expected failure
date:

     nextToFail:(list[maint],list[equip])=>(equip,date).

   The details of this algorithm, while critical to an actual
application, are of no concern to us here.

   Now, you deliver your software to your customer and the first thing
that they ask for is an ability to tweak it.  You see, you designed it
for generic pieces of equipment and they have particular pieces of
equipment, with particular foibles affecting the computations needed to
determine when equipment needs maintenance.  And they need to keep some
information in the description of equipment and maybe also in the
maintenance records that is not in your types.

   Your challenge is to permit this kind of extension without requiring
your code to be modified or even recompiled for each customer.

   The standard OO approach to addressing would be to permit the
customer to _sub-class_ some of the critical types (such as maint and
equip).  However, there are problems with using sub-types: in
particular, if your algorithm requires computing _new_ instances of data
structures then sub-classing cannot work: when your algorithm creates a
new equip record, it will not know how to create a customer variant of
that record:

     updateEquip(E,W) => equip{
       id = E.id.
       eqpType = E.eqpType.
       nextMaint = W.
     }

   with the result that the customer data is lost.  An alternative
approach is to allow some extensibility in the record by having a
special extra field:

     equip[t] ::= equip{
       id:string.
       eqpType:string.
       nextMaint:date.
       extra:t.
     }

   Since we do not want to constrain the kind of information our
customizer can store we make its type quantified.  The extra field is
there to support extensions; and, because we know about its existence,
we can carry the data with us:

     updateEquip(E,W) => equip{
       id = E.id.
       eqpType = E.eqpType.
       nextMaint = W.
       extra = E.extra.
     }

   The problem with adding such an extra field is its type: this version
changes the unquantified 'equip' type into a quantified one.  This will
have potentially devastating impact on your code -- especially if you
want to allow multiple extensions for multiple data structures.  The
reason is that potentially a large number of functions will be required
to carry the type parameters in their type signatures.  This is doubly
galling as these extra type parameters do not have any significance in
the main code: they are there only to support potential customizations.

   Instead of universal quantification, we can use an existential type
for the extra field:

     equip ::= exists t ~~ equip{
       id:string.
       eqpType:string.
       nextMaint:date.

       type t.
       extra:t.
     }

   This has the effect of permitting a local extension to a record type
while also effectively hiding the type from the main code.

   Of course, in order for extra to have any effect on our code, we have
to be able to make use of it within our algorithm.  This is another
customization point in the code: not only do we need to allow additional
data but we need to be able to reference it appropriately.  For example,
we might decide that the extra field should have a say in determining
the next maintenance date; so our updateEquip function should take it
into account -- but how?

   A simple way is to add to the equip record a set of extensibility
functions that the customer must supply, in addition to the data itself:

     equip ::= exists t ~~ equip{
       id:string.
       eqpType:string.
       nextMaint:date.
       type t.
       extra:t.
       extraDate:(t,date)=>date.
     }

   Then, our 'updateEquip' function calls this 'extraDate' function when
computing the new maintenance schedule:

     updateEquip(E,W) => equip{
       id = E.id.
       eqpType = E.eqpType.
       nextMaint = E.extraDate(E.extra,W).
       type t = E.t.     --- note evidence for type
       extra = E.extra.
       extraDate = E.extraDate.
     }

   Of course, the customer has to provide functions that create the
initial data structures, and the initial values of 'extra' and the
updating function 'extraDate'.  You, as the provider of the software,
will offer a default implementation:

     equip(Id,Tp,Maint) => equip{
       id = Id.
       eqpType = Tp.
       nextMaint = Maint.
       type t = ().  -- () is Star's void type
       extra = ().
       extraDate = (_,W) => W.
     }

   This approach meets our goals: we can allow customers of our software
access to key data structures in a safe way that does not require use to
modify our code for each customer or even to recompile it.


File: guide.info,  Node: Phew,  Prev: Abstract data types,  Up: Boiling the Ocean

6.4 Phew
========

This Chapter covers some difficult material!  We start with a
requirement to scale -- to be able to scale code from a single module
through to applications built by assembling libraries.  Along the way we
take in existential quantification and abstract data types.

   What we have not yet addressed are the needs of distributed
applications.  Managing distributed applications is one of the most
tedious and difficult challenges of modern software development.
However, before we can demonstrate *Star*'s approach to this, we must
look at _agent oriented systems_ and **actors** -- the subject of
Chattering Agents.


File: guide.info,  Node: Concept index,  Next: Function index,  Prev: Boiling the Ocean,  Up: Top

Concept index
*************


File: guide.info,  Node: Function index,  Next: List of Syntax Rules,  Prev: Concept index,  Up: Top

Standard function index
***********************


File: guide.info,  Node: List of Syntax Rules,  Prev: Function index,  Up: Top

List of Syntax Rules
********************



Tag Table:
Node: Top257
Node: Why be a Star programmer?839
Node: Programming has changed1384
Node: Programs are huge1942
Ref: Programs are huge-Footnote-13549
Node: Planning for change3642
Ref: Planning for change-Footnote-15938
Node: Programming safely and effectively5997
Node: Real-time is normal time.7002
Node: This train is leaving the station7755
Node: Technology9025
Node: Is Star for you?10704
Node: If you are already a Java (or C#)11156
Node: If you are already a C++ programmer13288
Node: If you are already a functional programmer15317
Node: Design goals for Star18449
Ref: Design goals for Star-Footnote-122938
Node: About this book23066
Node: Getting hold of Star24119
Node: Typographical conventions24474
Node: Acknowledgements25277
Node: A tour of Star26711
Ref: A tour of Star-Footnote-127871
Node: A first Star program27934
Node: Texture29497
Node: Lexical style30306
Ref: Lexical style-Footnote-132082
Ref: Lexical style-Footnote-232191
Ref: Lexical style-Footnote-332341
Node: Types32395
Node: Rules34139
Node: Patterns35082
Node: Packages36565
Node: Worksheets37246
Ref: Worksheets-Footnote-138491
Node: String interpolation38520
Node: Types more types and even more types39020
Ref: Types more types and even more types-Footnote-140890
Ref: Types more types and even more types-Footnote-240979
Node: Nominative types41095
Node: Reference Type42641
Node: Accessing Reference Variables44437
Node: Structural types44861
Node: Optional values46496
Node: The flavors of equality48637
Ref: The flavors of equality-Footnote-150526
Node: A tale of three loops50635
Node: A functional loop52210
Node: A totalizer query54233
Node: The homunculus in the machine55651
Node: Contracts and constrained types56657
Node: Implementing contracts59013
Node: Coercion not casting61133
Node: Actions64184
Ref: Actions-Footnote-166177
Node: There is more66245
Node: Functional Programming66804
Node: What is functional programming?68410
Ref: What is functional programming?-Footnote-170177
Node: Basics70299
Ref: Basics-Footnote-172090
Node: Functions72441
Ref: Functions-Footnote-177586
Ref: Functions-Footnote-277712
Node: Order of evaluation77783
Ref: Order of evaluation-Footnote-179906
Node: Another look at types80078
Node: Quantifier types80539
Node: Contract constrained types82714
Ref: Contract constrained types-Footnote-185226
Node: Algebraic data types85259
Ref: binaryStringTree90024
Node: Functions as values91440
Node: Functions and closures94028
Node: Let binding95396
Node: Generic types97835
Node: Generic functions100994
Ref: Generic functions-Footnote-1102002
Node: Going further102066
Node: Going even further107003
Node: Polymorphic arithmetic113538
Ref: Polymorphic arithmetic-Footnote-1116584
Node: Optional computing116649
Ref: Optional computing-Footnote-1118811
Node: Special syntax for optional values118901
Ref: Special syntax for optional values-Footnote-1120984
Node: A word about type inference121015
Ref: minusTypeFig123433
Ref: A word about type inference-Footnote-1125704
Node: Why is type inference restricted?125798
Node: Are we there yet?128647
Node: Collections129629
Node: Sequence notation131148
Node: Partial sequence notation132635
Node: The stream and sequence contracts134685
Ref: The stream and sequence contracts-Footnote-1136626
Node: Stream patterns136875
Node: Notation and contracts139297
Node: Indexing140520
Node: The indexed contract141407
Ref: The indexed contract-Footnote-1144146
Node: Adding and removing elements144325
Ref: Adding and removing elements-Footnote-1145896
Node: The index notation146040
Node: Implementing indexing149165
Ref: Implementing indexing-Footnote-1153356
Node: Index slices153488
Ref: Index slices-Footnote-1155451
Node: Doing stuff with collections155513
Node: Filtering156699
Ref: Filtering-Footnote-1158162
Node: The filter contract158223
Node: The sieve of Erastosthneses158520
Node: Mapping to make new collections161686
Ref: Mapping to make new collections-Footnote-1165003
Node: More Type Inference Magic165134
Ref: More Type Inference Magic-Footnote-1166717
Node: Compressing collections166798
Ref: leftFold168240
Ref: Compressing collections-Footnote-1173601
Ref: Compressing collections-Footnote-2173640
Node: Different types of collection173699
Node: The cons type174847
Node: The list type175768
Ref: twoarrays176892
Node: The map type176973
Node: The set type178863
Node: Collecting it together179708
Node: Making choices180481
Ref: onlineApp181035
Node: Queries182528
Ref: Queries-Footnote-1185202
Node: Satisfaction semantics185343
Node: Abstracting queries187492
Node: A Splash of Inference188524
Ref: A Splash of Inference-Footnote-1192916
Node: Anatomy of a Query193110
Node: Query Conditions194062
Ref: Query Conditions-Footnote-1195520
Node: The iterable contract195598
Node: Answer Templates196999
Node: Classifying198437
Ref: online199625
Ref: classifier200302
Ref: Classifying-Footnote-1200664
Node: Boiling the Ocean200697
Ref: Boiling the Ocean-Footnote-1202401
Node: Packages are records202518
Ref: Packages are records-Footnote-1205755
Node: Existential types205863
Node: Use and evidence207163
Node: Using existentially quantified types212681
Ref: Using existentially quantified types-Footnote-1215609
Node: Wrapping up packages215693
Ref: Wrapping up packages-Footnote-1216850
Ref: Wrapping up packages-Footnote-2217032
Node: Abstract data types217071
Ref: Abstract data types-Footnote-1220633
Node: Opening up220779
Ref: Opening up-Footnote-1227330
Node: Injection227412
Node: Extensible types229480
Node: Phew235693
Node: Concept index236415
Node: Function index236545
Node: List of Syntax Rules236698

End Tag Table
