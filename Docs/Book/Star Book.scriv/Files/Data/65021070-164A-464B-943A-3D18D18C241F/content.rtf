{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf200
{\fonttbl\f0\froman\fcharset0 Palatino-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl264\slmult1\pardirnatural\partightenfactor0

\f0\fs26 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Concurrency is about doing more than one thing at once. Concurrency in programming languages is important for two basic reasons: modern processors are easily capable of executing many tasks in parallel (and by implication, if your program does not then you may not be making good use of the machine) and many applications have to be able to respond to events without having to process everything in a fixed order.\
Building concurrent applications is hard for programmers primarily because many of the assumptions built into modern programming languages are not valid when performing activities concurrently. The most important of these assumptions relates to state \'97 for example, the values of mutable variables.\
In particular, the most basic assumption\
the value of a variable stays the same until it is modified\
is 
\i not
\i0  valid when the variable in question is shared by multiple concurrent activities. Or rather, technically, it is, but when a variable is shared, it is both difficult and tedious to determine when or if the variable has the expected state. It is difficult to overstate the importance of this.\
An interesting parallel shows up in dealing with weightlessness in space: astronauts \'91complain\'92 of items drifting about in the space station \'97 \'91nothing stays where you left it\'92. As a result, astronauts must get used to taping everything down while they are working.\
Programmers building concurrent systems have a similar experience: they are constantly having to \'91tape down\'92 pieces of state to make sure that they \'91stay put\'92.\
There are many potential models for concurrency in programming. However, two core ideas seem to stand out: the idea of 
\i sequence
\i0  and the idea of 
\i sharing
\i0 . Sequencing actions in most programming languages is almost completely implicit: the idea of performing actions in sequence is so hard-wired in languages like Java and C/C++ that one must think hard to step outside that frame of reference. However, in a concurrent execution, sequentiality is lost: two activitivies can proceed in parallel\{\\SCRV_FN=We reserve the right to conflate strict parallelism with apparent parellelism here.\\END_SCRV_FN\} and the relative order of actions between the activities is not possible to predict.\
The second concept that is so implicit is that of sharing: in a sequential program we assume that subsequent actions share the effects of earlier actions. But, again in a concurrent context, sharing is not obvious; and it is hard to reason about.\
Our approach is to make 
\i explicit
\i0  that which is 
\i implicit
\i0 . We reason about actions and their order explicitly \'96 by employing the classic software engineer\'92s approach of taking an extra level of indirection: we take a handle on the computation itself using the concept of a first-class task as a unit of execution. Similarly, we reason about sharing by focusing on explicit 
\i coordination
\i0  between tasks and on explicitly modeling sharing by 
\i communicating
\i0  between them.\
One thing that may strike the reader is that tasks, and as we shall see more generally computation expressions, are associated with actions. This is new in our text so far \'96 we have mostly focused on functions and related concepts. The reason for the change is straightforward: concurrency is inherently about the relative ordering of computation. That makes the action paradigm a natural fit for tasks and concurrency. However, we are fairly disciplined in our approach to actions; in particular, we strongly regulate implicit sharing of state. In fact, it is the implicit sharing of state that is the biggest issue with sequential programming.}