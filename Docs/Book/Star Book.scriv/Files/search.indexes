<?xml version="1.0" encoding="UTF-8"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="91B366E0-2440-4CFA-AC38-F3998A6E3A2A">
            <Title>Code Macros</Title>
            <Text>Code macros, as their name suggests, are normal Star functions that are invoked by the compiler on the text that the compiler is compiling. Code macros give the macro programmer almost the full power of Star to implement translation; in particular they can call other functions and use other types than quoted internally. However, code macros are a little more awkward to use than macro rules — there are some special restrictions on their form and what their scope is.
Code macros are embedded within a macro rule, in particular on the right hand side of a macro rule we can use a ## form — which, like its counterpart for validation rules, is a way of introducing nested scoping. For example, the top-level macro rule for processing triple graphs looks like:
 # graph{?Graph} ==&gt; list of [triples(Graph) ] ## {
  #fun triples(A) is wrapComma(mapSemi(triple,A))
  ...
The environment part of the ## block consists of regular Star code with one or more equations prefixed by a # to mark them as code macros. In this case the function triples is such a code macro. It is invoked by name in the ‘bound’ part of the ## block:
list of [triples(Graph)]
Only those functions within the block that are marked by the prefix # may be mentioned directly within the enclosing macro rule (or within any macro rules also defined within the ## block). Code macros must have the type
(quoted)=&gt;quoted
Other functions within the ## block may have any valid type — and this is where the great power of code macros comes from. In particular, whereas macro rules only ever ‘use’ quoted terms, code macros can use other types and can invoke arbitrary functions (of any type) in order to achieve their task.
In addition, where the evaluation of macro rules is somewhat complex — reflecting the potential for overlapping rules — the evaluation of code macros is identical to that of regular programs.
The triples code macro above calls two functions: mapSemi and wrapComma — neither of which are themselves code macros. The first applies the function triple to each term in the brace structure that is the source of the triple graph. The definition of mapSemi is:
fun mapSemi(F,A) is
  leftFold(F,list of [],unwrapSemi(A,list of []))
where unwrapSemi unfolds the sequence of terms from a ;-based structure to a list. Notice that mapSemi is not itself a code macro: it is simply a regular Star function; a fact that is reflected in it’s type signature:
mapSemi:((quoted)=&gt;quoted,quoted)=&gt;list[quoted]
There are some restrictions — you may not invoke any Star functions defined in the same package as the code macro itself is; other than functions defined within the same ## block. You can, however, refer to functions that have been imported from other packages. (This is more difficult to say than to follow.)
The wrapComma function is a kind of inverse to unwrapSemi except that it takes a collection of terms and forms it into the structure needed for list literals. It’s definition is:
fun wrapComma(list of [El]) is &lt;| ?El |&gt;
 |  wrapComma(list of [El,..More]) is &lt;| ?El , ?wrapComma(More) |&gt;
The definition of wrapComma makes good use of Star’s meta-language: in constructing what will be a list literal we have to form it from the appropriate structures. This figure illustrates the AST structure of the contents of a list literal:
#
Graphical View of a list Expression
It is useful that, in addition to being used to separate arguments in a list of arguments, the comma is also one of Star’s standard operators.
The most complex macros in our RDF macro package are those devoted to the processing of triples. The complexity arises from the inherent flexibility of the notation; in particular, any or all of the subject, predicate or object may be individual strings, named concepts or even lists of them.
We use an accumulator style of programming that is similar to what we saw earlier in [our Sieve of Erastosthenes][original-sieve]: the main functions all have a SoFar argument that represents the current accumulation.
For example, the function triple may result in any number of actual triples being generated; so it must generate a list of triples. It takes the form:
fun triple(SoFar,&lt;| ?Sub ! ?VP |&gt;) is
    SoFar++
    tripleJoin(trNounPhrase(list of [],Sub),
        trVerbPhrase(list of [],VP))
Here, SoFar refers to the triples found so far. This is appended to the triples gotten by parsing the components of the triple.
The ++ function is a standard function to concatenate sequences.
The function trNounPhrase may find a list of nouns — i.e., a list of concepts — in the subject part of the triple. The trVerbPhrase performs a similar function for the predicate and it also invokes trNounPhrase to scan for objects:
fun trVerbPhrase(SoFar,&lt;| [ ?VPs ] |&gt;) is
      SoFar++mapComma(trVerbPhrase,VPs)
 |  trVerbPhrase(SoFar,&lt;| ?V $ ?O |&gt;) is
      SoFar++pairJoin(trVerb(list of [],V),
        trNounPhrase(list of [],O))
trVerbPhrase has to ‘glue together’ an arbitrary collection of objects with the predicate; this is achieved with the pairJoin function:
fun pairJoin(L1,L2) is list of { (E1,E2) where E1 in L1 and E2 in L2 }
This function uses Star’s query expression notation to make the task simpler.
The triples in a triple graph are put together from the found subjects and predicate/object combinations — also using Star’s query notation:
fun tripleJoin(L1,L2) is
  list of { &lt;| n3Triple(?S,?V,?O) |&gt; where
    S in L1 and (V,O) in L2 }
The complete program (which we have not shown here) that implements the transformation of triple graphs is some 50 lines long. It is, in fact, part of Star’s standard library.</Text>
        </Document>
        <Document ID="14CDAA3D-AB4A-404A-A8ED-E9BDF3D057A0">
            <Title>Optional Values </Title>
            <Text>Notice that we identified a special case of noOne in our Person type. One reason for including this in a type is to be able to cope with non-existent people. However, this approach is not always the most effective one when modeling situations where a variable or field may not have a value.
Explicit null values, as found in Java and similar languages, cause a great number of problems: for example, null must have a special universal type; there are many scenarios where it is not possible for a variable to be null but the compiler must discover those for itself; and there is often a consequent tendency in defensive programming to test for null.
Star has no direct equivalent of a global null value. However, the standard option type allows the equivalent of selective nullability. Any variable that might not have a proper value can be marked with the option type rather than the underlying type. And you can use none in those cases to indicate the equivalent of no value.
So, for example, suppose that a Person might have a spouse — who is also a Person — but is not guaranteed to have one. Such a type can be described using:
Person ::= someOne{
   name : string.
   dob : date.
   spouse : ref option[Person]
}
Here we have done two things: we have eliminated the noOne case for Person and we have marked the spouse as being both read-write and optional.
Someone with no spouse would be written:
freddy : Person.
freddy = someOne{
  name = "Freddy".
  dob = today().
  spouse := none
}
whereas someone who has a spouse would be written:
someOne{
  name = "Lisa".
  dob = lastYear.
  spouse := some(johnny)
}
Of course, we can record freddy’s marriage to lisa using an assignment:
freddy.spouse := some(lisa)
lisa.spouse := some(freddy)</Text>
        </Document>
        <Document ID="D7266A1E-7AFB-4D05-884C-B2A2DB1909A5">
            <Title>Functional Programming</Title>
            <Text>There is a perception of functional programming that it is weird and difficult. This is unfortunate for a number of reasons; the most important being that functional programming is not weirder than procedural programming and that all programmers can benefit by programming functionally.
As for being difficult, a more accurate description would be that there is a deeper range of features in functional programming than in most modern programming languages: so a perception of complexity can arise simply because there is more to say about functional programming languages. However, the simplest aspects of functional programming are very simple and the ramp need not be steep.
What may be surprising to the reader who is not familiar with functional programming is that it is old: predating the origins of modern computing itself, that there is a huge amount that can be expressed functionally, and that functional programming is often at least as efficient and sometimes more efficient than procedural programming.
In this chapter we will show how we can utilize Star as a vehicle for functional programming. As a side-goal, we also hope to demystify some of the language and ideas found in functional programming.</Text>
        </Document>
        <Document ID="FBF32563-67FD-4E94-96CB-D9B123332453">
            <Title>Extensible Types</Title>
            <Text>Sometimes, rather than configuring a program with a numeric value (or any other value for that matter), we need to configure it with a type. This does not happen that often, and Star’s type constraints can eliminate many cases where it might be needed; but the requirement still shows up occasionally. Where it can show up is in situations where you need to develop customizable applications — applications that can be extended further by your customers without you having to change a line of your own code.
For example, you might need to build a system that attempts to predict the behavior of equipment based on historical performance and current demand. This kind of software could be very useful in determining a proper maintenance schedule. Suppose that you determine that what is important in predicting potential breakdowns is the number of units processed and the number of days since the last scheduled maintenance. You might keep track of this in a record:
maint ::= maint{
  date:date.
  units:integer.
}
And you will also probably have a description of each piece of equipment:
equip ::= equip{
  id:string.
  eqpType:string.
  nextMaint:date.
}
Using this, and similar records, together with some clever algorithms, you design a function that determines the next most likely piece of equipment to fail — perhaps together with an expected failure date:
nextToFail:(list[maint],list[equip])=&gt;(equip,date).
The details of this algorithm, while critical to an actual application, are of no concern to us here.
Now, you deliver your software to your customer and the first thing that they ask for is an ability to tweak it. You see, you designed it for generic pieces of equipment and they have particular pieces of equipment, with particular foibles affecting the computations needed to determine when equipment needs maintenance. And they need to keep some information in the description of equipment and maybe also in the maintenance records that is not in your types.
Your challenge is to permit this kind of extension without requiring your code to be modified or even recompiled for each customer.
The standard OO approach to addressing would be to permit the customer to sub-class some of the critical types (such as maint and equip). However, there are problems with using sub-types: in particular, if your algorithm requires computing new instances of data structures then sub-classing cannot work: when your algorithm creates a new equip record, it will not know how to create a customer variant of that record:
updateEquip(E,W) =&gt; equip{
  id = E.id.
  eqpType = E.eqpType.
  nextMaint = W.
}
with the result that the customer data is lost. An alternative approach is to allow some extensibility in the record by having a special extra field:
equip[t] ::= equip{
  id:string.
  eqpType:string.
  nextMaint:date.
  extra:t.
}
Since we do not want to constrain the kind of information our customizer can store we make its type quantified. The extra field is there to support extensions; and, because we know about its existence, we can carry the data with us:
updateEquip(E,W) =&gt; equip{
  id = E.id.
  eqpType = E.eqpType.
  nextMaint = W.
  extra = E.extra.
}
The problem with adding such an extra field is its type: this version changes the unquantified equip type into a quantified one. This will have potentially devastating impact on your code — especially if you want to allow multiple extensions for multiple data structures. The reason is that potentially a large number of functions will be required to carry the type parameters in their type signatures. This is doubly galling as these extra type parameters do not have any significance in the main code: they are there only to support potential customizations.
Instead of universal quantification, we can use an existential type for the extra field:
equip ::= exists t ~~ equip{
  id:string.
  eqpType:string.
  nextMaint:date.
  type t.
  extra:t.
}
This has the effect of permitting a local extension to a record type while also effectively hiding the type from the main code.
Of course, in order for extra to have any effect on our code, we have to be able to make use of it within our algorithm. This is another customization point in the code: not only do we need to allow additional data but we need to be able to reference it appropriately. For example, we might decide that the extra field should have a say in determining the next maintenance date; so our updateEquip function should take it into account — but how?
A simple way is to add to the equip record a set of extensibility functions that the customer must supply, in addition to the data itself:
equip ::= exists t ~~ equip{
  id:string.
  eqpType:string.
  nextMaint:date.
  type t.
  extra:t.
  extraDate:(t,date)=&gt;date.
}
Then, our updateEquip function calls this extraDate function when computing the new maintenance schedule:
updateEquip(E,W) =&gt; equip{
  id = E.id.
  eqpType = E.eqpType.
  nextMaint = E.extraDate(E.extra,W).
  type t = E.t.     — note evidence for type
  extra = E.extra.
  extraDate = E.extraDate.
}
A more succinct way of expressing this would be to use Star’s substitute operator:
updateEquip(E,W) =&gt; E substitute {
  nextMaint = E.extraDate(E.extra,W)
}
Of course, the customer has to provide functions that create the initial data structures, and the initial values of extra and the updating function extraDate. You, as the provider of the software, will offer a default implementation:
equip(Id,Tp,Maint) =&gt; equip{
  id = Id.
  eqpType = Tp.
  nextMaint = Maint.
  type t = ().  -- () is Star's void type
  extra = ().
  extraDate = (_,W) =&gt; W.
}
This approach meets our goals: we can allow customers of our software access to key data structures in a safe way that does not require use to modify our code for each customer or even to recompile it.</Text>
        </Document>
        <Document ID="96776348-B035-46DA-A5A5-210E99B93774">
            <Title>Front Matter</Title>
            <Notes>Note that in the “Metadata” pane of the Inspector, under “Section Type”, the “Default Subdocument Type” is set to “Front Matter”. This setting causes all subdocuments of this folder to use the “Front Matter” section type by default (that is, when “Structure-Based” is selected as the section type).

This saves us from having to manually assign the “Front Matter” section type for each document we place into this folder.

During Compile, documents assigned the “Front Matter” section type will be laid out differently from documents in the main body of the manuscript.</Notes>
        </Document>
        <Document ID="3B425F17-6AEB-4983-8C67-5167CFE5B4E9">
            <Title>Dedication</Title>
            <Text>










Insert dedication here.</Text>
            <Notes>Feel free to delete this document if you don’t need it.</Notes>
        </Document>
        <Document ID="2CE5E8C5-B016-4FE5-A5DC-4900BD2A9764">
            <Title>Doing Stuff With Collections</Title>
            <Text>One of the most powerful features of collections is the ability to treat a collection as a whole. We have already seen a little of this in our analysis of the visitor pattern. Of course, the point of collections is to be able to operate over them as entities in their own right. As should now be obvious, most of the features we discuss are governed by contracts and it is paradigmatic to focus on contract specifications rather than specific implementations.
The number of things that people want to do with collections is only limited by our imagination; however, we can summarize a class of operations in terms several patterns:
	•	Filtering
	•	Transforming into new collections
	•	Summarizing collections
	•	Querying collections
Each of these patterns has some support from Star’s standard repertoire of functions.</Text>
        </Document>
        <Document ID="EE17D847-A0F2-4AC2-98F4-30FCBBFA4ACA">
            <Title>Wrapping Up Packages</Title>
            <Text>Our original simple package record had the type
simple: exists f/1 ~~ {
  type foo/1.
  foo:all t ~~ (t) &lt;=&gt; f[t].
  bar:all t ~~ f[t].
  fooMe:all t ~~ (t)=&gt;f[t].
}
The type signature has a type foo and a constructor foo in it. This is permitted because types and values have different name spaces in Star. This is not in fact quite accurate; the precise type of the simple package record should be:

simple: exists foo/1 ~~ {
  type foo/1.
  foo:all t ~~ (t) &lt;=&gt; foo[t].
  bar:all t ~~ foo[t].
  fooMe:all t ~~ (t)=&gt;foo[t].
}

Otherwise we could not assume that the type simple.foo was the same as the type returned by fooMe (for example). We did not introduce it this way because the proliferation of foos in the example could lead to confusion. However, this formulation also means that we do not need to have the extra type alias in our package record itself.
Why, one might ask, is it so important for packages to have this kind of semantics? After all, few other programming languages make the effort to give a first class semantics for modules. The most straightforward answer is that it likely will not matter unless your programs because very large.
In mega-scale applications, programming between modules can easily become a major headache if not semantized (sic) correctly. However, we shall see an application of this for much smaller systems in Chapter 8 when we discuss building platforms rather than simple applications.

</Text>
            <Comments>Not allowing that would cause significant hardship for programmers: it would require that program names could not be the same as type names; including constructors like foo.
A notable exception being SML.</Comments>
        </Document>
        <Document ID="347E8EB6-6878-49AD-9309-1AB468ABC004">
            <Title>A Functional Loop</Title>
            <Text>A more idiomatic way of expressing a computation like the totalizer is to use a function. For example, we can write:
let{
  total:(cons[integer])=&gt;integer.
  total(nil) =&gt; 0.
  total(cons(E,L)) =&gt; total(L)+E
 } in total(L)
while short, this code too has some of the same drawbacks as the for iteration.
The type expression cons[integer] refers to the standard type of ‘cons lists’. Similarly, nil refers to the empty list and cons(E,L) refers to the list obtained by prepending E to the list L. We will explore this in more detail in Chapter 2.
Even if it is more declarative, there is still a lot of extra detail and architectural commitments here — like the commitment to cons lists and the commitment to integers. These result in a function that is needlessly restricted.
Like other functional languages, Star has a range of higher-order operators that may come to the rescue. For example, we can avoid the explicit recursion altogether by using leftFold:
leftFold((+),0,L)
where leftFold means
apply an accumulating function to the elements of the data, assuming that the applied operator is left associative.
This expression is clearly both more concise and higher-level than either the explicit loop or the explicit recursion; and it begins to illustrate the productivity gains that are potentially available to the functional programmer.
Using leftFold means that we can often abstract away the machinery of loops and recursion completely — instead we can solve the problem at a more holistic level. This is one of the hallmarks of functional programming – it is possible to eliminate many instances of explicit loops and recursions.</Text>
        </Document>
        <Document ID="6C84FEFB-DF6A-4B22-BA78-FF26ABF6C8EE">
            <Title>Code Repository</Title>
            <Text>Apart from simply being a place compiled code can be kept, a code repository has several other responsibilities: it must be possible to access the compiled code from multiple packages — including packages that were imported. In fact, a Star code repository is able to contain the compiled code of any number of packages — since code repositories are also used to hold compiled libraries as well as applications.
In addition, we have to be able to manage multiple versions of a given compiled package; in some cases an application may be using multiple versions of a package (different libraries may have dependencies on specific versions of a package).
The scale of the repository is also flexible: ranging from a and we must be able to manage different kinds of code repository: from individual packages, libraries of packages and even large-scale collections of libraries.
We will not go into the details of Star’s code repositories here. However, note that code repositories may store their code in different ways — file systems, memory, in a Star Archive. Furthermore, repositories can be combined into composite repositories.</Text>
        </Document>
        <Document ID="38B23195-E30E-47E7-8964-26CAEF29798E">
            <Title>Concurrent Actors</Title>
            <Text>A concurrent actor is similar to a regular light weight actor in that you can communicate with a concurrent actor using speech actions and you can define event rules for the concurrent actor.
However, a concurrent actor has an important performance guarantee: only one speech action may be processed concurrently by the actor. This makes it straightforward to ensure that the internal state of a concurrent actor is always consistent in the presence of concurrent access to the actor.
It should be noted that the internal structure of a concurrent actor is more complex than that of a regular light weight actor. This may translate into a run-time performance difference.
A concurrent actor is written using the concurrent prefix. For example, we can make our agentBank concurrent very straightforwardly:
agentBank = concurrent actor{
  private var accts := dictionary of []
  fun balance(N) is accts[N].balance()
  prc transfer(F,T,Amt) do{
    accts[F].debit(Amt)
    accts[T].credit(Amt)
  }
  on deposit(Nm,Amnt) on cashier do
    accts[Nm].debit(Amnt)
}
As might be clear if you have read up this point, a concurrent actor works by having an internal background task that is actually responsible for processing speech actions. This background task is responsible for actually responding to speech actions and it ‘serializes’ them – ensuring that only one is performed at any one time.
Performing speech actions on concurrent actors is identical to performing them on regular actors. However, concurrent actors have a different type – concActor of t – which means that one has to be careful when constructing functions that are to work with both kinds of actor. For example, the balQuery function:
balQuery:all t,a ~~ speech[t-&gt;&gt;a], a&lt;~{balance:(string)=&gt;float} |:
    (t,string)=&gt;float.
balQuery(A,U) =&gt; query A with balance(U)
will work with either of actor, concurrent actor or any entity that implements speech and whose API includes the balance function – because it’s type is carefully circumscribed. However, functions that have been type-specialized to work with actors will not type check when used with concurrent actors.</Text>
            <Comments>Like other references to the speech contract, the types in this function are slightly simplified.</Comments>
        </Document>
        <Document ID="5855F688-6A35-4E45-AF46-F6E6EEBB6150">
            <Title>Mapping to Make New Collections</Title>
            <Text>One of the limitations of the filter function is that it does not create new elements: we can use it to subset collections but we cannot transform them into new ones. The fmap function – part of the functor contract – can be used to perform many transformations of collections.
For example, to compute the lengths of strings in a list we can use the expression:
fmap(size,list of ["alpha","beta","gamma"])
which results in the list:
list of [5,4,5]
The fmap function is defined via the functor contract — thus allowing different implementations for different collection types:
contract all c/1 ~~ functor[c] is {
  fmap:all e,f ~~ ((e)=&gt;f,c[e])=&gt;c[f]
}
Notice how the contract specifies the collection type — c — without specifying the type of the collection’s element type. We are using a different technique here than we used for the sequence and filterable contracts. Instead of using a functional dependency to connect the type of the collection to the type of the element, we denote the type of the input and output collections using a type constructor variable as in c[e] and c[f].
We are also using a variant of the quantifier. A quantified type variable of the form c/1 denotes a type constructor variable rather than a regular type variable. In this case, c/1 means that the variable c must be a type constructor that takes one argument.
The reason for this form of contract is that functor implies creating a new collection from an old collection; with a possibly different element type. This is only possible if the collection is generic and hence the type expressions c[e] for the second argument type of fmap and c[f] for its return type.
One might ask whether we could not have used functional dependencies in a similar way to sequence and filterable; for example, a contract of the form:
  contract all c,e,f ~~ mappable[c-&gt;&gt;e,f] ::=  {
    mmap:((e)=&gt;f,c)=&gt;c.
  }
However, this contract forces the types of the result of the mmap to be identical to its input type, it also allows the implementer of the mappable contract to fix the types of the collection elements — not at all what we want from a fmap.
It is not all that common that we need to construct a list of sizes of strings. A much more realistic use of fmap is for projection. For example, if we wanted to compute the average age of a collection of people, which is characterized by the type definition:
person ::= someOne{
  name:string.
  age:()=&gt;float.
}
Suppose that we already had a function average that could average a collection of numbers; but which (of course) does not understand people. We can use our average by first of all projecting out the ages and then applying the average function:
average(fmap((X)=&gt;X.age(),People))
In this expression we project out from the People collection the ages of the people and then use that as input to the average function.
There is something a little magic about the lambda function in this expression: how does the type checker ‘know’ that X can have a field age in it? How much does the type checker know about types anyway?
In this particular situation the type checker could infer the type of the lambda via the linking between the type of the fmap function and the type of the People variable. However, the type checker is actually capable of giving a type to the lambda even without this context. Consider the function:
nameOf(R) =&gt; R.name
This function takes an arbitrary record as input and returns the value of the name field. The nameOf function is well typed, its type annotation just needs a slightly different form than that we have seen so far:
nameOf:all r,n ~~ r &lt;~ {name:n} |: (r)=&gt;n
This is another example of a constrained type: in this case, the constraint on r is that it has a field called name whose type is the same as that returned by nameOf itself.
The type constraint:
r &lt;~ {name:n}
means that any type bound to r must have a name field whose type is denoted by the type variable n in this case.
With this type signature, we can use nameOf with any type that that a name field. This can be a record type; it can also be a type defined with an algebraic type definition that includes a record constructor.</Text>
            <Comments>This also means that the collection type in fmap must be generic: it is not possible to implement functor for strings.</Comments>
        </Document>
        <Document ID="B3856A1E-A9B1-4A0C-AFB3-047EDD257F92">
            <Title>If You Are Already a Functional Programmer</Title>
            <Text>You have many choices for functional programming languages that are excellent. The author considers two languages that are principal sources of inspiration for many of the functional features of Star: Haskell and Standard ML – both of which are excellent; but not perfect.
For the functional programmer, the principal benefits of Star are readability, modernity and predictability.
One of the major drivers of the design of Haskell and (to a lesser extent) ML is conciseness. However, conciseness is not the same as readability. In modern software development environments there are many stakeholders beyond the developer. Having a language that is easy to follow by non-technical readers is a major benefit in mixed skill teams.
Like Haskell, Star has a powerful type system. Star’s type system has many features in common with Haskell’s type system – features that typically go beyond the capabilities of many OO languages. In particular, Star’s contract system is reminiscent of Haskell’s type classes; and Star’s existential and higher-kinded types give considerable expressive power to the programmer.
Star does not follow all of Haskell’s type features; and some type concepts are rephrased into terminology that is more familiar to main-stream (sic) programmers.
Like ML, Star has a powerful module system. However, unlike ML’s functors, Star modules are first class values. This means that there is no artificial separation between ‘ordinary’ programs and ‘functor’ programs.
The result is a balanced set of type features that provides capabilities that scale well from small programs to large systems.
Star’s evaluation is, like that of ML but unlike Haskell, strict. We believe that that makes it significantly easier to reason about the actual behavior and performance of programs. However, Star has a rich set of features that support productive concurrent and parallel programming – based on a combination of system threads and the features of Concurrent ML.
Like ML, Star is not a strictly ‘pure’ language. This was neither an accident nor an afterthought. Computer systems are built to fulfill purposeful activity (although there may be many times when the actual purpose is hard to discern). For example, if I deposit a check into my bank account, I require that the bank’s state is updated to reflect my new balance: the world has changed as a result of my action.
However, the converse does not follow: just because the world is stateful does not mean that all our programs should be needlessly stateful. Much, if not most, of a given application program can and should be crafted in a mathematical style – the merits of functional programming are very great.
Overall, the primary rationale in the design of Star is to empower the programmer in making obviously correct programs.</Text>
        </Document>
        <Document ID="C4EF0362-9FB3-4B1F-AB5E-B7CB6F16734B">
            <Title>A Component in Star</Title>
            <Text>One of the intentions behind boxes-and-arrows diagrams is to call out the major functional pieces of an application. The intuition is that individual components have a specific role in the application; but that they are typically ‘quite large’. We have already seen that a component may have multiple ports but we have not exposed what kind of computation may be going on inside.
One of the non-goals of boxes-and-arrows diagrams is to be a complete programming language. Instead, the idea is to capture the large scale granularity in a picture but to use text for the actual programming of components. Actually building components is best left to ‘real’ code; in our case, we denote the code of a component as a Star component; which is a special form of package.
The one exception to the strategy of using written code to build components is with composite components – components built by assembling and wiring other components. However, this is outside the scope of this book.
The code for a component must implement the various ports that the component has on the diagram and must also implement the functionality of the component. For example, our SplitComponent may start it’s implementation with:
import boxesNarrows.
component splitComponent =&gt; {
  respond incoming : {
    orders:event[order].
  }
  incoming = respond{
    on O on order do
      processOrder(O)
  }
  publish suppliers : {
    discriminator:(string)=&gt;port[{order:event[order]}].
  }
  originate parts : {
    assembly:set[(part,list[part])].
    supplier:set[(part,list[supplier])].
  }
  ...
}
The ports that are surfaced in the order processing diagram are also represented in the code for the splitComponent. Responding ports have a body that essentially the same as for an actor (or concurrent actor); typically most of the activity within a component is initiated by code that is present in or referred from responding ports.
Notice that, unlike regular actors, outgoing speech actions are also strongly indicated. Any outgoing speech actions must be ‘applied’ to one of the originating ports. For example, part of processing a parts order will be a query to the partsDB component; but the partsDB component is not explicitly identified in the splitComponent. Instead, the query is directed to the parts originating port:
assemblySuppliers:(part)=&gt;list[supplier].
assemblySuppliers(A) =&gt;
  query parts with list of { unique S where
      (A,Ps) in assembly &amp;&amp; P in Ps &amp;&amp;
      (P,Ss) in supplier &amp;&amp;
      S in Ss
    }
This function can be used by the order processing code to query the parts database for all the suppliers involved in a given assembly. It works by formulating the appropriate query speech action to the originating parts port.
The unique keyword here implies that the result will have duplicates eliminated from it.
Using a publishing port is slightly more complex than using a regular originating port. Since there may be any number of components attached to a publishing port we must first of all select which one we want to address our speech action to. We do this using the embedded discriminator function that is part of the publishing port. For example, to place a parts order for the supplier “Alpha Wheels Inc.”" we perform:
placeOrder(S,parts) do
  notify suppliers.discriminator(S.name) with
    order{ content=parts } on order
Notice that this is a regular speech action; the primary difference is that instead of a fixed recipient we compute who the recipient will be – based on the name of the supplier.</Text>
            <Comments>By functional we mean important to the solution not necessarily as in functional programming.</Comments>
        </Document>
        <Document ID="97D45E2F-EF81-4609-A86C-00F2C15F3887">
            <Title>Are We Done Yet?</Title>
            <Text>This book takes you on a journey starting with very simple Star programs and ending with a platform that is capable of being deployed on large scale distributed networks. In the meantime we also explored some quite deep topics in functional programming, concurrency and developing domain specific languages.
The truth is, of course, that we are only able to scratch the surface of many of the topics we cover. So, the question is what next?
If you want to go deeper into Star itself then the natural source is the language definition – which can be obtained at Github. This covers the language and its standard library.
If you want to go deeper in concurrency then there are several sources one may go to. However, we recommend Reppy’s [#Reppy1999] original book on Concurrent ML. Our only complaint about this book is that it is not based on Star!
There is much material on agent programming of various flavors. A good place to start is also Wikipedia. For a variety of reasons we also recommend investigating the different specifications produced by the Foundation of Physical Intelligent Agents FIPA.
Finally, dear reader, I would like to thank you for staying the course. I would like to recommend that you check out Star with your own programs and please keep us informed of your adventures by writing: fmccabe@gmail.com.</Text>
        </Document>
        <Document ID="898FBDE9-3A90-4F47-8F02-6185DD628B38">
            <Title>Requests</Title>
            <Text>Our final form of speech action is the request. A request is intended to denote a request that the recipient perform some action. This is subtly different to the query in that – apart from answering the question – a query should not cause any change of state in the recipient, whereas the request likely would.
Even though the difference between query and request may seem subtle to the average programmer the key difference is in the intended use.
The form of a request reflects the fact that an action is involved:
request Agent to Action
A simple request may be to invoke a procedure from the API; however complex scripts are also possible. For example, we can request that all stock items that are empty be deleted:
request Ag to delete (Id,_) from stocks where quantity(Id)=0
The action
delete (Id,_) from stocks where quantity(Id)=0
is part of the standard CRUD (Create-Read-Update-Delete) feature that allows collections to be updated.</Text>
        </Document>
        <Document ID="60D37027-5A0F-4CA1-BA8F-0F527FC7BB6D">
            <Title>Contents</Title>
        </Document>
        <Document ID="181BF4A6-AD8F-4E1F-B733-31D14717121E">
            <Title>Design Goals for Star</Title>
            <Text>Star is a multi-paradigm high-level symbolic language. It is designed to be scalable, readable, accurate, high performing and extensible.
Paradoxically, scalability in a programming language is always about large and small chunks of code. Scalability in Star is fostered by a range of elements that facilitate composition, change and re-use:
	•	The language is strongly statically typed. This encourages both safety and documentation.
	•	The type system is strong enough that there is very limited need to escape the type system. For example, modules can be given a first-class type semantics. This is important because it facilitates programmatic manipulation of modules in a safe manner.
	•	Programs are defined in terms of rules; for example, functions are defined in terms of equations. Apart from being more readable, rules are also a natural unit of change in an evolving system.
	•	A meta-language based on logical annotations makes it possible to build meaningfully connected documentation and facilitates processes such as code re-use, issue tracking, and code lifetime management.
	•	The package system is intrinsically versioned and abstracted away from any underlying storage system.
The syntax of Star is oriented towards readability rather than strict conciseness. The reason for this is that the programmer is only one of the stake holders in a given program. A readable program is one that is more easily trusted by non-programmers.
Experience also suggests that readability enhances programmer productivity also: much of team-based development involves comprehending and modifying other programmers’ code.
Star is a strongly, statically typed language. The purpose of a strong type system is to facilitate the communication of intent of the programmer. The purpose of static typing is to ensure that the compiler can rapidly ‘fail’ incorrect programs without requiring the program to be run. Furthermore, static type checking minimizes any run-time penalty for imposing type constraints.
Although Star is strongly typed, it uses type inference to eliminate much of the clutter that some type systems impose on the programmer – which itself is a productivity sink of course.
Generally, the stronger the type system, the more the language system can detect errors before programs are run. In addition, the more expressive the type system is, the less the temptation to try to subvert or bypass the type system.
However, even though it is technically feasible to completely eliminate type declarations of functions; doing so is in conflict with some of the other goals behind Star. For example, type declarations act as a form of documentation; and when there is a type error in your program, having no explicit type declarations can make tracking the culprit of the error very difficult. So all top-level variable definitions (typically functions) are required in Star to have explicit type annotations.
Star has a range of features that make exploiting parallelism easier to manage. For example, it has support for computation expressions and actors. Partitioning an application into different agents allows programming to follow a more human approach. Computation expressions allow the programmer to manipulate computations as easily as they do data values; that in turns greatly eases the development of parallel and concurrent applications.
There is no one technology that can solve all problems. This is as true for programming as for other domains. Star supports a range of programming paradigms that allows the developer to ‘use the best tool for the job’. However, we go beyond this ‘swiss army knife’ stance and make it straightforward to extend the language.
Virtually every non-trivial program can be factored into a combination of general purpose mechanism and specific policy for applying the mechanism. Star has powerful self-extension features that allow programmers to design their own policy structures (a.k.a. domain specific languages).
Many of Star’s own features – such as its query notation and its actor notation – are built using these extension mechanisms.</Text>
            <Comments>The term type declaration is reserved for defining a new type. Variable types are defined through type annotations.</Comments>
        </Document>
        <Document ID="CDE60377-7E81-41EE-87B9-6FBF3DD701CB">
            <Title>Universal Resource Identifier</Title>
            <Text>Each package is identified by a URI. The intention of a URI is that a given URI identifies exactly one package.
The URI has an IETF standard specification (RFC 2396) [#rfc2396].
One important source of confusion with the IETF URI is the distinction between URIs and URLs. Although they share a common syntax a URI is not intended to convey the location of the resource.
For example, the URI
file://foo/bar/x/y.star
looks like a file-based name: one might be tempted to believe that the file x.Star resides in the directory /foo/bar/x. While it is possible that it might, there is actually no such commitment. Furthermore, the same resource may be someplace else (and will be if the identified file is in a code repository).</Text>
        </Document>
        <Document ID="0778B444-3F1F-4924-83BA-B2AB7028AC3E">
            <Title>Back Matter</Title>
        </Document>
        <Document ID="F4D83EDC-0C1C-4E35-83CF-44DD6E87753F">
            <Title>The Sieve of Erastosthenese</Title>
            <Text>One of the classic algorithms for finding primes can be expressed using filters — the so-called sieve of Erastosthenes. This algorithm works by repeatedly removing multiples of primes from the list of natural numbers. We cannot (yet) show how to deal with infinite lists of numbers but we can capture the essence of this algorithm using a cascading sequence of filter operations.
The core of the sieve algorithm involves taking a list of numbers and removing multiples of a given number from the list. This is very similar to our even-number finding task, and we can easily define a function that achieves this:
filterMultiples:(integer,list[integer])=&gt;list[integer].
filterMultiples(K,N) =&gt; filter((X)=&gt;X%K=!=0,N).
The overall Erastosthenes algorithm works by taking the first element of a candidate list of numbers as the first prime, removing multiples of that number from the rest, and recursing on the result:
seive:(list[integer])=&gt;list[integer].
sieve([N,..rest]) =&gt; [N,..sieve(filterMultiples(N,rest))].
There is a base case of course, when the list of numbers is exhausted then we have no more primes:
sieve([]) =&gt; [].
The complete prime finding program is hardly larger than the original filter specification:
primes:(integer)=&gt;list[integer].
primes(Max) =&gt; let{
  sieve:(list[integer])=&gt;list[integer].
  sieve([]) =&gt; [].
  sieve([N,..rest]) =&gt; [N,..sieve(filterMultiples(N,rest))].

  filterMultiples:(integer,list[integer])=&gt;list[integer].
  filterMultiples(K,N) =&gt; filter((X)=&gt;X%K=!=0,N).

  iota:(integer,integer)=&gt;list[integer].
  iota(Mx,St) where Mx&gt;Max =&gt; [].
  iota(Cx,St) =&gt; [Cx,..iota(Cx+St,St)].
} in [2,..sieve(iota(3,2))]
The iota function is used to construct a list of numbers, in this case the integer range from 3 through to Max with an increment of 2. We start the sieve with 2 and the list of integers with 3 since we are making use of our prior knowledge that 2 is prime.
It should be emphasized that the sieve of Erastosthenes hardly counts as an efficient algorithm for finding primes. For one thing, it requires that we start with a list of integers; most of which will be discarded. In fact, each ‘sweep’ of the list of numbers results in a new list of numbers; many of which too will eventually be discarded. Furthermore, the filterMultiples function examines every integer in the list; it does not make effective use of the fact that successive multiples occupy predictable slots in the list of integers. In fact, building a highly optimized version of the sieve of Erastosthnese is not actually the main point here – it is to illustrate the power of Star’s collections processing functions.
We might ask whether the sieve function can also be expressed as a filter. The straightforward answer is that it cannot: the sieve is a kind of filter, but the predicate being applied depends on the entire collection; not on each element. The standard filter function does not expose the entire collection to the predicate. However, we will see at least one way of achieving the sieve without any explicit recursion below when we look at folding operations.</Text>
        </Document>
        <Document ID="157CC4CD-C6BD-464A-9317-4F129C3C2458">
            <Title>String Interpolation</Title>
            <Text>The expression
"fact(10) is \(fact(10))"
is an interpolated string expression. It means the string "fact(10) is \(fact(10))" with the substring \(fact(10)) replaced by the value of the expression embedded within. Interpolated string expressions are a convenient way of constructing string values; and, via the use of contracts, are also type safe.</Text>
        </Document>
        <Document ID="C8CF3047-F965-4A63-903D-9A26625F4FA3">
            <Title>Computing Primes With Tasks</Title>
            <Text>Before, we looked at implementing the sieve of Eratosthenes using operations over collections. The Sieve, as it is affectionately known, is also a great demonstrator of concurrent programming.
Recall that The Sieve has two parts: a collection of activities that filter a sequence of numbers looking for multiples of previously found primes and a master that grows the set of filters as new primes are found.</Text>
        </Document>
        <Document ID="BC0DC62B-D666-419C-A37C-870C3C678050">
            <Title>Manuscript Format</Title>
        </Document>
        <Document ID="CE3E8CAA-6CAE-4253-AEA7-9F7CF73B5448">
            <Title>Asynchronous Communication</Title>
            <Text>We stated at the beginning that ‘synchronous communication was more basic’ than asynchronous communication. Nevertheless, there are times when asynchronous communication is called for.
In general, asynchrony increases the potential for parallelism; it also increases the complexity of coordination.
One pattern that makes inherent use of asynchronous communication is the worker-queue pattern. The worker-queue pattern consists of a source of ‘work’, a queue to hold unfinished work items and one or more ‘workers’ that perform a typically compute intensive task on each item. For example, an image rendering farm might consist of an image generator, a queue and a set of image renderers. (Image rendering, such as resizing an image encoded in JPEG or converting from one form to another, often takes considerable compute resources.)
Our focus here will be on the implementation of the queue, rather than the implementation of any image rendering process.
Other patterns that make use of asynchronous communications include the publish-subscribe and the actor patterns. In applications where there is essentially a one-way flow of information from publishers to subscribers there is less need to tightly synchronize communications. Similarly, actors denote semi-autonomous activities that collaborate with each other to solve shared goals. In fact, almost every multi-user application can be modeled in terms of agents.</Text>
        </Document>
        <Document ID="09207F79-BF79-406C-B420-C68496956E8E">
            <Title>Appendix</Title>
        </Document>
        <Document ID="EEB7CF66-23BF-4AE3-9BB6-BE9E2DEC331B">
            <Title>Meta Language</Title>
            <Text>Star has a meta language and a standard type to go with it. This means that expressions in the language may also be values. For example, consider the expression:
X+2
If the value of X is 3, then the value of this expression is 5. However, we can also examine the language that this expression is made of — by using the quote notation. The expression:
&lt;| X+2 |&gt;
means the name of the expression; for compiler-buffs this is effectively the abstract syntax tree of the expression. Star has a standard type — quoted — whose abbreviated type definition is:
type quoted is nameAst(string)          -- identifier
            or integerAst(integer)      -- integer literal
            or applyAst(quoted,quoted)  -- application
            or tupleAst(list[quoted]) -- tuple term
            ...
and the quoted expression above is equivalent to:
applyAst(nameAst("+"),tupleAst(list of [nameAst("X"),integerAst(2)]))
Clearly, the quoted term is a lot less noisy than the ‘real’ version. The power of the meta language comes from the fact that any fragment of Star may be represented in the meta language — even fragments that are not part of the base language. It is also the basis of Star’s macro processing capabilities.
Any macro (whether its a macro rule or a code macro) is a function whose type signature is:
(quoted)=&gt;quoted
The special feature of macro programs is that they are invoked by the compiler on the source program itself — hence the ‘meta’ in ‘meta-language’.</Text>
        </Document>
        <Document ID="BC14390C-6171-46CD-912F-5EDB2CDED544">
            <Title>Existential Types</Title>
            <Text>What does it mean to have a type in a record? From a programmer’s point of view it is actually quite a natural extension of the concept of a record; there does not seem to be any intrinsic reason why a record shouldn’t have types in it. However, the logic of this bears a deeper look.
The declaration of the foo type involves the use of an existential quantifier:
simple:exists f/1 ~~ {
  ...
}
The meaning of an existentially quantified type is complementary to the universally quantified type. An existential type quantification is an assertion that the type exists, in this case that f exists and is a type constructor of 1 argument.
The statement:
  all t ~~ f[t] &lt;~ foo[t].
is actually a normal type alias statement. It states that the type f[t] is the same as the type foo[t] – for all t. A type alias statement is a way of giving a new name to a type; here we are using it to provide evidence that the record contains the required evidence for f.</Text>
        </Document>
        <Document ID="87331D24-0387-4B16-A93F-BA6C1C57FBDA">
            <Title>Types, More Types and Even More Types</Title>
            <Text>In many ways, the defining characteristic of a programming language is the approach to types. As we shall see, Star’s type system is quite extensive and powerful; however, simple types are quite straightforward.
The most basic question to ask about types is
What is a type?
There is some surprising variability to the answer to this question; for example, in many OO languages, types are conflated with classes. Star types are terms – i.e., names – that denote different kinds of values.
Type
A type is a term that denotes zero or more values. I.e., a type is the name of a collection of values.
Star’s type system can be broken down into a number of dimensions:
	•	How legal values of various kinds can be identified with a type;
	•	the treatment of type variables and quantifiers; and
	•	constraints on types, particularly type variables
Star distinguishes two basic styles of type: so-called structural or transparent types and nominative or opaque types. A structural type term echoes the values it models, whereas a nominative type typically does not.
For example, the standard type integer is nominative — its name gives no hint as to the representation, structure or kinds of values that are modeled by integer. However, a nominative type often indicates some actual entity being modeled – in this case integer values. Two nominative types which have different names always denote distinct values, whereas two structural types that look the same are actually identical.</Text>
            <Comments>Not a set of values because not all collections of values are mathematical sets.
I.e., everything you thought you knew about integers may or may not apply to the values denoted by integer.</Comments>
        </Document>
        <Document ID="4553DA12-6965-4712-90EB-977C6D47475C">
            <Title>Collections</Title>
            <Text>Modern programming — whether it is OO programming, functional programming or just plain C programming — relies on a rich standard library. Given that nearly every program needs to be able to manage collections of things, the central pearl of any standard library is the collections library. Recalling our mantra of hiding recursion; a well designed collections library can make a huge difference to the programmer’s productivity, often by hiding many of the recursions and iterations required to process collections.
The collections architecture in Star has four main components:
	1.	a range of standard collection types — including array-like lists, cons lists, red-black trees, first-in first-out queues, and dictionaries;
	2.	a range of standard functions — mostly defined in contracts — that define the core capabilities of functions over collection;
	3.	special notations that make programming with collections in a type independent way more straightforward; and
	4.	the final major component of the collections architecture is queries. Star has a simple yet powerful set of features aimed at simplifying querying collections.</Text>
        </Document>
        <Document ID="B416D9BA-4D3F-4899-8E12-7388F398863A">
            <Title>Typographical Conventions</Title>
            <Text>Any text on a programming language often has a significant number of examples of programs and program fragments. We show these using a typewriter-like font, often broken out in a display form:
P:integer;
...
We use the ... ellipsis to explicitly indicate a fragment of a program that may not be syntactically correct as it stands.
As we noted above, Star is a rich language with many features. As a result, some parts of the text may require more careful reading, or represent comments about potential implications of the main text. These notes are highlighted the way this note is.</Text>
        </Document>
        <Document ID="385069CF-AACF-4ECA-9928-92DF35BC801A">
            <Title>Index Slices</Title>
            <Text>Related to accessing and manipulating individual elements of collections are the indexed slice operators. An indexed slice of a collection refers to a bounded subset of the collection. The expression:
C[fx:tx]
denotes the subsequence of C starting with — and including — the element indexed at fx and ending — but not including the element indexed at tx.
As might be expected, the index slice notation is also governed by a contract — the sliceable contract. This contract defines the core functions for slicing collections and for updating subsequences of collections:
contract all s,k ~~ sliceable[s-&gt;&gt;k] ::= {
  _slice:(s,k,k)=&gt;s.
  _tail:(s,k)=&gt;s.
  _splice:(s,k,k,s)=&gt;s.
}
The _slice function is used extract a slice from the collection, _tail is a variant that returns the ‘rest’ of the collection, and _splice is used to replace a subset of the collection with another collection.
Like the indexing notation, there is notation for each of the three cases:
C[fx:]
denotes the tail of the collection — all the elements in C that come after fx (including fx itself); and
C[fx:tx-&gt;D]
denotes the result of splicing D into C. This last form has an additional incarnation — in the form of an assignment statement:
C[fx:tx] := D
This action is equivalent to the assignment:
C := _splice(C,fx,tx,D)
which, of course, assumes that C is correctly defined as a read/write variable.
Star encourages declarative programming but we fully recognize that side-effecting behavioral code is often the most effective solution to the problem.
The slice notation is an interesting edge case in domain specific languages. It is arguably a little obscure, and, furthermore, the use case it represents is not all that common. On the other hand, without specific support, the functionality of slicing is hard to duplicate with the standard indexing functions.</Text>
        </Document>
        <Document ID="74D57ED7-FB6B-40BF-BD6F-41EF3422DD09">
            <Title>Sequence Patterns</Title>
            <Text>The complete sequence contract has six signatures in it — the latter three signatures play an analogous role to the first three but for sequence patterns rather than sequence expressions. They also introduce a new form of type expression — the pattern type. For example, the signature for _pair — which is used to decompose sequences into a head and tail — is:
_pair:(e,t)&lt;=t.
Notice the direction of the arrow: we have not seen this form of type so far, and relates to a capability that we have not encountered yet in this book — pattern abstractions.
Pattern Abstraction
A pattern abstraction is an expression that denotes a pattern.
Pattern abstractions are exactly analogous to functions — another name for which is expression abstraction. Pattern abstractions allow patterns to be encapsulated and reused in the same way that functions allow expressions to be encapsulated and reused.
In this case, the pattern abstraction is critical because general sequence notation is independent of the types of the collections involved — and so we have no way of knowing what concrete patterns to apply.
Pattern abstractions are applied using the same application notation as for function application; for example, the _pair pattern in
first:all c,e ~~ sequence[c-&gt;&gt;e] |: (c)=&gt;e.
first(_pair(H,T)) =&gt; H.
is a pattern abstraction that is applied to the argument of first. What may be a little surprising initially is that the arguments to a pattern application are also patterns! So, here, the variables H and T in the call to _pair will be bound to the first element of the collection and the remainder respectively.
For example, applying _pair(H,T) to [1,2] binds H to the value 1 and binds T to the sequence [2]. The value returned by first will be 1. We can combine pattern applications in an exactly analogous manner to the way we combine function calls.
Obviously, there must also be a way of defining pattern abstractions. We can define a cPair pattern abstraction that applies to cons lists thus:
cPair:all e ~~ (e,cons[e])&lt;=cons[e].
cPair(H,T) &lt;= cons(H,T).
This takes a little careful reading, but is ultimately straightforward: the right hand side is the pattern that is being abstracted, the left hand side is an application template.
The general form of a pattern abstraction is:
name(E1,..,En) &lt;= Pattern
where the various Ei are expressions that represent the values ‘read off’ the Pattern — should the pattern be satisfied. This is quite analogous to the situation for rewrite equations — except that the roles of patterns and expressions are reversed.
Like functions, pattern abstractions may be defined with multiple pattern rules; and the pattern abstraction is satisfiable exactly when one of its pattern rules is.
Pattern abstractions are not as ubiquitous as functions; however, they certainly play a vital role in the overall design of Star; and are indispensable in the right circumstances.
One special use for pattern abstractions is to give higher-level names to particular patterns. This mimics the use of functions naming expressions, and has a similar importance for program design.
Given that we have seen how sequence expressions are transformed into function calls from the sequence contract, we can now straightforwardly give the equivalent translation for sequence patterns. Syntactically, there is no distinction between sequence expressions and sequence patterns — what distinguishes them is context: sequence patterns show up as patterns in functions and sequence expressions show up in the expression context.
A sequence pattern, as in the pattern [E,..X] for the non-empty case in concat:
concat([E,..X],Y) =&gt; [E,..concat(X,Y)]
is transformed into the pattern:
_pair(E,X)
and the entire rewrite equation becomes:
concat(_pair(E,X),Y) =&gt; _cons(E,concat(X,Y))
We can combine multiple pattern abstraction applications; for example, the function:
single:all c,e ~~ sequence[c-&gt;&gt;e] |: (c)=&gt;e.
single([H]) =&gt; H
which is a function that only matches singleton sequences requires two pattern applications from the sequence contract:
single(_pair(H,_empty())) =&gt; H
The sequence contract is one of the most important and commonly used contracts in the Star library. As we shall see further, many of the standard collections functions are built on top of it.</Text>
        </Document>
        <Document ID="8C6974CC-7B21-4DE5-B49A-13C8896CB47F">
            <Title>The Sequence Contract</Title>
            <Text>Underlying the sequence notation is the sequence contract. This contract contains type signatures in it that can be used to construct and to match against sequence values. The sequence notation is realized by the compiler translating sequence terms to a series of calls to those functions.
The actual sequence contract is
contract sequence[t-&gt;&gt;e] ::= {
  _nil:()=&gt;t.     -- empty sequence
  _cons:(e,t)=&gt;t. -- add to front
  _apnd:(t,e)=&gt;e. -- add to back
  _empty:()&lt;=t.   -- match empty sequence
  _pair:(e,t)&lt;=t. -- match front
  _back:(t,e)&lt;=t. -- match back
}
The first three entries in this contract should be fairly self-evident:
	•	_nil is a function that returns an empty sequence;
	•	_cons is a function that ‘glues’ a new element to the front of the collection; and
	•	_apnd appends elements to the back of the collection.
The compiler uses these three functions to transform sequence expressions into function calls. 
For example, the sequence expression:
[1,2,3]
is transformed into
_cons(1,_cons(2,_cons(3,_nil())))
If a sequence expression has an explicit type marker on it, then its translation is slightly different — to allow the type checker to make use of the type information. For example, 
cons of [1,2]
is translated as:
_cons(1,_cons(2,_nil())):cons[_]
This annotation is all that is needed to force the compiler to treat the result as a concrete cons list. Type inference does the rest of the hard work.</Text>
            <Comments>This could also have been a simple value.
The type expression _ is a special type that denotes an anonymous type: each occurrence of the type expression denotes a different unknown type. It is useful in situations, like this one, where only some of the type information is known.</Comments>
        </Document>
        <Document ID="44BCD44F-C15C-4CBF-959A-1A4C0FC1AFBA">
            <Title>What is Functional Programming?</Title>
            <Text>The foundations of functional programming rest on two principles:
	1.	Programs are expressed in terms of functions, where a somewhat mathematical view of functions is taken — functions always produce the same output for the same input. This is what people mean when they say that functional programs are declarative.
	2.	Functions are values: they can be passed as arguments to functions, returned from functions, and they can be put into and retrieved from data structures.
This last principle is what people mean when they refer to functions as being first class values. It is also what is meant when we say that functional programming languages are higher order.
It is worth asking why these two principles are so important. Although most early programmers were mathematicians; one would not have seen much evidence of mathematical thinking in the way that people program. However, almost by accident, programming and mathematics share some attributes: the importance of composition and the power of abstraction. Functional programming is almost ideally placed to exploit both of these to the full.
Star is not actually a pure functional programming language; i.e., it is possible to write programs that violate the declarative principle. However, it is a functional-first programming language: a declarative style is strongly encouraged. In this book we shall mostly focus on writing pure functional programs in Star.
</Text>
            <Comments>The term declarative has a technical definition. But this captures much of the essence of declarativeness (sic).</Comments>
        </Document>
        <Document ID="B8F8E3CD-1E05-4966-BE2C-6651CD48C0A4">
            <Title>Query Quantifiers</Title>
            <Text>The QuantifierTerm in a query specifies ‘how many’ answers we want. There are essentially three forms of QuantifierTerm — if we want all the answers then we use a term of the form:
{ all (X,Y) where ... }
On the other hand, if we want a fixed number, then we use:
{ 5 of (X,Y) where ... }
Of course, there might not be five answers, and so this is called a bounded QuantifierTerm.
We have only scratched the surface of possibilities of query expressions here. They are, in fact, one of Star’s most powerful high-level features.</Text>
        </Document>
        <Document ID="FE0B0903-65A9-4DEF-85A4-45F9EE63FB4B">
            <Title>The filterable contract</Title>
            <Text>As noted above, the filter function is governed by a contract, the filterable contract:
contract all c,e ~~ filterable[c-&gt;e] ::= {
  filter:((e)=&gt;boolean,c) =&gt; c.
}</Text>
        </Document>
        <Document ID="36CB8454-EC22-4465-B530-891353E22F9D">
            <Title>Going Further</Title>
            <Text>Although better than the original sTest program there is still one major sense in which the test program is not general enough. We can see by looking at another example: a function that counts elements in the tree:
count:all t ~~ (tree[t]) =&gt; integer.
count(tEmpty) =&gt; 0.
count(tNode(L,_,R)) =&gt; count(L)+count(R)+1
This code is very similar, but not identical, to the test function.
The issue is that test is trying to do two things simultaneously: in order to apply its test predicate to a binary tree it has to implement a walk over the tree, and it also encodes the fact that the function we are computing over the tree is a boolean-value function.
We often need to do all kinds of things to our data structures and writing this kind of recursion over and over again is tedious and error prone. What we would like to do is to write a single visitor function and specialize it appropriately when we want to perform a specific function.
This principle of separating out the different aspects of a system is one of the core foundations of good software engineering. It usually goes under the label separation of concerns. One of the beautiful things about functional programming is that it directly supports such good architectural practices.
Since this visitor may be asked to perform any kind of computation on the labels in the tree we will need to slightly generalize the type of function that is passed to the visitor. Specifically, the type of function should look like:
F : (a,t)=&gt;a
where the a input represents accumulated state, t represents an element of the tree and the result is another accumulation.
Using this, we can write a tVisit function that implements tree walking as:
tVisit:all a,t ~~ (tree[t],(a,t)=&gt;a,a)=&gt;a.
tVisit(tEmpty,_,A) =&gt; A.
tVisit(tNode(L,Lb,R),F,A) =&gt; tVisit(R,F,F(tVisit(L,F,A),Lb)).
Just as the accumulating function acquires a new ‘state’ parameter, so the tVisit function also does. The A parameter in the two equations represents this accumulated state.
The second rewrite equation for tVisit is a bit dense so let us open it out and look more closely. A more expanded way of writing the tVisit function would be:
tVisit(tEmpty,_,A) =&gt; A.
tVisit(tNode(L,Lb,R),F,A) =&gt; let{
      A1 : a.
      A1 = tVisit(L,F,A).
      A2 : a.
      A2 = F(A1,Lb).
    } in tVisit(R,F,A2)
where A1 and A2 are two local variables that represent the result of visiting the left sub-tree and applying the accumulator function respectively. We have used the let expression form to make the program more obvious, rather than to introduce new functions locally; but this is a legitimate role for let expressions.
The tVisit function knows almost nothing about the computation being performed, all it knows about is how to walk the tree and it knows to apply functions to labels in the tree.
Given tVisit, we can implement our original check and count functions as one-liners:
check(T,S) =&gt; tVisit(T,(A,X)=&gt;(A || X==S),false).
count(T) =&gt; tVisit(T,(A,X)=&gt;A+1,0).
Notice that we have effectively hidden the recursion in these function definitions — all the recursion is encapsulated within the tVisit function. One of the unofficial mantras of functional programming is hide the recursion.
The reason we want to hide recursions that this allows the designer of functions to focus on what is being computed rather than focusing on the structure of the data and, furthermore, this allows the implementation of the visitor to be shared by all users of the tree type.
Notice that, while a and t are type variables, we did not put an explicit quantifier on the type of F. This is because the quantifier is actually put on the type of tVisit instead:
tVisit:all a,t ~~(tree[t],(a,t)=&gt;a,a)=&gt;a
Just like regular variables, type variables have scope and points of introduction. Also like regular variables, a type variable may be free in a given type expression; although it must ultimately be bound by a quantifier.</Text>
        </Document>
        <Document ID="394DAFAE-3BF8-4395-9B0D-43035EB9D002">
            <Title>Using Existentially Quantified Types</Title>
            <Text>So, what are the rules for existentially quantified types?
The first is one that we have already been looking at:
An existential variable may be bound to a type when providing evidence that a value has a certain type, but may not be constrained when using a value with an existential quantifier in its type.
The second is related to this:
Each occurrence of an existentially quantified type is potentially different.
Think about a function with the type:
exFn:for all t ~~
  (t)=&gt;exists e ~~ R[e,t]
For the moment, we don’t much care about R. Now, consider how we might use exFn:
X1 = exFn("alpha")
...
X2 = exFn(“alpha”)
An important question is “what is the relationship between the type of X1 and the type of X2?”. Unfortunately, the fundamental answer is ‘we cannot know in general’; which in type terms means effectively there is no relationship: they are different. The reason is that the internal type used within the implementation of exFn may result in different instantiations for e for each invocation. The result is we cannot assume any link between the types of X1 and X2: they are different. This has some serious consequences for how we use existentially quantified types.
On the other hand, consider the similar sequence of definitions:
Y1 = exFn("alpha")
...
Y2 = Y1
In this case we do know that the type of Y1 is identical to the type of Y2. This leads us to the third rule:
Each use of an existential quantification introduces a new type — called a Skolem type — that follows the normal inference rules for all types.
I.e., once a type has been introduced as a Skolem type, it behaves just like any regular type and the normal rules of inference apply. This applies equally to the two fragments of code above; but the additional constraint on the immutable values of Y1 and Y2 make it easier to propagate type information.
We can see this a little clearly by looking at the effective type annotations of Y1 and Y2:
Y1:R[e345,string]
Y1 = exFn("alpha")
...
Y2:R[e345,string]
Y2 = Y1
where e345 is the skolemized variant of the existential type e.
The effective annotations for X1 and X2 will have different skolem constants:
X1:R[e235,string]
X1 = exFn("alpha")
...
X2:R[e678,string]
X2 = exFn("alpha")
If Y1 or Y2 were declared to be re-assignable variables then, once again, we would not be able to connect the types of Y1 and Y2 together.</Text>
            <Comments>Technically, the type is denoted by a Skolem Constant or a Skolem Function.</Comments>
        </Document>
        <Document ID="04486F4C-20B6-4F45-9A62-CDC9DDE9EB04">
            <Title>Responding to Change</Title>
            <Text>One of the sources of inefficiencies in software development is the effort needed to adopt a technology: be it a library, a service or a programming technique. This becomes especially obvious when some change is necessary; for example, when changing technologies or when responding to a change in requirements.
There are three common sources of change at the macro-level: a need to reuse a software artifact, a need to repurpose the artifact and/or a need to refactor the artifact. The first corresponds to using a software artifact in a similar role but in a different context. For example, when a Math library developed for real-time analytics is to be used for a Machine Learning application: it is still a Math library but its context is new.
Repurposing occurs when the artifact is being used to solve a different problem. For example, a messaging platform might be repurposed as an Applications server (because the technology involved can be helpful).
Refactoring occurs when new requirements mean that new properties are desired. For example, a Json parser might be refactored to make it faster when the scale of the data being processed increases.
Another form of refactoring occurs when the implementation technology changes – for example if the Json parser needs to be reimplemented in C++ because its original Java implementation ‘does not fit’ any more.
All these kinds of change often expose assumptions that were implicit before. It is these hidden assumptions that can increase the difficulty and friction in responding to change.
Although it is impossible to completely avoid assumptions – otherwise known as architectural committments – the best way of preparing for evolution is to minimize all assumptions and to make dependencies explicit through the judicious use of interfaces. This use of interfaces is the hallmark of our approach to building platforms.</Text>
        </Document>
        <Document ID="02CF2B5D-8ABA-4C69-8BFF-98C6CD2989AC">
            <Title>Planning for Change</Title>
            <Text>Furthermore, change is the dominant fact of life for large systems. Software systems evolve, grow, are repurposed to meet new objectives, and are re-implemented to take advantage of new technologies.
Managing change in code is often left to Source Code Control Systems; but these systems only address part of the problem. For example, code re-use is also a requirement and a challenge for large systems. In fact, one could image the “three R’s” for large systems: reusing, repurposing, and refactoring.
When the code base is over 500 Kloc, the theoretical probability of being able to re-use existing code is high, the actual probability of re-use may be very low – software engineers may simply not be able to find the needle in the code haystack, or, typically, the cost of re-using code may outweigh the cost of developing from scratch.
The need to repurpose reflects the fact that requirements change and that software written for one purpose may be used for something different. Finally, even if the requirements dont change, the context almost certainly will: increasing workload can lead to a need to refactor existing code to enable it to meet changing needs.</Text>
        </Document>
        <Document ID="EAAFEE6B-5316-410E-8511-9C8C97F642DB">
            <Title>Packages are Records</Title>
            <Text>If we take a slightly deeper look at Star’s package we will see some surprising features; features that are sadly not very common in programming languages.
Let’s start with a super simple source package:
ss{
  public double:(integer)=&gt;integer.
  double(X) is X*2.
}
This package structure is semantically equivalent to a function:
ss:{double:(integer)=&gt;integer}.
ss = let{
  public double:(integer)=&gt;integer.
  double(X) is X*2
} in {. double=double .}
In effect, a package reduces to a variable declaration whose type is a record type and whose value is an anonymous record of all the functions defined in the package.
The special form:
{. double=double .}
is a record structure; the periods in the braces signify that the record is not recursive: the definitions within the record cannot reference each other.
What about types though? Like other programming languages Star allows us to export types from packages too:
simple{
  public all t ~~ foo[t] ::= foo(t) | bar.

  public fooMe:all t ~~ (t)=&gt;foo[t].
  fooMe(X) =&gt; foo(X)
}
The ability to export types — and their constructors — is an extremely important part of the package functionality. However, it turns out that we can account for types too in our semantics of packages. Similarly to our previous unfolding, the simple package is equivalent to:
simple: exists f/1 ~~ {
  type foo/1.
  foo:all t ~~ (t) &lt;=&gt; f[t].
  bar:all t ~~ f[t].
  fooMe:all t ~~ (t)=&gt;f[t].
}
simple = {
  all t ~~ foo[t] ::= foo(t) | bar.

  fooMe:all t ~~ (t)=&gt;foo[t].
  fooMe(X) =&gt; foo(X).

  all t ~~ f[t] &lt;~ foo[t].
}
This is clearly quite a bit more complex than our super-simple example; but the basic story is still there: a package consists of a variable definition whose value is an anonymous record. In this case, the anonymous record contains a type, two constructors as well as a regular function. It also includes a type alias statement that, in this case, informs the type system where the existentially quantified type f is realized as the foo type.
Having constructors in a type is only a small extension to the conventional notion of a record — while many languages restrict records to containing just data values, most functional programming languages do allow functions in records. A constructor is just a special kind of function.
On the other hand, having a type in a record is quite different.</Text>
            <Comments>This is somewhat analogous to the difference between a let and a letrec form in languages like SML.</Comments>
        </Document>
        <Document ID="CB0EEE35-84F6-4D12-9A81-6827BED783B8">
            <Title>Paperback</Title>
        </Document>
        <Document ID="B7BC5B87-8E32-4C3E-89C9-2156EA2FF42A">
            <Title>Are We There Yet?</Title>
            <Text>The straightforward answer to this is no. There is a great deal more to functional programming than can be captured in a few pages. However, we have covered some of the key features of functional programming — particularly as it applies to Star. In subsequent chapters we will take a closer look at collections, at modular programming, at concurrency and even take a pot shot at Monads.
If there is a single idea to take away from this chapter it should be that functional programming is natural. If there is a single piece of advice for the budding functional Star programmer, it should be to hide the recursion. If there is a single bit of comfort to offer programmers it should be that Rome was not built in a day.</Text>
        </Document>
        <Document ID="EFA77E9F-5A99-428C-B6C7-4D091FDD98FF">
            <Title>A First Star Program</Title>
            <Text>It is traditional to introduce a new programming language with something like the hello world example. Which we will do in a moment. However, the factorial function often makes a better first example for functional programming languages:
sample.factorial{
  import star.               -- Access standard language features

  public fact : (integer)=&gt;integer.
  fact(0) =&gt; 1.              -- base case
  fact(N) where N&gt;0 =&gt; N*fact(N-1).
}
This is not an executable program per se.; however, it does represent a more typical source code unit — most programs are built from many smaller modules which come together to make the application. This program is small, but already introduces many of the key elements of the Star language.
In this module, we see the name of the module – sample.factorial – and an import statement and a function definition – of the fact function.
Source code can be in any form of textual container. There is, for example, no specific requirement that this sample.factorial package be in a file called factorial.star; although that may be good advice. Instead, the Star system relies on a catalog based system that maps package names to text entities. The catalog system also serves as an anchor point in the version management of Star programs. We will cover this, and the related repository system for generated artifacts, in Chapter 4.</Text>
        </Document>
        <Document ID="E984488B-35BC-4EAB-87B4-24D553312EB9">
            <Title>Summary</Title>
        </Document>
        <Document ID="C419AC6E-0AA3-4B30-96FF-30359CE254E9">
            <Title>Programming Safely and Effectively</Title>
            <Text> At the same time, safety and security are also critical: no-one likes to have their private information exposed to the bad guys. Most main-stream programming languages were designed in an era when safety was not uppermost in programmers’ minds – usually it was performance. Some seemingly trivial design choices – such as C’s conventions for laying out strings in memory – turn out to have potentially devastating security implications.
In addition, systems that are built assuming a shielded execution environment, behind closed doors as it were, are often actually expected to perform in the full glare of the Internet. Hardening programs so that they stand up to that glare can often dramatically add to the cost of development – both in time and in money.</Text>
        </Document>
        <Document ID="4AE677B5-E62F-452B-A208-2F2D02A8CE41">
            <Title>Algebraic Data Types</Title>
            <Text>An algebraic data type definition achieves several things simultaneously: it introduces a new type into scope, it gives an enumeration of the legal values of the new type and it defines both constructors for the values and it defines patterns for decomposing values. This is a lot for a single statement to do!
For example, we can define a type that denotes a point in a two-dimensional space:
type point ::= cart(float,float).
This kind of statement is called a type definition statement and is legal in the same places that a function definition is legal.
The new type that is named by this statement is point; so, a variable may have point type, we can pass point values in functions and so on.
The constructor cart allows us to have expression that allow ‘new’ point structures to be made:
cart(3.4,2.1)
cart is also the name of a pattern operator that we can use to take apart point values. For example, the euclid function computes the Euclidian distance associated with a point:
euclid:(point)=&gt;float.
euclid(cart(X,Y)) =&gt; sqrt(X*X+Y*Y).
Of course, this particular point type is based on the assumption that point values are represented in a cartesian coordinate system. One of the more powerful aspects of algebraic data types is that it is easy to introduce multiple alternate forms of data. For example, we might want to support two forms of point: in cartesian coordinates and in polar coordinates. We can do this by introducing another case in the type definition statement:
point ::= cart(float,float)
        | polar(float,float).
Of course, our euclid function also needs updating with the new case:
euclid:(point)=&gt;float.
euclid(cart(X,Y)) =&gt; sqrt(X*X+Y*Y).
euclid(polar(R,T)) =&gt; R.
cart and polar are called constructor functions. The term constructor refers to the common programming concept of constructing data structures. They are called functions because, logically, they are functions.
For example, we might give a type to polar:
polar : (float,float)=&gt;point
In fact, constructor functions are one-to-one functions. Variously known as free functions (in logic), bijections (in Math), one-to-one functions are guaranteed to have an inverse. This is the logical property that makes constructor functions useful for representing data.
Of course, we are talking of a logical property of constructor functions. Internally, when implementing functional languages like Star, the values returned by constructor functions are represented using data laid out in memory — just like any other programming language.
Star actually employs a special type for constructor functions; so the correct type of polar is given by:
polar : (float,float)&lt;=&gt;point
The double arrow representing the fact that constructor functions are bijections.
In addition to constructor functions, an algebraic type definition can introduce two other forms of data: enumerated symbols and record functions. Enumerated symbols are quite useful in representing symbolic alternatives. The classic example of an enumerated type is daysOfWeek:
daysOfWeek ::= monday
           | tuesday
           | wednesday
           | thursday
           | friday
           | saturday
           | sunday.
Another example is the standard boolean type which is defined:
boolean ::= true | false.
Unlike enumerated symbols in some languages, there is no numeric value associated with an enumeration symbol: an enumerated symbol ‘stands for’ itself only. The reason for this will become clear in our next type definition which mixes enumerated symbols with constructor functions:
sTree ::= sEmpty | sNode(sTree,string,sTree)
In addition to mixing the enumerated symbol (sEmpty) with the sNode constructor, this type is recursive: in fact, this is a classic binary tree type where the labels of the non-empty nodes are strings. (We shall see shortly how to generalize this).
Whenever you have a recursive type, its definition must always include one or more cases that are not recursive and which can form the base case(s). In that sense, an enumerated symbol like sEmpty plays a similar role in Star has some of the same character as null does in other languages; except that sEmpty is only associated with the sTree type.
We can use sTree to construct binary trees of string value; for example:
sNode(sNode(sEmpty,"alpha",sEmpty),
      "beta",
      sNode(sEmpty,"gamma",sEmpty))
denotes the tree in:
#
A Binary string Tree
One of the hallmarks of languages like Star is that every value has a legal syntax: it is possible to construct an expression that denotes a literal value of any type.
Just as we can define sTree values, so we can also define functions over sTrees. For example, the check function returns true if a given tree contains a particular search term:
check:(sTree,string) =&gt; boolean.
check(sEmpty,_) =&gt; false.
check(sNode(L,Lb,R),S) =&gt; Lb=S || check(L,S) || check(R,S)
Here we see several new aspects of Star syntax:
	•	An empty pattern — marked by _ — matches anything. It is called the anonymous pattern and is used whenever we don’t care about the actual content of the data.
	•	The || disjunction is a short-circuit disjunction; much like || in languages like Java. Similarly, conjunction (&amp;&amp;) is also short-circuiting.
	•	Functions can be recursive. Star permits mutual recursion just as easily: there is no special requirement to order function definitions in a program. 
We can use sTest to check for the occurrence of particular strings:
T : sTree.
T = sNode(sNode(sEmpty,"alpha",sEmpty),
               "beta",
               sNode(sEmpty,"gamma",sEmpty)).
show check(T,"alpha").          -- results in true
show check(T,"delta").          -- results in false</Text>
        </Document>
        <Document ID="5D934E35-A167-4F02-BDFE-1D970DCE26AE">
            <Title>At the Watering Hole</Title>
            <Text>We noted in the introduction that developing software is a team sport. This is true at multiple levels – from building libraries to constructing platforms that support thousands of applications and users. In this chapter we turn our attention to the issues involved in the latter and see how Star’s features can support this goal.
One of the most robust form of relationships is that between vendor and customer. The reason for this is that it is easy to be clear about the different responsibilities of the parties: the vendor supplies the solution and the customer uses it; and pays for it. This relationship is key to understanding the reason why platforms are compelling and also why it can be hard to implement.
It is easy to see that an applications platform can be viewed as a marketplace where clients can access solutions and providers can offer technologies:
Applications Platform
An applications platform is a system which allows clients and providers to share technologies and to gain efficiencies of scale.
What distinguishes a good platform from a poor one is a combination of the ease with which clients and providers can interact – and choose alternatives – and the richness of the offerings: both in terms of the number of potential clients (the size of the market) and the variety of the technologies on offer (the size of the market).
We cannot mandate a large platform in this book; but we can explore the technical aspects of what makes a platform great.</Text>
            <Comments>Interact here means more than simple message passing. When clients use an offering in their application, they are interacting with the providers of the technology.</Comments>
        </Document>
        <Document ID="6A2981D0-EE77-461E-BF97-D9ED0D27A5D2">
            <Title>Endnotes</Title>
            <Text>&lt;$--ENDNOTES--&gt;</Text>
            <Notes>The &lt;$--ENDNOTES--&gt; tag will be replaced by the footnotes during Compile. Using this tag allows us to have the footnotes inserted wherever we like, without having a separator placed above them.</Notes>
        </Document>
        <Document ID="662A19FB-74BC-4FDF-A9B1-F2597E27886C">
            <Title>Programs are huge</Title>
            <Text> If your program is 100 lines long, it does not actually matter what programming language you use to write it in – unless you are learning programming for the first time.
However, in practice, software systems are huge. It is normal to develop and to encounter programs measured in 100 Klocs (thousands of lines of code) and it is not uncommon to encounter software systems measured in the millions of lines.
Software development at this scale does not share much with developing small systems. Large scale software is invariably a team effort, spread over multiple years. It often involves large numbers of components and multiple sub-systems interacting with each other. Overall, this is a scenario of staggering heterogeneity and complexity.
If there is one theme that is common in large code bases is that tooling is critical. It is our opinion that language tooling starts with a sound programming language design: the sounder the foundations, the taller the structure that can be built on it.</Text>
        </Document>
        <Document ID="43AD902F-657D-45E7-8327-72BFFFD73432">
            <Title>Catalogs</Title>
            <Text>A catalog is a mapping from identifiers to package URIs; i.e., it is the missing link between package identifiers and package URIs. Catalogs are sometimes written explicitly — in the form of a file that you can place in the same directory as your source files — but usually the compiler makes an automatic catalog depending on where the source code of a package is actually located.
Like other features, the catalog system is under-pinned by a contract – in this case, the contract is used by the compiler to access the source URIs. 
The catalog contract looks like:
contract catalog ::= {
  defltVersion: version.
  catalogEntry:(string)=&gt;uri.
}
Having the catalog be driven by a contract allows for multiple potential implementations. One standard implementation is via JSON files.
A sample JSON catalog file looks like:
{
  "content": {
    "sample.factorial": "factorial.star"
  },
  "fallback" : "../Star/catalog",
  "version": "2.3.1"
}
Other implementations of catalogs are possible; one of the other standard catalog structures automatically populates a catalog based on the contents of a directory.
For most projects you will not need to be very aware of code repositories, catalogs and so on. However, the code product architecture is an important part of Star’s strategy for helping you building all scales of system. </Text>
        </Document>
        <Document ID="9326CEE4-3970-4BF9-A7DE-ACF13592B1C3">
            <Title>Sieving for Primes</Title>
            <Text>The final piece of The Sieve is the sieve function which constructs a network of filters to sieve out the primes.
At any one time there is an existing network of filters removing multiples of prime numbers found so far. For example, after processing 13, the next number to look at will be 15 and the network of filters will look like:
#
The Sieve of Eratosthenes at 13
The 3 filter removes 15 because 15 is a multiple of 3. However, the following number — 17 — survives all the filters and the sieve responds by spinning off a new filter for 17 at the end of the chain:
￼
The Sieve of Eratosthenes at 17
I.e., The Sieve also receives input from a source, but its response is to spin off a new filter. We can program this using the sieve function:
fun sieve0(inChannel) is valof{
  def nxPrime is valof (wait for recvRv(inChannel))
  valis sieve0(filter(nxPrime,inChannel))
}
The crux of the sieve function is that the same channel that it is listening to for the next prime number is then passed to the newly created filter task and the result of that filter will be the input for the next cycle of the sieve.
This variant of The Sieve does not report the primes it finds; in fact, it will not terminate until the stack overflows. A more appropriate variant will find a list of primes up until some number. We first of all engineer another task that listens to a channel for a sequence of terms and collects the result into a list:
fun collectTerms(ch,Count) is background task{
  var data := list of []
  for cx in range(0,Count,1) do {
    def nxt is valof ( wait for recvRv(ch) )
    data := list of [data..,nxt]
  }
  valis data
}
Given this function, we augment our sieve with the additional channel for sending the results to:
sieve(inChannel,results) =&gt; valof{
  nxPrime = valof (wait for recvRv(inChannel))
  perform wait for sendRv(results,nxPrime)
  valis sieve(filter(nxPrime,inChannel),results)
}
The complete system can be put together into a primes function that uses multiple tasks to sieve for primes:
primes(Max) =&gt; let{
  resltCh = channel().
  { ignore background task { valis sieve(naturals, resltCh)}}
} in valof collectTerms(resltCh,Max)</Text>
        </Document>
        <Document ID="4DC87F46-C118-4DE7-B296-B90ABDD9E8FB">
            <Title>Queries</Title>
            <Text>Consider, if you will, the problem of finding a set of grandparent-grandchild pairs — given information about parent-child relationships. For example, suppose that we had a list of pairs - each pair indicating a parent and child:
parent:list[(string,string)].
parent = [("john","peter), ("peter","jane"), ... ].
and that we wanted to construct a result list – also pairs – along the lines of:
GC:list[(string,string)].
GC = [("john","jane"),...].
Computing the list grandparent/grandchildren pairs involves searching the parents for pairs  that satisfy the grandparent relationship. This involves a double iteration: each pair in the parents list might represent the upper or lower half of a grandparent/grandchild relationship. Based on the collection operators we have seen so far, we can build such a search using two leftFold operations:
leftFold(
  (SoFar,(X,Z)) =&gt; leftFold(
    let {
      acc:(list[(string,string)],(string,string))=&gt;list[(string,string)].
      acc(gp1,(ZZ,Y)) where Z==ZZ =&gt; [gp1..,(X,Y)].
      acc(gp1,_) =&gt; gp1.
    } in acc,
    SoFar,parent),
  list of [],
  parent)
This, rather intimidating, expression uses one leftFold to look for the grandparent, for each candidate grandparent a second leftFold finds all the grand-children. All without any explicit recursion.
The acc function defined above in the let expression implements the logic of deciding what to accumulate depending on whether we had found a grandparent or not.
The various filter, fmap and leftFold functions are powerful ways of processing entire collections. However, as we can see, they can be difficult to construct and harder to follow; something that is not helped by the occasional need to construct complex functions in the middle, as in this case.
It turns out that Star has a special notation that makes this kind of complex computation significantly easier to write and comprehend. Star’s query notation is a very high level way of expressing combinatorial combinations of collections. We can write the equivalent of the previous grandparent expression in this query notation as:
GC = list of { all (X,Y) where
                      (X,Z) in parent &amp;&amp; (Z,Y) in parent }
It may not be obvious, but these expressions compute the same values. What should be obvious is that the query is much easier to read and easier to verify that it is correct.
The syntax and style of Star’s query notation is similar to SQL’s syntax — deliberately so.
Specifically, we take SQL’s relational calculus subset — the language of wheres and of boolean combinations. Star’s query expressions do not have the equivalent of explicit relational join operators.
There are several variations of query expression, but the most common form is:
SequenceType of { QuantifierTerm where Condition Modifier }
where SequenceType is any type name that implements the sequence contract, QuantifierTerm is a form that indicates the form of the result of the query, Condition is a condition and the optional Modifier is used to signal properties of the result — such as whether the result is grouped or sorted.</Text>
            <Comments>There are, unfortunately, some functional programmers that revel in complex code expressions like this one. We are not one of them!</Comments>
        </Document>
        <Document ID="497CB634-2C60-4813-BB33-E7C2F764443C">
            <Title>Ports and Speech</Title>
            <Text>First, let us take a slightly deeper look at the anatomy of a port. Ports are intended to represent the points of connectivity of a component: in effect, they form the gateways into and out of the component. By restricting ourselves to components that only interact via their ports we foster re-usability of components and re-purposability (sic) of code.
This is an important point: unlike most programming languages, and unlike Star itself, our diagramming notation does not rely on scoping to communicate between elements. Instead, all connections and references are explicit. This is a strong constraint that helps to enforce so-called loose coupling between components. This greatly simplifies the kinds of task of assembling applications from components.
Ports denote types as well as data flow: each port is associated with an API schema that determines the type of data that is going through the port. For example, the responding port of the SplitComponent has a schema associated with it that shows that the component expects Orders coming in:
#
An Order Port
The originating port will be connected to a data source component. It’s type schema declares what kind of data it is looking for:
￼
The Parts DB Port
It should come as no surprise at this point that we also declare that ports are intimately associated with speech actions: communication between components is mediated by speech actions and ports codify both the sender/receiver relationship and the type of communication.
Notice that we are speaking in terms of originating and responding here. This reflects the fact that in any given communication we have an originating speaker and a responding listener. We have deliberately avoided using terms such as input or output here because connections between components can involve both flows of information but only one side ever initiates the action. For example, the SplitComponent will raise a query to the PartsDB component: the query itself involves sending the query in one direction and receiving the data from the PartsDB component in the other.
Our final kind of port is a specialization of the originating port: the publishing port as in:
￼
The Supplier Port
The difference between a normal originating port and a publishing port is two-fold: it represents a one-to-many fan-out – i.e., it may be connected to an arbitrary number of other components – and it also has an additional feature: the discrimination function that will allow the SplitComponent internally to ‘select’ the right supplier to send parts orders to.
The net effect of this is that we have a diagramming notation that supports a high-level modeling of applications that is intuitive to many software engineers. Furthermore, we can support type safety of communication between elements of the application.
This notation is not quite a complete ‘spanning set’ if the end goal is to construct a platform for building a variety of distributed applications. However, it is the kernel of such a set and can easily form the basis of a complete distributed applications platform.</Text>
        </Document>
        <Document ID="5372808E-9B3B-40C7-8058-28764840DF39">
            <Title>Comment</Title>
            <Text>The concurrency features of task expressions and rendezvous avoid the synchronization problems that plague concurrent and parallel programming in traditional languages. Moreover, tasks and rendezvous scale easily to hundreds of thousands of threads.
However, it should also be clear that they are also a little low-level: in order to program something like a semaphore or a message queue requires fairly careful attention to detail.
The true merit of Star’s concurrency features is that they do allow these higher level constructions to be programmed in a way that does not require extreme effort.
Indeed, as we shall see in [agent programming][chattering-agents] it is possible to implement a system of parallel agents with extremely minor tweaks to the regular sequential versions of the program.</Text>
        </Document>
        <Document ID="660D34BB-A9CC-4081-AEC9-FD0F018B8C59">
            <Title>The Value of a Task</Title>
            <Text>As we saw above, a task computation produces a value by performing the valis action:
valis X+Y
The value of a task value is obtained using the valof operator:
valof T
denotes the value returned by the task computation T.
It is important to remember, in thinking about tasks, that there is the extra level of indirection: the value of a task expression is a computation. When the computation is performed, it too has a value.
Note that the valof operator also has the effect of forcing the completion of the task T; in particular, the valof operator is blocked until the task has completed and its value can be returned.
Although the valof operator denotes the end of the computation, it does not denote the start of the computation.
If a task does not return a value then its type will be
task[()]
Obviously a task that does not have a value does not require it’s body to contain a valis action; however, if it does, then that action should be:
valis ()
In effect, tasks that do not purport to return values do return a value — the void tuple value. To avoid the clumsiness of handling empty tuples, especially in the context of actions, it is better to perform such a task instead. Like the valof expression form, the perform action forces completion of its task argument — thus achieving some synchronization with the performed task.
For example, the action:
perform task {
  nothing
}
performs the task (which does nothing) and waits for nothing to finish (sic).
Void tasks are occasionally useful — for example, multi-tasked applications often contain a server component: a component that responds to requests made by other tasks. Such a component typically is not expected to return and so does not have a useful return value.</Text>
        </Document>
        <Document ID="511B7E67-F5C6-4434-97AF-A7809CD5947E">
            <Title>Lexical Style</Title>
            <Text>Star’s lexical syntax is a combination of special operators and keywords.
It can be difficult for language designers to decide when to use a keyword and when to use a special operator. In the case of Star we use special operators for common elements and keywords when either a graphical operator would be obscure and/or are not common.
For example, in the factorial module, we use braces for grouping; but we also use the import and the where keywords. The rationale here is that programmers have become used to seeing braces for grouping statements; whereas the import and where elements are somewhat rarer.
Notice that every statement is terminated with a period. This is one of those places where a little redundancy can help when building large programs: the statement terminator is not technically necessary; but it helps to reduce the scope of error messages.
The precise rule is slightly more nuanced: a period is required to terminate a statement; unless the last character of the statement is a closing brace – or unless the statement is itself the last statement in a brace sequence.
Another aspect of Star’s texture that may not be visible at this stage is the reliance on an underlying meta-grammar – specifically on an operator precedence grammar. OPGs are likely already familiar to you: it is the almost universally used grammar that underpins arithmetic expressions. We take the OPG and stretch its use to include the whole language. The relevance of this will be apparrent when we look at extending Star with new language features.</Text>
            <Comments>Those who remember Algol 60 will understand that braces are not the only way of grouping statements.
We use the period rather than the commonly used semi-colon because Star statements are statements, not instructions to perform in sequence.
This is one of those somewhat pedantic notes!</Comments>
        </Document>
        <Document ID="16476111-0FDE-42C5-8641-519DBB129794">
            <Title>Boiling the Ocean</Title>
            <Text>It is a commonplace in software engineering that you should not try to ‘boil the ocean’; which is a synonym for ‘biting off more than you can chew’. However, it is possible to build very large scale systems if you approach it in the right way. This is the purview of architecture and of software development teams.
The ‘right’ way to boil an ocean is one cup at a time. The ‘smart’ way to do it is to build a machine that makes cups that boil the ocean.
Building software in the context of a team is quite different to writing individual functions in a program. It is not enough for your code to compute the correct function; it must also interact properly with the environment it is in. As a result, professional programmers find themselves often more concerned with making sure that all the ‘pieces’ are in the right place than simply the correctness of an algorithm. Interfaces, contracts, APIs and integration issues often dominate the system builder’s landscape.
One’s attitude to basic language features like types is also different: having to deal with the knowledge that is in your co-worker’s head (and not in yours) should be enough to convince anyone of the merits of strong static types.
Star’s modularity features are built from the same functional programming foundation as the other features of the language. This has important implications for the programmability of larger systems.</Text>
            <Comments>This is not to deny that correctness is important. It is just that algorithmic correctness is not enough.</Comments>
        </Document>
        <Document ID="E2994B4F-05AF-4F0D-96AA-A8036DBDC38B">
            <Title>Chattering Agents</Title>
            <Text>Software agents represent one of those concepts that are very compelling – perhaps precisely because it speaks to natural human intuitions. Agents are also a popular model for distributed systems. Distributed systems are often characterized by multiple loci of control – typically one per machine in the system – and assigning an agent to each locus is a very natural architecture.
In this chapter we look at how we can build agent-based systems and hence distributed systems.
You will notice that the style of this chapter is a little different from previous chapters. This is because many of the concepts we discuss relatively high-level and are not commonly directly grounded in conventional programming paradigms. However, building distributed systems is a programming task; even if many of the issues and concepts don’t have much in the way of touch points in language features.
On the other hand, Star does have some important features that simplify building this style of application: notably the actor, speech actions and support for component architectures.</Text>
        </Document>
        <Document ID="1FE8AD86-BB71-4552-8E69-4DDD034853FA">
            <Title>If You Are Already a Java (or C#, or C++) Programmer</Title>
            <Text>Most OO languages are embracing some of the simpler features of functional languages. Even Java 8 with its lambda expressions and stream features represents a nod to the power of functional programming.
However, at the same time, there is a substantial gap in the capabilities of most OO languages compared to modern functional programming languages. This is a problem because the better a language is able to ‘understand’ your objectives, the better the tools will be able to support those objectives.
Fundamentally, OO languages revolve around nouns rather than verbs. Verbs (methods) are relegated to being inside the scope of some noun (object): they are not first class. However, this leads to unnatural representations where functions do not naturally fit inside some class. In functional programming languages, like Star, there is more of an balance between nouns and verbs.
Functional languages allow a more subtle interplay between data and functions. It is possible to have functions that are about data; it is also quite straightforward to have data structures with functions embedded in them. In fact, a simple definition of a module is a record that contains functions in it.
While OO languages like Java provide excellent data abstraction tools, the same cannot be said for control abstractions. The result is that OO languages are ‘stuck in the 1970s’ when it comes to control abstractions. However, concepts such as map/reduce, computation expressions, and continuations bring a rich suite of new control possibilities that solve important problems in modern programming.
Similarly, the type systems of languages like Java (or C# or C/C++) make are not as expressive or sensitive as modern type systems in functional languages can be. Professional programmers will recognize the typical response to insufficiently expressive types: lots of casting and dynamic meta-programming. But, while powerful, these techniques amount to giving up on types and their important advantages. Furthermore, contrary to many programmers’ expectations, a modern type system is quite capable of dealing with scenarios that require dynamic programming in languages like Java.
In addition, Star’s extensibility directly addresses a real world requirement: that of supporting a separation of policy from mechanism. Typically OO languages are excellent at describing mechanisms but do not fare so well in describing policies – which are typically declarative in nature. Hence the tendency in software engineering circles to adopt text frameworks such as XML and JSON to express policies. However, these technologies lose one of the major advantages of Java – that of type safety.</Text>
        </Document>
        <Document ID="662DB4CF-5056-41A9-A753-67239BC20D5A">
            <Title>There is more to a platform than this</Title>
            <Text>We have actually just scratched the surface of the potential of this kind of programming platform. In truth, like many of the chapters in this book, a full treatment of a boxes-and-arrows platform would justify a book in its own right.
Other aspects that we have omitted include composite components, component templates, wiring diagrams as Star programs, the dynamic behavior or components and so on.</Text>
        </Document>
        <Document ID="4B92C099-83BF-4515-AA63-7B20E776B132">
            <Title>A Totalizer Query</Title>
            <Text>While concise, expressions involving much use of leftFold (and the analogous rightFold) can be difficult to follow. An even clearer way of adding up numbers is to use a query expression:
fold X with (+) where X in L
or the even more succinct
total X where X in L
This query expression frees us from most of the commitments we endured before: it can add up the elements of any kind of collection — not just cons lists — and it can add up floating point numbers just as easily as integers. Finally, we have not had to say exactly how the numbers should be added up: the language system is free to use a parallel algorithm for the computation should it be more optimal.
The query expression is also very close to the natural specification:
Add up the numbers in L
Star’s query expressions — which are similar to but also more expressive than LINQ — can be used to encapsulate a wide range of such computations. We shall look deeper into them when we look at Collections in Star.
Of course, SQL programmers have long had access to this kind of conciseness and declarative expressiveness. However, SQL is constrained by the fact that it is intended to represent queries and actions over a very particular form of data — the relational table.</Text>
        </Document>
        <Document ID="4FAA6B1B-12CF-46C9-9C53-6CF5581C2E7F">
            <Title>Let Binding Environments</Title>
            <Text>We noted that it is difficult to achieve the effect of the (X)=&gt;X==S lambda expression with named functions. The reason is that the lambda is not defined in the same way that named functions are defined — because it occurs as an expression not as a statement. If we wanted to define a named function which also captures S, we would have to be able to define functions inside expressions.
There is an expression that allows us to do this: the let expression. A let expression allows us to introduce local definitions anywhere within an expression. We can define our lambda as the named function isS using the let expression:
let{
  isS:(string)=&gt;boolean.
  isS(X) =&gt; X==S
} in isS
The region between the braces is a definition environment and Star allows any definition statement to be in such an environment. We can define check using a let expression:
check(T,S) =&gt; fTest(T,
  let{
    isS:(string)=&gt;boolean.
    isS(X) =&gt; X==S.
  } in isS)
This is a somewhat long-winded way of achieving what we did with the anonymous lambda function — we would not normally recommend this way of writing the check function as it is significantly more complicated than our earlier version. However, there is a strong inter-relationship between anonymous lambdas, let expressions and variable definitions. In particular, these are equivalent:
let{
  isS:(string)=&gt;boolean.
  isS = (X) =&gt; X==S
} in isS
and
(X)=&gt;X==S
Apart from being long-winded, the let expression is significantly more flexible than a simple lambda. It is much easier within a let expression to define functions with more than one rewrite equation; or to define multiple functions. We can even define local types within the let binding environment.
Conversely, lambda functions are so compact because they have strong limitations: you cannot easily define a multi-rewrite equation function with a lambda and you cannot easily define a recursive function as a lambda.
In short, we would use a let expression when the function being defined is at all complex; and we would use a lambda when the function being defined is simple and small.
Assembling functions in this way, either by using anonymous lambdas or by using let expressions, is one of the hallmarks of functional programming.</Text>
        </Document>
        <Document ID="A4ADB48E-CDA1-4202-B137-AA6D77618C91">
            <Title>Mixing Operators</Title>
            <Text>The standard N3 notation includes some simple extensions to make certain patterns of facts easier to write. For example, we can write:
:Joe ! isa $ :giraffe.
:giraffe ! [subclass $ :mammal, has $ :long-neck].
:mammal ! [subclass $ :animal, has $ :four-legs].
[:giraffe, :mammal, :animal] ! isa $ class.
instead of
:Joe ! isa $ :giraffe.
:giraffe ! isa $ class.
:giraffe ! subclass $ :mammal.
:giraffe ! has $ :long-neck.
:mammal ! isa $ class.
:mammal ! subclass $ :animal.
:mammal ! has $ :four-legs.
:animal ! isa $ class.
For the purposes of the operator grammar, literal values like strings, numbers and bracketed values have a priority of 0. However, within a bracketed term — such as
[:giraffe, :mammal, :animal]
each element of the list has an expected priority; in case of lists the expected priority is 999. I.e., the maximum priority of any term in a list sequence is 999. If you want to embed an operator expression whose top-level operator is higher than 999 in a list then you need to surround it with parentheses.</Text>
        </Document>
        <Document ID="E085F2D9-429D-4493-B767-3C924A758ACE">
            <Title>Escaping our DSL</Title>
            <Text>We are making a subtle choice with these validation rules for concept. We are affirming that the only way that we can embed ‘normal’ Star expressions in a concept graph is through variables.
In general, it is normally desirable to allow arbitrary expressions to be embedded in DSL extensions. However, one has to design the syntactic mechanism that permits this escape. In our case we take the extremely simple approach of only allowing variables.</Text>
        </Document>
        <Document ID="2BB80CC2-1EE6-4A27-B426-3E4F17CB5C64">
            <Title>Macro Rules[macro-rules]</Title>
            <Text>Macros are programs that are executed by the compiler in order to transform terms into simpler forms — with the eventual goal that the terms produced by macro processing are directly understood by the main compiler.
There are two kinds of macros in Star: the macro rule and the code macro. A macro rule is a substitution rule that is applied by the Star compiler during normal compilation. For example, the macro rule:
 - ?X ==&gt; __uminus(X)
is used by the compiler to replace occurrences of unary minus with a call to the standard __uminus function that is part of the standard arithmetic contract.
This rule will ‘fire’ whenever an occurrence of unary minus occurs in your program. It will not fire for regular subtraction — since the pattern on the left of the ==&gt; arrow only matches the unary case.
Like validation rules, macro rules are pattern based: that is, the left hand side of a macro rule is a pattern that is applied to input Star terms. In particular, there may be multiple macro rules that potentially match a given term. This makes macro rules quite a bit more expressive than either the macros in C/C++ or LISP. For example, the following macro rule can replace a multiplication by 2 with an addition:
 2 * ?X ==&gt; let { def x is X } in x+x
This rule will only match multiplication expressions where the left hand side is the literal integer 2. It will not fire for any other form of multiplicative expression.
The eagled-eyed reader may notice a small problem with this macro rule — the variable x may already be free within X. The macro rule notation has a way of dealing with this scenario; which we omit for the sake of clarity.
Although we can use operators when we write macro rules, they are actually insensitive to operators. For example, we could have written this macro rule using a normal function call pattern on the left hand side:
 (*)(2,?X) ==&gt; let { def x is X } in x+x
The (*) forces the compiler to suppress any operator interpretation of the the * character.
Macros written with macro rules can be quite complex, and it is possible to construct cascading sequences of macro rules to implement some impressive transformations. For example, the actor notation and the [speech action notation][speech-actions] is transformed into more regular Star by means of a package of macro rules.
In our case, we will use a macro rule to handle the translation of a says condition:
  ?G says ?S ! ?P $ ?O ==&gt;
    n3(trCon(S),trCon(P),trCon(O)) in G
By itself, rule would translate the term denoting the condition:
personnel says :john :works_in D
into the term:
n3(trCon(:john),trCon(:works_in),trCon(D)) in personnel
This translation is not complete because trCon is not a known function symbol. In fact, we also want to map :john etc. into concepts, i.e., into n3Concept terms; for which we need another set of macro rules:
 trCon(string?S) ==&gt; n3S(S)
 trCon(: #(identifier?N)# ) ==&gt; n3C(N)
The macro pattern string?S matches any literal string — and, if successful, binds the macro variable S to the matched string; similarly the macro pattern identifier?N matches any identifier and binds N to the found identifier.
We also need a third rule that allows for graph variables by mapping identifiers directly to identifiers:
 trCon(identifier?N) ==&gt; N
Notice that, even in this simple situation of matching concept, we had to construct a two-level macro cascade involving the strictly macro-time trCon symbol to ensure that they were translated appropriately. This requirement is one the weaknesses of the macro rule.</Text>
            <Comments>For the technically inclined reader, Star macros are not hygienic. This is a deliberate choice, based on the anticipated role of macros in Star.</Comments>
        </Document>
        <Document ID="1C82C9B2-39A7-4406-B7A3-1D13A61AE0AB">
            <Title>Computing with tasks</Title>
            <Text>Like other values, tasks are first class: they can be assigned to variables, kept in data structures, passed to and from functions and so forth. This flexibility leads to great expressive power — many patterns of computations can be readily encoded as task-valued functions.
For example, consider the mp function — which is a facsimile of the standard fmap function specialized for list values:
mp:all s,t ~~ ((s)=&gt;t,list[s])=&gt;list[t].
mp(F,[]) =&gt; [].
mp(F,[X,..Y]) =&gt; [F(X),..mp(Y)].
In this mp function, the functional variable F denotes the computation to be applied to each element of the input list. Suppose that each computation of F were non-trivial, and we wanted to spread the load across multiple cores — i.e., to perform the map operations in parallel. The task notation, in particular background tasks will help us to achieve that.
Recall that fmap is a standard Star function that is defined in the functor contract and is implemented for many different collection types.
We will first of all transform mp to use tasks rather than simply calling the function:
taskmap:all s,t ~~ ((s)=&gt;t,list[s])=&gt;list[t].
taskmap(F,[]) =&gt; [].
taskmap(F,[X,..Y]) =&gt; [valof task{ valis F(X) } ,.. taskmap(F,Y)].
As it stands, this function has very similar performance characteristics to mp; except that we are using the task expression notation. In order to run the different elements in parallel we need to also use the background operator:
parmap1:all s,t ~~ ((s)=&gt;t,list[s])=&gt;list[t].
parmap1(F,[]) =&gt; [].
parmap1(F,[X,..Y]) =&gt; [valof background task{valis F(X)} ,.. parmap1(F,Y)].
This program computes each element of the result in a separate background task. However, it is not a true parallel fmap because we wait for each element before continuing to the next element.
A better approach is to first of all construct a list of tasks and then to separately collect their values in a second phase:
parmap2:all s,t ~~ ((s)=&gt;t,list[s])=&gt;list[t].
parmap2(F,L) =&gt; let{
  spread:(list[s])=&gt;list[task[t]].
  spread([]) =&gt; [].
  spread([X,..Y]) =&gt; [background task{ valis F(X)} ,.. spread(Y)].

  collect:(list[task[t]])=&gt;list[t].
  collect([]) =&gt; [].
  collect([T,..Ts]) =&gt; [valof T,..collect(Ts)].
} in collect(spread(L))
This function does not wait for any task to complete until they have all been spun out into background activities. This gives the maximum opportunity for the independent tasks to complete before we actually need their values.
A good rule of thumb is:
when you are programming with tasks, everything that is not a completely task-less subcomputation should be enclosed in task brackets.
So, a more idiomatic way of writing the parmap function is to make it a task valued function; we also generalize away from the specific list collection type and use the standard fmap function to allow for any collection type:
parmap:all s,t,c/1 ~~ functor[c] |: ((s)=&gt;t,c[s])=&gt;task[c[t]].
parmap(F,L) =&gt; let{
  spread:()=&gt;list[task[t]].
  spread() =&gt; map((X)=&gt;background task{valis F(X)},L).
  collect:(list[task[t]])=&gt;list[t].
  collect(Lt) is fmap((T)=&gt;valof T,Lt).
} in task{
  valis collect(spread())
}</Text>
            <Comments>Notice that we do not need to mark the sequence expressions as list sequences – type inference takes care of this for us.</Comments>
        </Document>
        <Document ID="68CFB316-5DD5-47BA-8943-0F70F9604FED">
            <Title>The Homunculus in the Machine</Title>
            <Text>Programming is often taught in terms of constructing sequences of steps that must be followed. What does that imply for the programmer? It means that the programmer has to be able to imagine what it is like to be a computer following instructions.
It is like imagining a little person — a homunculus — in the machine that is listening to your instructions and following them literally. You, the programmer, have to imagine yourself in the position of the homunculus if you want to write effective programs in most languages today.
Not everyone finds such feats of imagination easy. It is certainly often tedious to do so. Using query expressions and other higher-order abstractions significantly reduces the programmer’s burden — precisely by being able to take a declarative approach to programming.</Text>
        </Document>
        <Document ID="45A93B69-E22A-479B-A731-1653A81468E6">
            <Title>Programming Speech</Title>
            <Text>A critical aspect of human communication is the vocabulary employed – more formally the ontology being referenced. The natural analog of this in software systems is the Application Programming Interface (API). An API specifies which functions you may invoke, what their types are and what the expected results of invoking the functions should be.
The major semantic difference between an API and an Ontology is that the latter can often convey more semantics. For example, it is possible, in an ontology but not in a typical API, to express the fact that a function called plus adds its numbers together.
As it happens, Star has a natural way of expressing a complete API – by using the record type. For example, the record type defined with the type alias:
myAPI &lt;~ {
  products:list[(string,string)].
  quantity:(string)=&gt;integer
}
can be viewed as the specification of an API. We are not limited to exposing functions in APIs: we can expose literal values, variables and even types.
Having a type that can represent an entire API also allows us to be careful about distinguishing the API from access to the API. We determine access to an API by computing a value whose type is the API – for example by having a variable of the right type:
A:myAPI
Here the API is determined by the myAPI type; access to it is mediated via the variable A. We access the API by accessing A, as in:
show list of { all y where
    (y,"1in-washer") in A.products and
    A.quantity(y)&gt;20 }
While perfectly serviceable, there are some substantial issues with this approach to accessing APIs. For example, we have this A variable showing up everywhere; and it is hard – at first glance – to see how this style of API can help us with building distributed systems.</Text>
            <Comments>There are definite technical limits to this specificity though.</Comments>
        </Document>
        <Document ID="B0417A1B-D617-4CBD-900A-14A6AB12C289">
            <Title>Ideas</Title>
        </Document>
        <Document ID="6DCA380C-91E8-4E8D-9B3B-049355319383">
            <Title>Functions as Values</Title>
            <Text>The second principle of functional programming is that functions are first class. What that means is that we can have functions that are bound to variables, passed into functions and returned as the values of functions. In effect, a function is a legal expression in the language. It also means that we can have function types in addition to having types about data.
We can see this best by looking at a few examples. One of the benefits of passing functions as arguments to other functions is that it makes certain kinds of parameterization easy. For example, suppose that you wanted to generalize check to apply an arbitrary test to each node — rather than just looking for a particular string.
We will first of all define our fTest function itself:
fTest:(sTree,(string)=&gt;boolean)=&gt;boolean.
fTest(sEmpty,_) =&gt; false.
fTest(sNode(L,Lb,R),F) =&gt; F(Lb) || fTest(L,F) || fTest(R,F).
The substantial change here is that, rather than passing a string to look for, we pass fTest a boolean-valued function to apply; within fTest we replace the equality test Lb==S with a call F(Lb).
Notice that the type annotation for fTest shows that the type of the second argument is a function type – from string to boolean.
Given fTest, we can redefine our earlier check function with:
check(T,S) =&gt; fTest(T,(X)=&gt;X==S)
We have a new form of expression here: the anonymous function or lambda expression. The expression
(X)=&gt;X==S
denotes a function of one argument, which returns true if its argument is the same value as S.
Interestingly, it would be difficult to define a top-level function that is equivalent to this lambda because of the occurrence of the variable S in the body of the lambda. This is an example of a free variable: a variable that is mentioned in the body of a function but which is defined outside the function. Because S is free, because it is not mentioned in the arguments, one cannot simply have a function which is equivalent to the lambda as a top-level function.
Free variables are a powerful feature of functional programming languages because they have an encapsulating effect:  in this case the lambda encapsulates the free variable so that the fTest function does not need knowledge of S.
</Text>
        </Document>
        <Document ID="5FF6F734-FE66-4497-9EAD-7CBBB3B733E9">
            <Title>Generic Types</Title>
            <Text>What actually makes fTest more constrained than it could be is the type definition of sTree itself. It too is unnecessarily restrictive: why not allow trees of any type? We can, using the type definition for tree:
all t ~~ tree[t] ::= tEmpty | tNode(tree[t],t,tree[t]).
Like the original sTree type definition, this statement introduces a new type: tree[t] which can be read as ‘tree of something’. The name tree is not actually a type identifier — although we often refer to the tree type — but it is a type constructor.
In an analogous fashion to constructor functions, a type constructor constructs types from other types. Type constructors are even bijections — one-to-one functions from types to types.
The identifier t in the type definition for tree denotes a type variable. Again, similarly to regular variables and parameters, a type variable denotes a single unspecified type. The role of the type variable t is like a parameter in a function: it identifies the unknown type and its role.
The tree type is a universally quantified type. What that means is that instead of defining a single type it defines a family of related types: for example:
tree[string]
tree[integer]
...
are tree types. We can even have trees of trees:
tree[tree[string]]
We capture this genericity of the tree type by using a universal quantifier:
all t ~~ tree[t]
What this type expression denotes is a set of possible types: for any type t, tree[t] is also a type. There are infinitely many such types of course.
The all quantifier is important: as in logic, there are two kinds of quantifiers in Star’s type system: the universal quantifier all and the existential quantifier exists. We will take a deeper look at the latter in a later chapter when we look deeper at modular programming.
Star uses context to determine whether an identifier is a type variable or a type name. Specifically, if an identifier is bound by a quantifier then it must refer to a type variable.
The types of the two constructors introduced in the tree type definition are similarly quantified:
tEmpty:all t ~~ tree[t].
tNode:all t ~~ (tree[t],t,tree[t)]&lt;=&gt;tree[t].
The type tree[t] on the right hand side of tEmpty’s type annotation raises a couple of interesting points:
	1.	This looks like a type annotation with no associated definition. The fact that the tEmpty symbol was originally introduced in a type definition is enough of a signal for the compiler to avoid looking for a definition for the name.
	2.	The type of a literal tEmpty expression — assuming that no further information is available — will be of the form tree[t34] where t34 is a ‘new’ type variable not occurring anywhere else in the program. In effect, the type of tEmpty is tree of some type t34 where we don’t know anything more about t34.</Text>
        </Document>
        <Document ID="67214C4B-A175-4498-9FD7-53BDED8BA242">
            <Title>This Train is Leaving the Station</Title>
            <Text>Perhaps most importantly, we need to be able to do these things now – time to market is a critical factor in many if not most modern applications. Its no good developing the world’s best widget if you run out of ‘runway’ trying to build it.
A major bottleneck is the relative poor productivity of most modern programming languages. It is simply too hard to produce correct robust code in languages like C/C++, Python etc.
Productivity is an issue for individual programmers but is especially salient for programmer teams.
Every successful software project involves a team.
The requirements for team-based development tend to put certain aspects of programming language design into sharp focus. For example, strong types and clear interfaces may be excellent aids for individual programmers but they are absolutely paramount for team development.
More generally, in a competitive environment, the only way to reliably out-perform the competition in reaching the market is to use radically more productive technology.</Text>
        </Document>
        <Document ID="31F45C97-EF45-48D1-B69C-99EE26E10A15">
            <Title>Rendezvous</Title>
            <Text>Tasks support a very simple communication pattern between parts of a computation: a task computes for a while, and then (if appropriate) returns a result to anyone asking for its value — typically a parent task. While a task can spawn sub-tasks, there is no immediate way for tasks to communicate with each other while they are running. Many applications, however, require more fine-grained coordination between tasks.
To permit this better coordination, and to permit data to flow between tasks, we have the rendezvous. A rendezvous is a meeting between two or more tasks or other computations. Data can flow between tasks at a rendezvous; with the guarantee that the data is consistent for the parties at the rendezvous.
The rendezvous mechanism is inspired by Reppy’s Concurrent ML events [#Reppy1999], and by Hoare’s Communicating Sequential Processes[#hoare78] [#hoare85] (which also involves the rendezvous concept). As we shall see, the rendezvous is a simple mechanism that allows implementing almost arbitrarily complex choreographies between concurrent computations. However, we shall also see that the rendezvous mechanism is relatively low-level and there are good higher-level abstractions that make writing concurrent applications significantly easier.</Text>
        </Document>
        <Document ID="C90EE28F-1EFD-4B68-9D0A-31EB962D2130">
            <Title>Getting the value more than once</Title>
            <Text>Note that it is possible to access a task value multiple times. However, the actions in the task body will only ever be performed once: subsequent attempts to get the value of the task simply return the value previously computed.</Text>
        </Document>
        <Document ID="A10B314C-390B-4CF0-A6AC-D533F63C398F">
            <Title>Patterns</Title>
            <Text>A function is defined as a sequence of rewrite equations each of which consist of a pattern and an expression. There are three general forms of rewrite equations:
Pattern =&gt; Expression
or
Pattern where Condition =&gt; Expression
or
Pattern default =&gt; Expression
The left hand side of a rewrite equation consists of the pattern which determines the applicability of the equation; and the right hand side represents the value of the function if the pattern matches.
Pattern
A pattern represents a test or guard on a value. Patterns can be said to succeed or fail depending on whether the value being tested matches the pattern.
We also refer to a pattern being satisfied when matching a value.
The pattern in a rewrite equation is a guard on the arguments of the function call. For example, given a call
sign(34)
the patterns in the different equations of the sign function will be applied to the integer value 34.
When the pattern on the left hand side of a rewrite equation succeeds then the equation fires and the value of the expression on the right hand side of the equation becomes the value of the function.
As we noted earlier, patterns are ubiquitous in Star. They are used in equations, in variable declarations, in queries and in many other places. Here, we shall look at three main kinds of pattern, and in later sections, we look at additional forms of patterns.
Variable Pattern
A variable pattern is denoted by an identifier; specifically by the first occurrence of an identifier.
A variable pattern always succeeds and has the additional effect of binding the variable to the value being matched.
For example, the X in the left hand side of
double(X) =&gt; X+X
is a variable pattern. Binding X means that it is available for use in the right hand side of the equation — here to participate in the expression X+X.
The part of the program that a variable has value is called its scope.
	•	Variables in rewrite equations always have scope ranging from the initial occurrence of the variable through to the whole of the right hand side of the equation.
	•	Variable patterns are the only way that a variable can get a value in Star.
	•	Variables are never redeclared within a given scope. It is not permitted to hide a variable with a new variable that is defined within the natural scope of the variable.
This is somewhat different to the scope rule for most other functional (and non-functional) languages — which allow outer scoped variables to be effectively eclipsed or hidden by inner variables.
The rationale for this choice is based on the observation that errors that arise from mistakenly hiding outer variables are often particularly difficult to track down.
Literal Pattern
A literal pattern — such as a numeric literal or a string literal — only matches the identical number or string.
Clearly, a literal match amounts to a comparison of two values: the pattern match succeeds if they are identical and fails otherwise.
Equality is based on semantic equality rather than reference equality. What this means, for example, is that two strings are equal if they have the same sequence of characters in them, not just if they are the same object in memory.
There is no automatic coercion of values to see if they might match. In particular, an integer pattern will only match an integer value and will not match a float value — even if the numerical values are the same. I.e., there will be no attempt made to coerce either the pattern or the value to fit.
This, too, is based on the desire to avoid hard-to-detect bugs from leaking into a program.
Guard Pattern
Sometimes known as a semantic guard, a guard pattern consists of a pair of a pattern and a condition:
Pattern where Condition
Conditions are boolean-valued and the guard succeeds if both the pattern matches and if the condition is satisfied. Star has a normal complement of special conditional expressions which we shall explore as we encounter the need. In the case of the equation:
 sign(X) where X&gt;0
the guard pattern is equivalent to:
X where X&gt;0
We can put guard pattern anywhere that a pattern is valid; and, for convenience, we can also put them immediately to the left of the rewrite equation’s is operator.
Notice that any variables that are bound by the pattern part of a guarded pattern are in scope in the condition part of the guard.
In the pattern above, the variable X will be bound in the variable pattern X and will then be tested by evaluating the condition X&gt;0.
Subsequent occurrences of variables in a pattern ‘stand for’ equality guards. For example, the equation:
same(X,X) =&gt; true
is exactly equivalent to:
same(X,X1) where X==X1 =&gt; true
or just:
same(X,X1) =&gt; X==X1</Text>
            <Comments>This terminology originates from Logic — where a formula can be satisfied (made true) by observations from the world.</Comments>
        </Document>
        <Document ID="0BC7C19E-EC39-46EC-891F-3639865149D0">
            <Title>After the Translation</Title>
            <Text>Translating a DSL into core Star is not always the end of the story. In addition it is important to consider what other ways it should be integrated with the language. For example, we designed a new condition so that triple graphs could participate in queries. In addition, there may be one or more standard contracts that should be implemented for elements of the DSL.
Finally, we recall that we fixed on the representation of a triple graph as a list of triples, specifically of n3Triple terms. However, this was, perhaps not the best ultimate representation — who is to say that a list is the best way of representing all graphs? In fact, fixing on lists to represent graphs represents a premature commitment: and no single choice is necessarily any better than lists.
A better approach is to insulate the architectural choice point by introducing a contract layer. The purpose of the contract is to delay the actual implementation choice to a point where the choice is easier to make. The contract should encapsulate the choice point; in this case we would specify the operations we might expect of a triple graph:
contract graphStructure over t is {
  emptyGraph:t
  addToGraph:(t,n3Triple)=&gt;t
  findAllSubjects:(t,n3Concept)=&gt;list[n3Triple]
  findAllPredicates:(t,n3Concept)=&gt;list[n3Triple]
  findAllObjects:(t,n3Concept)=&gt;list[n3Triple]
  removeFromGraph:(t,n3Triple)=&gt;t
}
Using such a contract as an underlying specification of requirements for implementation has the merit of allowing evolution in the representation of triple graphs; because, like all languages, we should expect our DSL to evolve too.</Text>
        </Document>
        <Document ID="D4CF0A06-98DD-4E9E-B7E6-E9C8BC65119A">
            <Title>Phew</Title>
            <Text>This Chapter covers some difficult material! We start with a requirement to scale — to be able to scale code from a single module through to applications built by assembling libraries. Along the way we take in existential quantification and abstract data types.
What we have not yet addressed are the needs of distributed applications. Managing distributed applications is one of the most tedious and difficult challenges of modern software development. However, before we can demonstrate Star’s approach to this, we must look at agent oriented systems and actors — the subject of Chattering Agents.</Text>
        </Document>
        <Document ID="185096CD-6C52-48D5-B7CA-8378849F8AD1">
            <Title>Real-time is normal time</Title>
            <Text>Many kinds of business are becoming more and more ‘real-time’: a 100 millisecond slowdown in loading a web page can mean the loss of 5% of revenue for an e-commerce site; an unrented car, like an unrented hotel room, represents a permanent loss of business and a competitive disadvantage.
For the modern programmer, this means that applications must be engineered from the start to be responsive and multitasking – aspects that challenge even the most professional of programmers.</Text>
        </Document>
        <Document ID="9223A0A4-F847-462E-8CB9-9462F4BCB34C">
            <Title>Coercion, not Casting</Title>
            <Text>Star does not support automatic type casting, as found in languages like Java and C/C++. This is for many reasons, not the least of which is safety and predictability of code.
Casting in many languages is really two kinds of operations-in-one which we can refer to as casting and coercion. Casting is mapping of a value from one type to another without changing the value itself; and coercion involves converting a value from one type to another.
For example, the Java cast expression:
(Person)X
amounts to a request to verify that X is actually a Person object. In particular, this only checks the value of X to see if it is a Person. On the other hand, casting an integer to a double involves changing the value to conform to the floating point representation.
Star does not support casting, but does support coercion. However, coercion in Star is never silent or implicit — as it can be in Java and C/C++. An expression of the form:
3+4.5
will fail to type in Star — because there is an attempt to add an integer to a floating point number.
The reason for signaling an error is strongly related to safety and predictability: automatic conversion of integers to floating point can be a common source of errors in languages like C — because such coercions are not always guaranteed to be semantics preserving (not all integers can be represented as floating point values). The implicit coercion of numeric values is easy to miss when reading arithmetic expressions.
Star provides a coercion notation that allows programmers to be precise in their expectations:
(3 :: float)+4.5
denotes the explicit coercion of the integer 3 to a float and type checks as expected.
In fact, type coercion in Star is mediated via a contract and this expression is equivalent to
(_coerce(3):float)+4.5
where _coerce is defined in the coercion contract involving two types:
contract all s,t ~~ coercion[s,t] ::= {
  _coerce :: (s)=&gt;t
}
The coercion contract is an interface, but has no analog in most OO languages: it involves two types — the source type and the destination type. Each implementation of coercion specifies both types. For example, the implementation of coercion between integers and floating point is explicitly given:
implementation coercion[integer,float] =&gt; { ... }
This implementation gives the implementation for coercing integers to floats. Other implementation statements give the definitions for other forms of coercion.
Having coercion as a contract makes it straightforward to add new forms of coercion. This is used quite extensively within Star itself: for example, parsing JSON can be viewed as coercion from string values to json values. Thus the interface to parsers can be standard across all types and parsers.</Text>
        </Document>
        <Document ID="3FE72617-6BC9-44E8-8C48-8424BA92465E">
            <Title>A Taxonomy of Agents</Title>
            <Text>There is a natural hierarchy of types of agent – which roughly aligns with their roles and capabilities in distributed applications:
Agent
An agent is an entity that can act; often on behalf of another entity.
By extension, a software agent is one that acts primarily in the software domain.
The difference between an agent and a function is that the latter must be invoked before any actions in it’s body can be performed; whereas an agent is ‘already’ ready to act.
There is a vast potential range in capabilities of agents; for example, at a lower limit, one can argue that a thermostat can be viewed as a very simple agent – because it represents a localized responsibility for controlling the temperature in a room. Another common, though considerably more complex example, is the web browser: from the perspective of most web servers, browsers are agents that acts in the network for and on behalf of consumers.
There is even a fair amount of autonomy in browsers: modern browsers have features that attempt to keep their owners safe from phishing attacks and also are able to warn users that certain websites are dangerous to visit.
We have already encountered a Star feature that can be interpreted as being agent-like – the task. In fact, tasks have quite a few characteristics of autonomy: they can be performed in the background, they can collaborate via rendezvous and they can be used to implement systems involving multiple loci of activity.
However, a more accurate assessment would be that tasks and the rendezvous are good candidate technologies to realize an agent; just as bits of wire and bimetal can be bent together to make a thermostat. Moreover, Star’s concurrency concepts are more suited to constructing concurrent algorithms than whole distributed systems. This is because the primary issues that drive the complexity of distributed systems are different to those of concurrent algorithms: in particular, the necessity of handling communication between different computers.
Given the basic concept of agent, one might ask what are the characteristic attributes of an agent. Clearly, the most basic attribute is a capacity for action (this is built in to the definition of agenthood). A corollary of this is some kind of sensing capability – to determine what action to take (or not).
Once one can sense and act, the next required capability is the capacity to decide to act. This leads us to a hierarchy of introspective capabilities which we call cognition:
Cognitive Agent
A software agent that has an explicit representation of its own activities and capabilities.
Typically, this model takes the form of internal data structures that contain representations of some of the data the agent has (beliefs), some of the activities the agent is engaged in (actions) and some of the objectives the agent is pursuing (goals). Cognitive agents may also have explicit structures that describe the capabilities of the agent (plans).
Note that mechanical thermostats cannot be considered to be cognitive: there is no representation within a typical thermostat of its switching capabilities: it simply flips when the temperature moves; it cannot know that that is what it is doing – much less know about about the heating system it controls. On the other hand, some modern advanced computer-controlled thermostats certainly do understand that they are controlling heating and cooling systems.
Cognitive agent architectures are useful in situations where the activities of the agent are likely to be dynamic; perhaps responding to multiple stimuli, perhaps being able to perform a range of different capabilities.
#
Cognitive Agent
A cognitive system can be interacted with at a higher level too. One can ask such a system if it is busy, for example.
One systematic way of endowing a software system with such a model is to model the beliefs, goals and ongoing actions as logical structures, perhaps as RDF graphs:
...
beliefs:graph[fact]
goals:graph[goal]
actions:graph[action]
capabilities:graph[plan]
...
Together, of course, with the semantics that links changes in the real world with changes in these variables and with rules that allow beliefs, goals and actions to be properly integrated.
We should also emphasize that many software systems already do have some limited cognitive capabilities. For example, a system with a ‘plug-in’ architecture can be said to have some awareness of its capabilities; similarly, in multi-threaded systems, task queues represent a simple form of self-awareness of current activities.
However, having a systematic representation of beliefs, desires, intentions and capabilities should simplify and accelerate the development of heterogenous agent-based applications.
While the difference between a ‘dumb agent’ and a cognitive agent may be one of degree, self-awareness is viewed as the major distinguishing feature of cognitive agents. Self-awareness is especially useful in situations where agents have to deal with complex overlapping scenarios; such as, for example, with human-computer interaction; or when trying to make sense of noisy physical data with multiple potential interpretations.
A self aware agent can even refuse to perform some activity – one of the more popular definitions of software agents is that they are autonomous systems. This can arise when a cognitive agent can measure the utility value for performing tasks: the basis for refusing to do something would be if the utility is below some threshold.
Even for agent systems that do have explicit representations of themselves there is always a large and deep ‘sub-conscious’ part of the agent – i.e., that part of the agent that is not modeled within the agent. It is simply not possible for an agent to know everything about itself – a situation familiar to most human agents.
Collaborative Agent
A collaborative agent is one which participates in a network of agents; and is able to achieve goals by involving other agents in the network.
A particularly interesting case of collaboration is where one agent recruits one or more agents to help it achieve it’s goals.
Collaborative agents abound in distributed systems; for example, the triumvirate of the browser, the web server and the database server can be viewed as three collaborating agents.
Collaboration is independent of cognition. It is perfectly feasible to design networks of agents that have limited or no self-awareness. Conversely, cognitive agents are not required to participate in a network; although much of the motivation for self-awareness disappears if the agent has no-one to talk to.
The obvious hallmark of collaborative agents is communication: you cannot recruit others to do your bidding if you can’t talk to them. There are many styles of communication possible between software entities; the one that we suggest for use in Star agents is based on speech actions.</Text>
        </Document>
        <Document ID="F74D32DE-B624-4D96-AD66-77C3B5612B20">
            <Title>Basics</Title>
            <Text>It is often easier to introduce functional programming using numerical examples. Last chapter we saw, for example, the factorial program. This is mostly because most programmers are already familiar with numbers. Continuing that tradition, here is a function that returns the sign of a number:
sign:(integer) =&gt; integer.
sign(X) where X&lt;0 =&gt; -1.
sign(0) =&gt; 0.
sign(X) where X&gt;0 =&gt; 1.
Each of these equations applies to different situations: the first equation applies when the input argument is negative, the second when it is exactly zero and the third when it is strictly positive. These represent the three possible cases in the definition of the sign function.
A Star function may be built from any number of rewrite equations; however, they must all be contiguous within the same group of statements.
Although it is good practice to ensure that equations in a function definition do not overlap, Star will try the equations in a function definition in the order they are written in. We could have relied on this and written sign using:
sign(X) where X&lt;0 =&gt; -1.
sign(0) =&gt; 0.
sign(X) =&gt; 1.
Sometimes it is important to mark a particular equation as the default case: i.e., an equation that should be used if none of the other cases apply:
sign(X) where X&lt;0 =&gt; -1.
sign(0) =&gt; 0.
sign(X) default =&gt; 1.
An explicitly marked default equation does not need to be the last equation; but, wherever it is written, default equations are only attempted after all other equations have failed to apply.</Text>
            <Comments>There is a theorem — called the Church Rosser Theorem — that guarantees some independence on the order of the rewrite equations provided that the different rewrite equations that make up function definitions do not overlap. Usually, however, it is too fussy to require programmers to ensure that their equations do not overlap; hence the reliance on ordering of equations.</Comments>
        </Document>
        <Document ID="0E6C19EF-E552-403E-914B-BC27945AB6BB">
            <Title>Background Tasks</Title>
            <Text>A task can be started in the background — i.e., concurrently with other tasks — by using the background operator, which is a function from tasks to tasks. It’s type is given by
background:all t ~~ (task[t])=&gt;task[t].
Applying background to a task not only yields a task of the same type, but also one yielding the same value.
The background operator has a simple effect — it starts its argument task so that it will operate in parallel with other computations.
If it happened to be the case that a backgrounded task already completed, then the background operator has no effect.</Text>
        </Document>
        <Document ID="10F335DB-5C4B-4685-8C61-B865FBC86D1C">
            <Title>Transducer</Title>
            <Text>The connection between a URI and the actual resource must be established by a transducer. A transducer is any system that can take a URI and produce a copy of identified resource. If you will, the transducer establishes the link between a URI and a URL.
The Star compiler has a range of transducers built-in to it; and also has a extensibility mechanism so that you can define your own URI scheme and have it mapped to some physical storage mechanism.</Text>
        </Document>
        <Document ID="553112C1-C7EF-4D29-BDA8-C80B30245C5B">
            <Title>Abstract Data Types</Title>
            <Text>Abstract data types can be viewed as the mathematics behind object oriented programming.
Abstract Data Type
An abstract data type is a mathematical model of a set of related values that share a common set of semantics.
In programming, it is the common semantics that defines the structure; but, of course, programming languages are not able to capture the full semantics of a program or type and hence the stand-in for this is usually a type specification.
Perhaps an example is overdue. In our chapter on Collections we looked at many operators over collections and not a few example collection types. Although programs using the sequence contract are fairly abstract, the type of the collection itself is still visible. Suppose we wanted to build a set structure where only the fact that there is a set, and the set-like operators over the set were visible. The representation type for the set should otherwise be completely opaque.
One might start with a type definition that defines some operators over sets:
type exists coll/1 ~~ genSet ::= genSet{
  type coll/1.
  z:all t ~~ coll[t].
  addElement:all t ~~ (t,coll[t])=&gt;coll[t].
  present:all t ~~ (t,coll[t])=&gt;boolean.
}
The essence of this type declaration is a collection of operators that define set-style operators. By protecting the coll type with an existential quantifier, we ensure that the representation of genSet values is not accessible externally; whilst at the same time we do allow other programs to use the set operators.
One example implementation of genSet might use lists to represent the set structure itself:
LS = genSet{
  all t ~~ coll[t] &lt;~ list[t].
  z = list of [].
  addElement(X,L) where X in L =&gt; L.
  addElement(X,L) =&gt; list of [X,..L].
  present(X,L) =&gt; X in L
}
The statement:
all t ~~ coll[t] &lt;~ list[t].
which is a type alias statement, represents one of the ways that we can give evidence for the existence (sic) of the coll type. We could also have simply declared that:
type coll = list
Given LS, we can use it like a set generator — LS provides a set of operators over sets:
Z = LS.z
One = LS.addElement(1,Z)
Two = LS.addElement(2,One)
The type of LS gives no clue as to the internal type used to represent sets generated by it:
LS:genSet
But Z, One and Two have more interesting types:
Z:collK341[integer]
where collK341 is a Skolem type — a unique type generated when we assign a type to LS. In effect, LS is a module that exports the set type and associated operators; this module is referenced by name and is used to construct particular sets.
A reasonable question here is ‘where is the Abstract Data Type?’. What we have is a record with a type and some functions in it. Recall that an ADT is a ‘model of a set of related values that share a common set of semantics’. The semantics in common are the functions in the record; and the type itself is the existentially quantified type in that record — coll.
Notice how we index off the LS variable to access the operators for this set; even while passing into it instances of sets created and modified by LS. This is one of the hallmarks of a module system.</Text>
            <Comments>Not to be confused with Algebraic Data Types — which represent the mathematical foundation for enumerations and other non-object values.</Comments>
        </Document>
        <Document ID="4942CDCA-7653-45FC-9CE7-E95FFEF1C7D4">
            <Title>Is Star for you?</Title>
            <Text>Choosing a programming language – when you actually have a choice – is highly personal. Here are some reasons to think about Star.</Text>
        </Document>
        <Document ID="32ED86AF-A177-4395-BCC0-DC328C99B92A">
            <Title>Tasks</Title>
            <Text>A task expression denotes a possibly suspended computation — which itself is a sequence of actions — and which may have a value when the computation is performed. Furthermore, tasks have the potential to be executed concurrently or even in parallel with other tasks.
Like tasks, functions can also be viewed as representing suspended computations. However, unlike functions, a task expression represents a single suspended computation.
A task expression is written as a sequence of actions, enclosed in a task block; for example, in:
T = task{
  X = 1;
  Y = 2;
  valis X+Y
}
the variable T is bound to a task expression whose returned value will be 3 — the value returned by the action:
valis X+Y
Task expressions have a type: task[t] where t is the type of the value returned by the task. So, for example, the type of T above is given by
T:task[integer]
as it produces an integer value.</Text>
            <Comments>As in other programming languages, actions are separated by semi-colons in Star.</Comments>
        </Document>
        <Document ID="0203591A-3978-471E-925A-EB6DEF00B015">
            <Title>Adding and Removing Elements From a Collection</Title>
            <Text>The function _set_indexed is used to add an element to a collection associating it with a particular index position; and the function _delete_indexed removes an identified element from the collection.
Both of these functions have a property often seen in functional programming languages and not often seen elsewhere: they are defined to return a complete new collection rather than simply side-effecting the collection. This is inline with an emphasis on persistent data structures and on declarative programming.
One might believe that this is a bit wasteful and expensive — returning new collections instead of side-effecting the collection. However, that is something of a misconception: modern functional data structures have excellent computational properties and approach the best side-effecting structures in efficiency. At the same time, persistent data structures have many advantages — including substantially better correctness properties and behavior in parallel execution contexts.
It should also be stressed that the indexable contract allows and encourages persistence but does not enforce it. It is quite possible to implement indexing for data structures that are not persistent.</Text>
            <Comments>A persistent structure is one which is never modified.</Comments>
        </Document>
        <Document ID="CD1800D1-0F12-4B86-AD24-4E703F0AEF54">
            <Title>Embedding Graphs</Title>
            <Text>One of Star’s standard syntactic paradigms is the ‘brace term’. We use it to represent records, as in:
someone{ name="fred" }
We also use it to represent more complex entities like packages and worksheets:
worksheet{
  show 1+2
}
Continuing in this tradition we will use a similar syntactic structure to represent complete triple graphs:
graph{
  [:peter, :john] ! :in_department $ :accounting.
  :john ! :address $ "2 smart place".
  :john ! :name $ "John Smith".
}
Since this is a first class value, we can have variables bound to triple graphs, as in:
OrgTree : graph[string].
OrgTree = graph {
  [:peter, :john] ! :in_department $ :accounting.
  ...
}
and our queries
list of {all X where OrgTree says X ! :department $ :accounting }
So far, we have designed a new syntax for a triple graph extension to Star with very little effort: a few operator declarations. However, we are not done yet., we need to make sure that the Star compiler understands the new syntax.</Text>
        </Document>
        <Document ID="4F8EB340-2D47-4C32-9408-4FF0F6FF0CC6">
            <Title>A Tour of Star</Title>
            <Text>Our first task is to introduce you to the Star language. It is difficult to find the right order in which to present a programming language like Star; this is because many of the features are highly inter-related. For example, types depend on values which depend on functions which depend on types!
Instead, our approach in this book is to take a series of horizontal slices through the whole language; going progressively deeper as you become more comfortable with the language. Each layer represents a reasonably workable subset of the complete language.
Since a layered approach means that any given description may be incomplete or slightly inaccurate, there is a temptation to use footnote annotations which declare ‘... but there is also ...’.</Text>
            <Comments>Please forgive these pedantic notes when you see them.</Comments>
        </Document>
        <Document ID="17D72D59-F230-4554-A7F7-79E7401E1526">
            <Title>Filtering With filter</Title>
            <Text>The simplest operation on a collection is to subset it. The standard function filter allows us to do this with some elegance. Using filter is fairly straightforward; for example, to remove all odd numbers from a collection we can use the expression:
filter((X)=&gt;X%2==0,Nums)
For example, if Nums were the list:
list of [1,2,3,4,5,6,7,8,9]
then the value of the filter expression would be
list of [2,4,6,8]
The first argument to filter is a predicate: a function that returns a boolean value. The filter function (which is part of a standard contract) is required to apply the predicate to every element of its second argument and return a new collection of every element that satisfies the predicate.
Note that the % function is arithmetic remainder, and the expression X%2==0 amounts to a test that X is even (its remainder modulo 2 is 0).
By using a function argument to represent the predicate it is possible to construct many filtering algorithms whilst not making any recursion explicit. However, not all filters are easily handled in this way; for example, a prime number filter can be written
filter(isPrime,N)
but such an expression is likely to be very expensive (the isPrime test is difficult to do well).</Text>
            <Comments>The original collection is unaffected by the filter.</Comments>
        </Document>
        <Document ID="65021070-164A-464B-943A-3D18D18C241F">
            <Title>Concurrency</Title>
            <Text>Concurrency is about doing more than one thing at once. Concurrency in programming languages is important for two basic reasons: modern processors are easily capable of executing many tasks in parallel (and by implication, if your program does not then you may not be making good use of the machine) and many applications have to be able to respond to events without having to process everything in a fixed order.
Building concurrent applications is hard for programmers primarily because many of the assumptions built into modern programming languages are not valid when performing activities concurrently. The most important of these assumptions relates to state — for example, the values of mutable variables.
In particular, the most basic assumption
the value of a variable stays the same until it is modified
is not valid when the variable in question is shared by multiple concurrent activities. Or rather, technically, it is, but when a variable is shared, it is both difficult and tedious to determine when or if the variable has the expected state. It is difficult to overstate the importance of this.
An interesting parallel shows up in dealing with weightlessness in space: astronauts ‘complain’ of items drifting about in the space station — ‘nothing stays where you left it’. As a result, astronauts must get used to taping everything down while they are working.
Programmers building concurrent systems have a similar experience: they are constantly having to ‘tape down’ pieces of state to make sure that they ‘stay put’.
There are many potential models for concurrency in programming. However, two core ideas seem to stand out: the idea of sequence and the idea of sharing. Sequencing actions in most programming languages is almost completely implicit: the idea of performing actions in sequence is so hard-wired in languages like Java and C/C++ that one must think hard to step outside that frame of reference. However, in a concurrent execution, sequentiality is lost: two activitivies can proceed in parallel and the relative order of actions between the activities is not possible to predict.
The second concept that is so implicit is that of sharing: in a sequential program we assume that subsequent actions share the effects of earlier actions. But, again in a concurrent context, sharing is not obvious; and it is hard to reason about.
Our approach is to make explicit that which is implicit. We reason about actions and their order explicitly – by employing the classic software engineer’s approach of taking an extra level of indirection: we take a handle on the computation itself using the concept of a first-class task as a unit of execution. Similarly, we reason about sharing by focusing on explicit coordination between tasks and on explicitly modeling sharing by communicating between them.
One thing that may strike the reader is that tasks, and as we shall see more generally computation expressions, are associated with actions. This is new in our text so far – we have mostly focused on functions and related concepts. The reason for the change is straightforward: concurrency is inherently about the relative ordering of computation. That makes the action paradigm a natural fit for tasks and concurrency. However, we are fairly disciplined in our approach to actions; in particular, we strongly regulate implicit sharing of state. In fact, it is the implicit sharing of state that is the biggest issue with sequential programming.</Text>
            <Comments>We reserve the right to conflate strict parallelism with apparent parellelism here.</Comments>
        </Document>
        <Document ID="B8977328-6A9C-47D9-BC5B-9765393225D9">
            <Title>No Time</Title>
            <Text>The notify speech action does not explicitly refer to time, including the time of the event itself. This is because there may be multiple senses in which time must be conveyed: the time of the occurrence, the time of its being noticed, or the time of this speech action.
Furthermore, not all applications need time to be explicitly identified. An extreme example of this would be the ticking of a clock. Any listener to a mechanical clock’s ticking would confirm that neither tick nor tock is timestamped! However, each tick does announce the passing of another second. Instead, it is assumed that the listener has some other way of determining the time (by looking at the clock face). In general, it is expected that each application will incorporate an appropriate model of time for its notify events.</Text>
        </Document>
        <Document ID="758594EB-4005-4184-A4B3-3301CB58CF79">
            <Title>Cover</Title>
        </Document>
        <Document ID="55FFCE35-60F0-4B83-81CB-FF9958ECB2AB">
            <Title>Programming has Changed</Title>
        </Document>
        <Document ID="4DA7D0E9-226A-405B-8373-E4018633E878">
            <Title>Types</Title>
            <Text>Star is a strongly, statically typed language. This means that all variables and expressions have a single type; and that all type constraints are enforceable at compile-time. This is a fairly strong (sic) statement but it is a key aspect of Star’s design – we need everything to be well typed and we also want to guarantee completeness of the type system.
The type annotation statement:
fact : (integer)=&gt;integer.
is a statement that declares that the type of fact is a function of one integer argument and which returns an integer result.
Star requires all top-level definitions – like fact here – to have explicit type annotations. For top-level functions, that annotation is often contiguous in the text; but in other cases that may not be the case.
Other variables – like the variable N which is part of the second recursive equation – do not need type annotations. This is possible because underlying the type system is a powerful type inference system that can actually infer all types.
The result is that a lot of the ‘clutter’ that can pervade a strongly typed language is just not necessary; but the use of explicit type annotations for top-level definitions provides useful structure and documentation.
Note that the requirement is that top-level definitions have explicit type annotations. We don’t distinguish functions in any way here. In particular, functions which are not part of a top-level definition – for example lambda functions – do not need type annotations; on the other hand, other top-level variables still need an explicit type annotation.</Text>
            <Comments>The definition of top-level is simply whether the definition statement is immediately embedded in a {} sequence of statements – it is quite possible to have top-level definitions appearing within expressions.</Comments>
        </Document>
        <Document ID="31320BB5-ECF8-4D50-A86C-EEAE925EA8D4">
            <Title>Patterns</Title>
            <Text>Patterns are ubiquitous in Star: they form the basis of many rules: they are used to define equations, they are used to control action programs, they are used to implement macros.
A pattern can be viewed as a combination of a test — does a value match a particular pattern — and as a way ( the way in Star) of binding a variable to a value.
An equation’s pattern defines when the equation is applicable. The first equation for fact above:
fact(0) =&gt; 1.
has a literal pattern on the left hand side of the =&gt; operator. This equation only applies when fact is called with zero as its argument.
The pattern in the second equation:
fact(N) where N&gt;0 =&gt; N*fact(N-1).
has a guard on it — after the where keyword. Guards are additional conditions that constrain patterns. In this case, the equation only applies if the argument is greater than zero.
Any pattern may be qualified with a guard; we could have written the guard inside the argument pattern:
fact(N where N&gt;0) =&gt; N*fact(N-1).
We did not because having the guard outside the arguments is neater.
Note that the fact function’s equations are not fully covering: there are no cases for fact for negative numbers. This means that the function is partial; and if called with a negative number will result in a run-time exception.</Text>
        </Document>
        <Document ID="9C47FE92-A3E7-4D4A-A856-E4CA8C558327">
            <Title>Manuscript Format</Title>
        </Document>
        <Document ID="3E11C944-3029-462E-985E-325628B182D2">
            <Title>A Stream of Naturals</Title>
            <Text>Before we look at the main sieve, let us see how we construct a generator stream. In the case of the sieve, we need a stream of positive integers (naturals) that will act as the source of potential prime numbers (to be filtered of course).
The naturals function is set up to return a channel on which will be placed the odd integers greater than 2:
def naturals is let {
  def natChannel is channel()
  { ignore background task {
      var counter := 3
      while true do{
        perform wait for sendRv(natChannel,counter)
        counter := counter+2
      }
    }
  }
} in natChannel
This has a similar structure to the filter function except that the loop does not wait for any input — it keeps on generating odd numbers. This is where the synchronous nature of communication is important: the action:
perform wait for sendRv(natChannel,counter)
will only complete if there is a task that is waiting for the message. So this streamer will not gush out numbers unless there is someone listening.</Text>
        </Document>
        <Document ID="8AEC929A-ECD3-4FF5-9C39-C470EE03CF7B">
            <Title>The Index Notation</Title>
            <Text>Given the indexable contract we can now show the specific notation that Star has for accessing elements of a collection. Accessing a collection by index follows conventional notation:
C[ix]
will access the collection C with element identified by ix. For example, given a dictionary D of strings to strings, we can access the entry associated with “alpha” using:
D["alpha"]
Similarly, we can access the third character in a string S using:
S[2]
As might be expected, given the discussion above, the type of an index expression is optional. This is because the element may not be there; i.e., it is an example of a tentative computation.
The most natural way of making use of an index expression is to use it in combination with a ?.= condition or an ?| expression — which allows for smooth handling of the case where the index fails. For example, we might have:
nameOf(F) where N ?.= names[F] =&gt; N.
nameOf(F) =&gt; ...
We will take a deeper look at exceptions and more elaborate management of tentative computation in the section on Computation Expressions.
Star also has specific notation to represent modified collections. For example, the expression
D["beta"-&gt;"three"]
denotes the dictionary D with the entry associated with "beta" replaced by the value "three". Note that the value of this expression is the updated dictionary.
For familiarity’s sake, we also suppose a form of assignment for the case where the collection is part of a read-write variable. The action:
D["beta"] := "three"
is entirely equivalent to:
D := D["beta"-&gt;"three"]
always assuming that the type of D permits assignment.
Similarly, the expression:
D[\+"gamma"]
which denotes the dictionary D where the value associated with the key "gamma" has been removed.
In addition to these forms, there is also a test expression:
present D["delta"]
which is a predicate that is true if the dictionary D contains an entry for "delta".
Although, in these examples, we have assumed that D is a dictionary value (which is a standard type in Star); in fact the index notation does not specify the type. As with the sequence notation, the only requirement is that the indexable contract is implemented for the collection being indexed.
In particular, index notation is supported for the built-in list types, and is even supported for the string type.
In addition to the indexed access notation described so far, Star also allows a variant of the sequence notation for constructing indexable literals (aka dictionaries). In particular, an expression of the form:
["alpha"-&gt;1, "beta"-&gt;2, "gamma"-&gt;3]
is equivalent to a sequence of tuples, or to: 
_cons(("alpha",1),_cons(("beta",2),_cons(("gamma",3),_nil()))) 
which is understood by indexable types as denoting the contruction of a literal.
Note that there are two levels of domain-specific notation here: the representation of indexed literals in terms of a sequence of two-tuples and the implicit rule governing indexable types: they should implement a specific form of sequence contract. Both are actually part of the semantics of representing indexable literals.</Text>
        </Document>
        <Document ID="B5F004D8-3919-461F-85D7-00415199CE6C">
            <Title>Generic Programs</Title>
            <Text>If we take a slightly closer look at fTest, we can see that it seems pretty generic: it has a function as argument that it calls in the right places; but otherwise makes few assumptions about the function it calls. However, the type annotation of fTest is rather specific:
fTest : (sTree,(string)=&gt;boolean)=&gt;boolean
which means, for example, that its second argument must be a function from string to boolean. On the other hand, nothing in the actual definition of fTest seems to depend on strings.</Text>
        </Document>
        <Document ID="BB360289-7F77-4AF1-A155-43057C51131A">
            <Title>Title Page</Title>
            <Synopsis>Title page to the manuscript.</Synopsis>
            <Text>Your Name
Your Address
Your phone number
Your e-mail address

(Your agent’s name)
(Your agent’s address)
&lt;$wc100&gt; words










&lt;$projecttitle&gt;

by &lt;$fullname&gt;</Text>
            <Notes>This is the title page of the manuscript. Note that its “Section Type” is automatically set to “Front Matter” in the Metadata pane of the Inspector, allowing the title page to be formatted differently from body text during Compile.

The &lt;$PROJECTTITLE&gt; and &lt;$fullname&gt; tags get replaced with metadata information you can set when compiling. Other information is taken from Contacts when the project is created.

The &lt;$wc100&gt; words tag will be replaced with the word count rounded to the nearest 100 during Compile (this and other tags can be inserted from the Insert menu).

Feel free to edit the text of the title page as required.</Notes>
        </Document>
        <Document ID="CCFD0B3E-955C-46BB-BC11-D8D2E66DC79B">
            <Title>frontCover</Title>
        </Document>
        <Document ID="59362CF3-E791-4A36-BB4C-ADAC8A5E5D1E">
            <Title>A rendezvous is a Future Event</Title>
            <Text>A rendezvous is an object describing an event that may or may not happen in the future. The basic operation for a rendezvous is waiting for the event, and when the event happens, it may yield or consume a value.
This is reflected in the rendezvous type itself:
all t ~~ rendezvous[t]
One of the simplest forms of rendezvous denotes a timeout:
timeoutRv:(integer) =&gt; rendezvous[()].
The rendezvous returned by timeoutRv(10) describes an ‘‘alarm clock’’ event that happens 10 milliseconds in the future. Note that the type of the rendezvous returned specifies () as the type of the value yielded by the event - the timeout event provides no information other than that it happened.
It may seem a little odd that a rendezvous — which is after all a meeting — is denoted by an actual value. The primary reason for this is that is allows specific synchronizations between tasks to be computed, not just programmed. This is a simple but major departure from the approach taken to concurrency by languages that support locking features; but is actually highly reminiscent of Unix’s select function.
Timeouts can be useful in their own right, but they are commonly used in conjunction with other rendezvous. The pattern being that if one of the other ‘things going on’ does not happen, then the timeout will tell us that. However, recovering from a timeout can be very problematic — since you now have to decide what to do about the other activities that were pending.</Text>
        </Document>
        <Document ID="4137AD24-515E-4399-B662-85BA016DAAE4">
            <Title>Speech Actions</Title>
            <Text>As the term suggests, speech actions are a communications model based on an anthropomorphic understanding of how agents communicate. Speech actions were first investigated by John Austin[#austin:60] in the 1940’s as a vehicle for understanding the role of speech in human society. Since that time the basic ideas have been progressively formalized by John Searle [#searle:69] and standardized in KQML [#kqml:93] and FIPA [#FIPA].
Beyond the anthropomorphism, there are sound technical justifications basing computer communications on speech act theory.
Speech Action
A speech action is a communication action intended to have an effect on the listener.
For example, the classic:
I pronounce you man and wife
is a speech action. It has an effect; even though no boulders are moved directly as a result of it, it has a substantial impact on the social fabric and therefore can have unbounded consequences in the physical world.
Informally, a speech action can be considered to be a pair – a performative verb and a content – which in some formalizations takes the form of a declarative proposition. The performative, a communication verb like inform, command, query, or declare, indicates how the content is to be interpreted by a listener. This is highly stylized of course, but it represented a huge advance at the time regarding how communication between intelligent entities (people) should be understood.
Another aspect of speech actions is that context is very important. It matters that the agent speaking has the authority to make the pronouncement, and that it is done in the right way and in the appropriate situation. Imagine a real judge saying ‘‘I pronounce you man and wife’’ in a theatrical play.
Under normal circumstances, i.e., in the setting where the judge would typically make such a pronouncement, the result of the utterance is a new married couple. However, in this case it would not count as an official action because of the fact it was a line in a play. The person making the declaration is the right person, and has the authority to perform the action but the context is wrong.
An interesting implication here is that within the play the characters should respond to the matrimonial as though it were real – if the play is to be believable!
Speech actions are a great basis for expressing the content of communication between software agents – they permit systematization of communication at a far higher level than purely syntactic structures (such as XML and JSON). That higher level enables us to build a common platform that addresses some key questions:
	•	Is the communication valid, authenticated, and authorized?
	•	Is any requested action congruent with our own objectives?
	•	Can I reliably route the communication to another agent – in a way that properly conveys the intention of the communication?
The latter issue relates to the question of public semantics. This is often misunderstood, but can be defined:
Public Semantics
A communication has a public semantics if a third party listening to the communication would understand the communication in the same manner as the sender and receiver of the communication.
Note that public semantics does not require a third party listening in to every communication; it only requires the adherence to shared standards when communicating speech actions.
Since we are talking about communication in the context of software, other considerations are also important; in particular, type safety, crossing machine boundaries and efficiency.</Text>
        </Document>
        <Document ID="728B9E42-036B-4F39-8952-33EFF99DE51C">
            <Title>Summary</Title>
            <Text>Collections form an important part of any modern programming language. The suite of features that make up the collections architecture in Star consists of a number of data types, contracts and special syntax that combine to significantly reduce the burden of the programmer.
The collections facility amounts to a form of DSL – Domain Specific Language – that is, in this case, built-in to the language. We shall see later on that, like many DSLs, this results in a pattern where there is a syntactic extension to the language that is backed by a suite of contracts that define the semantics of the DSL.</Text>
        </Document>
        <Document ID="6F09EA9E-89B9-4045-9F65-6E70DA6E51CE">
            <Title>Worksheets</Title>
            <Text>The other main kind of compilation unit is the worksheet. Worksheets are a modern replacement for the REPL that you see in many functional programming languages.
We say a modern replacement for REPLs because worksheets fit much better in the typical environment of an IDE.
A worksheet can be used to implement the infamous hello world example in just a few lines:
worksheet{
  show "hello world".
}
We can also use a worksheet to display the results of using and testing our fact function:
worksheet{
  import sample.factorial.
  show "fact(10) is \(fact(10))".
  assert fact(5) == 120.
}
Worksheets are like a combination of a module and the transcript of a session. In an IDE, the ideal mode of displaying a worksheet is via an interactive editor that responds to edit changes by recomputing the transcript and displaying the results in-line.
The key features of a worksheet that we will use are the ability to import packages, define elements, show the results of computations and define assertions.</Text>
            <Comments>Read-Eval-Print-Loop</Comments>
        </Document>
        <Document ID="B6E994CA-B2D2-4904-AD52-23FE1AA72349">
            <Title>Rules</Title>
            <Text>In Star, most programs are defined using rules. In this case, fact is defined using equations. The equations that make up a function definition (or any program definition for that matter) are statements that are written in order.
Rule-based programs support a case driven approach to programming: a program is defined in terms of the different cases or situations that apply. Using rules to cover different cases allows the programmer to focus on each case in relative isolation.
In addition, as we shall see later on, the partitioning of programs into cases like this is very helpful in supporting large-scale issues such as code annotations, versioning and life-cycle management.
Star has various kinds of rules, including function definitions, pattern definitions, variable definitions and macro definitions. Furthermore, it is possible for new kinds of rules to be introduced – via the extensibility mechanisms of Star (more on that in Chapter 7).</Text>
        </Document>
        <Document ID="990353AC-59B0-484B-ACA6-2A421CF6C569">
            <Title>Packages</Title>
            <Text>The normal compilation unit is a package. The sample.factorial package contains just the function fact, but packages can contain functions, type definitions, import statements and many other elements that we will encounter.
Package names and references to packages do not refer to file names; package names are symbolic – in general a package name consists of a sequence of identifiers separated by periods.
The catalog and repository system explored in Chapter 7 that supports the language ensures a proper connection between files and packages.</Text>
        </Document>
        <Document ID="C4B1ABF9-8DE9-4E82-BAF9-325BC6957FBC">
            <Title>Injection</Title>
            <Text>Injection is a technique where we specialize a program with additional information; especially where that additional information is not part of the normal argument flow. Of course, it can be hard to be crisp about ‘not part of the normal argument flow’; but injection is an architectural technique to apply if and when it makes a difference in the readability of your code.
Injection is often used to manage configuration of code: the configuration is injected into the main program; for example, we might configure an application server with the path name of a particular application, or with the port on which the app server should be listening. Neither of these would normally be considered part of the normal information flow in an application server.
There is a standard functional programming style that can be used to represent injection — namely functions that return functions. To take an extremely simple example, suppose that we wanted to have a function that counted the percentage of a class that passes an exam. The function itself is pretty simple:
passes(L) =&gt; fraction(
   size(list of { all X where X in L and X.score&gt;Pass}),
   size(L))
The configuration parameter here is obviously the Pass value; this is an important parameter to the function but is not part of the normal argument flow (think about computing the pass count for an entire school).
We can use the function returning approach to inject an appropriate value of Pass:
passes(Pass) =&gt;
  (L)=&gt;fraction(
     size(list of { all X where X in L and X.score&gt;Pass}),
     size(L))
Using this passes is a two-step process; we first use a specific passing grade to construct the test function and then use this to measure performance on groups of students:
HS = passes(60)
allPass = list of { all C where C in Courses and HS(C)&gt;0.80 }
The two-step process is a key part of the injection technique.</Text>
        </Document>
        <Document ID="1DB553CD-C2B4-4AFD-93DE-B09715BE22A2">
            <Title>The Set Type</Title>
            <Text>There are many instances where a programmer needs a collection but does not wish specify any ordering or mapping relationship. The standard set type allows you to construct such entities.
Using a set type offers the programmer a signal that minimizes assumptions about the structures: the set type is not ordered, and offers no ordering guarantees. It does, however, offer a guarantee that operations such as element insertion, search and set operations like set union are implemented efficiently.
Like dictionary, the set type is not publicly defined using an algebraic type definition: its implementation is private. It’s type is given by the template:
all t ~~ equality[t] |: set[t]</Text>
        </Document>
        <Document ID="4284A83D-210C-4039-8BFA-ECA609AEB0B1">
            <Title>Functions and Closures</Title>
            <Text>If a function is an expression, what is the value of the function expression? The conventional name for this value is closure:
Closure
A closure is a structure that is the value of a function expression and which may be applied to arguments.
It is important to note that, as a programmer, you will never ‘see’ a closure in your program. It is an implementation artifact in the same way that the representation of floating point numbers is an implementation artifact that allow computers to represent fractional numbers but which programmers (almost) never see explicitly in programs.
Pragmatically, one of the important roles of closures is to capture any free variables that occur in the function. Most functional programming languages implement functions using closure structures. Most functional programming languages (including Star) do not permit direct manipulation of the closure structure: the only thing that you can do with a closure structure is to use it as a function.
In the world of programming languages, there is a lot of confusion about closures. Sometimes you will see a closure referring to a function that captures one or more free variables.</Text>
        </Document>
        <Document ID="2D2560F1-727C-4521-8811-6C795E63B61F">
            <Title>Indexing</Title>
            <Text>Accessing collections conveniently is arguably more important than a good notation for representing them. There is a long standing traditional notation for accessing arrays:
L[ix]
where L is some array or other collection and ix is an integer offset into the array. Star uses a notation based on this for accessing collections with random indices; suitably generalized to include dictionaries (collections accessed with non-numeric indices) and slices (contiguous sub-regions of collections).
Before we explore Star’s indexing notation it is worth looking at the contract that underlies it — the indexable contract.</Text>
        </Document>
        <Document ID="C21C5773-343E-4F28-8182-743C5A58A76E">
            <Title>Texture</Title>
            <Text>All programming languages can be said to have a particular style or texture. This is often so strong that it is often possible to identify a programming language from a single line of source code. In the case of Star, this line might be:
public fact : (integer)=&gt;integer.
which is a type annotation statement declaring the type of the function fact.
The public annotation means that the function is exported by this module and will be available in other modules if the sample.factorial module is imported.</Text>
        </Document>
        <Document ID="E214E689-54EF-49A0-8294-EC5B019C1B2C">
            <Title>Generic Functions</Title>
            <Text>Given this definition of the tree type, we can construct a more general form of the tree test function; which is almost identical to fTest:
test:all t ~~ (tree[t],(t)=&gt;boolean)=&gt;boolean.
test(tEmpty,_) =&gt; false.
test(tNode(L,Lb,R),F) =&gt; F(Lb) || test(L,F) || test(R,F).
and our original string check function becomes:
check(T,S) =&gt; test(T,(X) =&gt; X==S)
The type of check is also more generic:
check:all t ~~ (tree[t],t)=&gt;boolean
I.e., check can be used to find any type of element in a tree — providing that the types align of course.
Actually, this is not the correct the type for check. This is because we do not, in general, know that the type can support equality. The precise type for check should take this into account:
check:all t ~~ equality[t] |: (tree[t],t) =&gt; boolean</Text>
        </Document>
        <Document ID="9CD8A291-B83F-4CC5-9196-196E15FBDF2B">
            <Title>Use and Evidence</Title>
            <Text>To get a better grip on type quantification in general and existentially quantified types in particular it can be helpful to see what the rules and expectations are for a universally quantified type. Occurrences of a type variable can be classified into two forms — depending on whether the context is one of using a variable or whether one is defining it — or providing evidence for it.
Consider the simple function d with its type declaration:
d:all t ~~ arith[t] |: (t)=&gt;t.
d(X) =&gt; X+X.
In the equation that defines d the rules of type inference lead us to determine that — within the equation — type of X is t. The only assumption we can make about t is that it implements the arith contract; no other constraints on t are permitted. For example, if we had forgotten the arith constraint, the compiler would have complained because we try to add X to itself, which implies that X had an arithmetic type.
In general, whenever we define a universally quantified type we cannot make any assumptions about what it can and cannot do – apart from any explicitly defined constraints.
On the other hand, when we call d, the rules are more generous: calls of the forms d(2) and d(2.4) are both permitted because we are allowed to use any type — that implements the arith contract.
The reason we can substitute any type for t is a result of the universal quantifier: an all quantifier means we can use any type for t.
We can summarize this by stating that, for a universally quantified type, 
	•	we can use it for any type, but
	•	we can make no assumptions about it when giving evidence for a correct implementation. 
For existentially quantified types the situation is reversed: when giving evidence in an implementation involving an existential type we can use what ever type we want — again, providing that other constraints have been met — but outside the defining expression we can’t make any assumptions or additional constraints about the quantified type.
For various reasons, which we will explore further, existentially quantified types are mostly associated with records — like the package record we saw earlier. 
As with universally quantified types, there are two kinds of contexts in which we use existentially quantified type variables: use and evidence contexts. In the former we are using the type and in the latter we are providing evidence that an expression has the right type.
The existential quantifier means that within an instance of this record we can instantiate f to any type that meets the constraints. The simplest way is to provide a type definition for f:
simple = {
  all t ~~ f[t] &lt;~ foo[t].
  all t ~~ foo[t] ::= foo(t) | bar.

  fooMe(X) =&gt; foo(X).
}
The type alias statement gives evidence for f by identifying it with the type foo – which is declared within the theta record. We can do this because, within the implementing expression, we can do whatever we like for the type f – so long as it exists.
Notice that we are actually need to achieve two separate but related goals when describing the package as a record: we need to define a type within the record and we need to be able to have external references to the foo type field.
Externally, when we use the psckage record, the opposite happens: we can make use of the existence of foo but we cannot further constrain it. We can use simple’s foo type, as well as the fooMe function that relies on it; for example, in:
m:for all t ~~ (t)=&gt;simple.foo[t].
m(X) =&gt; simple.fooMe(X).
There is something a little different going on here: the type of m appears to be dependent on a field of the variable simple; i.e., the type of m apparently depends on the value of simple. This is not something that we would normally sanction in a type system because of the potential for disaster.
For example, consider the scenario where ‘simple is not simple’; i.e., suppose that its value were computed; for example suppose that the value of simple depended on a conditional computation. In that case the actual type simple.foo might also depend on the conditional computation; why does this not cause problems?
Normally such ‘dynamic types’ do cause substantial problems. However, the type rules for existentially quantified variables are crafted so that it must not matter what the actual type simple.foo is. So long as no additional constraints on the simple.foo are permitted then the program is provably compile-time type-safe. I.e., uses of an existentially quantified type may not further constrain the type — the exact complement of the situation with universally quantified types. 
Of course, as with universally quantified types, we can constrain the existential type with a type constraint. This would mean that, when implementing it we have to give evidence for the constraint being satisfied and when using it we could rely on that implementation – without knowledge of the actual implementation.</Text>
        </Document>
        <Document ID="3EACB397-C9FC-446E-A918-EF014078DB0D">
            <Title>How to Design a Domain Specific Language</Title>
            <Text>One of the primary reasons for wanting to have domain specific languages is to be able to succinctly express policy. Pretty much every significant application and module tends to be more general than the individual problem it was designed for; which means that actually applying the application (sic) amounts to using a subset of the capabilities of a general mechanism to solve the specific problem.
Policy has a number of definitions, one of which is
Policy
An expression of a constraint that governs the behavior of a system.
The term system here is intended to be used broadly, in particular to include users and humans agents participating in the system.
It is possible to see — without squinting too hard — that a policy is a statement that describes a subset of the potential behaviors of a system. Furthermore, one can often express a particular use of a capability or application in terms of constraints — i.e., in terms of policies.
An example might help. Suppose that you want to apply statistical algorithms to measure how well your stocks are doing. You might want to know if your portfolio is doing better than the average for example.
If you did not have access to any statistical functions, then you would have a choice when it comes to computing the average performance: you can construct a function that only computes the average of your stock portfolio, or you can be sensible and split the problem in two: write (or get) the necessary statistical code and apply it to the problem of computing averages in your portfolio. In fact, you are most likely to construct a range of ‘portfolio functions’ in the expectation that you may have other calculations to make: like computing overall profit and loss of your stock; you will then select which function to apply when you use the application.
In this case, the core algorithms for computing statistical averages (and other statistical functions) form a mechanisn that can be applied to solving the problem at hand. Using the portfolio analysis application becomes, in effect, a matter of choosing which policy to use to constrain all the available uses of statistics into the one that is important.
It might seem a stretch to refer to selecting a function as a constraint, but consider a slightly different scenario: suppose that there are several forms of regression analysis functions in your library. You want to pick the best one to enable you to forecast your stock. Picking the best regression function is often a case of selecting which features are most important — i.e., it is a matter of policy.
The pressure to factor the problem into mechanism and policy is so strong that it is easy to believe that this split is the only way of solving the problem. It isn’t, of course, and there are many less obvious examples than figuring out portfolios.
DSLs and policy languages often involve quite different semantics from the host language; they are often declarative in nature — specifying what needs to be done and leaving the how to the mechanism. One of the simplest purely declarative language systems is the triple logic as seen in RDF. RDF makes a great basis for representing graph structures generally and configurations specifically.</Text>
            <Comments>A regression function is a line fitting function.</Comments>
        </Document>
        <Document ID="5C36B898-91CD-4AB4-8B99-FB158146929A">
            <Title>Order of Evaluation</Title>
            <Text>Star is a so-called strict language. What that means is that arguments to functions are evaluated prior to calling the function. Most programming languages are strict; for two main reasons:
	1.	It is significantly easier for programmers to predict the evaluation characteristics of a strict language.
	2.	It is also easier to implement a strict language efficiently on modern hardware. Suffice it to say that modern hardware was designed for evaluating strict languages, so this argument is somewhat circular.
There many possible styles of evaluation order; one of the great merits of programming declaratively is that the order of evaluation does not affect the actual results of the computation.
It may, however, affect whether you get a result. Different strategies for evaluating expressions can easily lead to differences in which programs terminate and which do not.
One other kind of evaluation that is often considered is lazy evaluation. Lazy evaluation means simply that expressions are only evaluated when needed. Lazy evaluation has many potential benefits: it certainly enables some very elegant programming techniques.
Essentially for the reasons noted above, Star does not use lazy evaluation; however, as we shall see, there are features of Star that allow us to recover some of the power of lazy evaluation.
The other dimension in evaluation order relates to the rewrite equations used to define functions. Here, Star uses an in-order evaluation strategy: the equations that make up the definition of a function are tried in the order that they are written — with the one exception being any default equation which is always tried last.</Text>
            <Comments>Even predominantly lazy languages like Haskell have features which implement strict evaluation. It reduces to a question of which is the default evaluation style.</Comments>
        </Document>
        <Document ID="3ECF6511-EC5A-4B9A-B12F-039C0CDD2CB7">
            <Title>Copyright</Title>
            <Text>
















Copyright © &lt;$year&gt; &lt;$author&gt;
All rights reserved.
ISBN:
ISBN-13:
</Text>
            <Notes>Feel free to delete this document if you don’t need it, or edit it for your needs.</Notes>
        </Document>
        <Document ID="E311189B-D01A-476D-9DC2-6AACB13E99C2">
            <Title>Acknowledgements</Title>
            <Text>Noone is an island, and no project of this scale is one person’s work. I have had the great fortune to be able to develop Star in the context of real world applications solving hard problems. Individuals have also played a large role; and it can be hard to ensure that all are properly acknowledged: please forgive any omissions.
Of particular significance, I would like to thank Michael Sperber for our many discussions on the finer topics of language design; and for his not insignificant contributions to the implementation itself.
I would also like to thank my colleagues at Starview inc., in particular Steve Baunach and Bob Riemenschneider who were the world’s first Star programmers! In addition, I would like to thank David Frese and Andreas Bernauer who helped with crucial parts of the implementation of the concurrency features. I would also like to thank Keith Clark, Kevin Cory, Prasenjit Dey, Chris Gray, Mack Mackenzie, and Kevin Twidle for their help and advice. I would like to acknowledge the support of Thomas Sulzbacher who originated the project and Jerry Meerkatz for keeping the faith.
Last, but definitely not least, I would like to acknowledge the love and support of my family; without whom none of this makes sense.</Text>
        </Document>
        <Document ID="523195FA-6263-4D3D-BCA5-4365F4020A64">
            <Title>Satisfaction Semantics</Title>
            <Text>The foundation for the query notation is the notation for conditions. Conditions are boolean valued — but they are not always expressions. For example, the first condition for grandparent is that there is a parent; this was expressed using the condition:
(X,Z) in parent
This condition is not evaluated in the way that expressions are normally evaluated — by testing to see if a given pair of X and Z are in the parent collection. Instead, the condition is evaluated by trying to find X and Z that are in the collection. In effect, the condition becomes a search for suitable candidate pairs.
Technically this is called satisfying the condition — to distinguish what is going on from evaluating the condition. Of course, satisfying and evaluating are close cousins of each other and amount to the same thing when there is no search involved.
In addition to individual search conditions like this, it is also possible to use logical operators — called connectives — to combine conditions. In the case of our grand-parent query, there is a conjunction; which involves a variable Z that acts as a kind of glue to the two search conditions.
In database parlance the conjunction amounts to an inner join operation; however, it is also simply logical conjunction.
The available connectives include the usual favorites: &amp;&amp;, ||, and \+. They also include some less familiar connectives: *&gt; (read as implies) and otherwise.
The implies connective is a way of testing complete compliance with a condition; for example, we can define a query capturing the situation that a manager earns more than his/her members by requiring that anyone who works for the manager earns less than they do:
managerOk:(string)=&gt;boolean.
managerOk(M) =&gt; (X,M) in worksFor *&gt; X.salary=&lt;M.salary.
Notice that we can use conditions’ satisfaction-oriented semantics outside of query expressions.</Text>
        </Document>
        <Document ID="91CB4723-CD35-4EAA-B92C-835D5AEA6951">
            <Title>Actors</Title>
            <Text>So far we have discussed what talkative agents say to each other but not on how they are built. The simplest structure that responds to this is the actor. Actors are lightweight entities that can respond to speech actions. For example, this actor models some aspects of a bank:
agentBank is actor{
  private var accts := dictionary of []
  fun balance(N) where accts[N] matches Ac is Ac.balance()
  prc transfer(F,T,Amt) do{
    def Fr is accts[F]
    def To is accts[T]
    Fr.debit(Amt)
    To.credit(Amt)
  }
}
The public elements of the actor determine the kinds of speech actions it can respond to. This actor can respond to a balance query:
query agentBank with balance("fred")
and can also respond to a request to transfer some money between two of its accounts:
request agentBank to transfer("fred","tom",100.0)
If we wanted our bank to be able to respond to events, such as check posting events then we need to add an occurrence handler for them. Occurrence handlers take the form of an on...do rule, such as:
on deposit(Nm,Amnt) on cashier do
  accts[Nm].debit(Amnt)
There are three parts to an on...do rule: the pattern that denotes the kind of events this rule will respond to, a channel identifier and an action that is performed when a suitable event is received. The complete rule is effectively a program that has type: occurrence of type; or in the case of this rule:
cashier:occurrence of moneyTransaction
assuming that deposit was a constructor in the type:
type moneyTransaction is
  deposit(string,float) or
  withdraw(string,float)
Multiple rules for the same channel may be present, and if two or more rules fire for a given occurrence then they all will be executed – although the relative order of performing the rules is not defined. This multiple rule firing is useful at times; for example it makes it easier to implement over-arching processing as well as specific processing:
on Tx on cashier do
  logMsg(info,"Transaction $Tx")
on deposit(Nm,Amnt) on cashier do
  accts[Nm].debit(Amnt)
In this case two actions will take place when a deposit is received: it will be logged and the appropriate account will be debited.
If two occurrence rules fire for a given notify the order of their firing is not defined: it may be either order. Therefore, you should make sure that occurrence rules that overlap should not overlap in their actions.
Actors have a type of the form actor of recordType; where recordType is the actor’s API. For example, if we include our occurrence processing rules in our agentBank; then its type will be:
agentBank:actor of {
  balance:(string)=&gt;float
  transfer:(string,string)=&gt;()
  cashier:occurrence of moneyTransaction
}
One common technique when programming with actors is to use functions that generate actors. One is likely to need only a single bank actor in a system, but one may well need multiple client actors.</Text>
        </Document>
        <Document ID="048F7A58-7D73-4F12-A40B-2C15AA5D4328">
            <Title>Code Products</Title>
            <Text>One important question that must be answered in any scheme that permits importing is “where is the code coming from?”. Star has three architectural elements that are the basis of code management: the code repository, a system of universal resource identifiers (URI) to identify packages uniquely, and _catalog_s to reduce the bureaucratic burden.
The issues that show up when managing resources tend to fall in the ‘annoyingly complex’ rather than ‘rocket science’ category. However, that does not make them less important, and addressing them certainly helps with that oceanic problem.</Text>
        </Document>
        <Document ID="DDFC701A-4AE4-4B57-84E4-C31E10EBA7C1">
            <Title>Importing Packages</Title>
            <Text>There are two basic ways of importing a packageYou cannot import a worksheet. into your code: the open import and the named import. The open import is the simplest; to import a package you just import it (sic):
worksheet{
  import myPackage.

  -- Use definitions from myPackage
}
You can also import packages outside the worksheet structure:
import myPackage.
worksheet{
  -- Use definitions from myPackage
}
Any definition that is contained in myPackage is available throughout the worksheet (or other package if you are building a package).
Like other forms of definition, the import statement may appear anywhere at the top-level of the importing worksheet or package. However, it is normally at the beginning of the package.
The second way of importing a package is to use the named import. As suggested, a named import associates a local identifier with the import:
worksheet{
  import myPackage as mP.

  -- Use definitions from myPackage via mP
}
For example, suppose that myPackage looked like:
myPackage{
  public all t ~~ foo[t] ::= foo(t) | bar.

  public unFoo:all t ~~ (foo[t]) =&gt; t.
  unFoo(foo(X)) =&gt; X.
}
To use the unFoo function in our second worksheet, we simply reference it as a field in the mP variable:
worksheet{
  import myPackage as mP.

  getTheStuff:all t ~~ (mP.foo[t])=&gt;t.
  getTheStuff(F) is mP.unFoo(F).
}
What may be a little surprising is that this applies to the foo type also, and also to the foo and bar constructors:
worksheet{
  import myPackage as mP.

  wrapF:all t ~~ (option[t])=&gt;foo[t].
  wrapF(none) =&gt; mP.bar.
  wrapF(some(X)) =&gt; mP.foo(X).
}
One of the benefits of the named import is that it makes it possible to import packages even when there are potential clashes amongst the packages being imported and/or definitions in the importing package itself.
By using named imports the effect is to establish a local namespace for the imported package. Different definitions imported from different places can be reliably distinguished using the normal record field access syntax (i.e., a period).</Text>
        </Document>
        <Document ID="5EB99001-4959-47B8-BAD9-0B336E57A86B">
            <Title>Meta Variables</Title>
            <Text>In addition to the basic quote notation for expressions, it also has support for meta-variables. Meta variables are variables embedded in quoted terms — they are variables of the quoted term, not variables in the object language. They are very useful in programs that process quoted expressions — both as expressions and as patterns.
For example, to denote a triple pattern one may construct the quoted pattern:
&lt;| ?S ! ?P $ ?O |&gt;
where S, P and O are meta-variables that would be bound to the subject, predicate and object of the triple respectively. This quoted pattern is equivalent to the normally written term:
applyAst(nameAst("n3Triple"),tupleAst(list of [S,P,O]))
Note that the type of meta-variables is always quoted.
If we wanted to construct a quoted triple using the quoted notation we might have the following sequence:
var S is &lt;| :john |&gt;
var P is &lt;| :name |&gt;
var O is &lt;| "John Smith" |&gt;
var T is &lt;| ?S ! ?P $ ?O |&gt;
As a result of this, the variable T would be bound to the equivalent of the term:
var T is &lt;| :john ! :name $ "John Smith" |&gt;
The quoted notation is sufficiently powerful to almost never require explicit use of the constructors that actually make of the quoted type definition. This is reinforced by the implementation of the coercion contract between standard types and quoted.</Text>
        </Document>
        <Document ID="77D82D77-D581-4179-8169-6968B0D46347">
            <Title>Operator Precedence Grammar</Title>
            <Text>Star has a very flexible syntactic foundation based on Operator Precedence Grammars. You are already quite familiar with operator precedence grammars — they are used in nearly every programming language to represent arithmetic expressions. For example
X+Y*3
is a very common way of representing the addition of X to the result of multiplying Y by 3. It also represents the application of two operators: + and * which happen to be binary operators.
The operator structure of an expression is completely independent of any type information or its run-time performance.
We should be careful to note that the term operator here has at least two overlaid meanings: there is the sense in which operator is a syntactic structure — with rules for legal sequences of tokens — and there is the sense in which operator is a function to be applied to arguments — with rules for type safety and information flow. In this section, we are focused on the syntactic aspects of operators.
Star makes quite extensive use of operators in its own grammar, nearly every feature of the language relies on operators for its syntax.
There are many different kinds of operator: we can have prefix operators like unary -; infix operators like * and postfix operators like ;. Although prefix and infix forms tend to be much more commonly used than the postfix form. It is quite possible for the same operator to have multiple forms: for example the - is both infix and prefix — which allows us to use the same symbol for subtraction and to represent negative numbers.
To support parsing and operator combination, an operator is associated with a priority number — which encodes the relationship between operators. In the case of Star’s operators, this is an integer in the range 0 to 2000; where the higher the priority the more dominant the operator is in the syntax. For example, the priorities for + and * are 720 and 700 respectively. It is this relative difference that determines that X+Y*3 means the same as X+(Y*3) and not (X+Y)*3.
The final attribute of an operator is its associativity. Associativity determines what happens when you have multiple operators of the same priority in sequence. For example, arithmetic operators are traditionally left associative. This means that
X-Y-Z
is the same as
(X-Y)-Z
rather than
X-(Y-Z)
All this can be put together in a single Star statement: the operator declaration. In the case of our triple we are using two operators ! and $ to ‘glue’ together the parts of a triple. We also use the : operator to mark named concepts. The operator declarations we need to represent them are:
 # right("!",500).
 # right("$",450).
 # prefix(":",100).
 # infix("says",908).
One may ask where these priority numbers come from? Operators in Star are stratified into different levels depending on their syntactic role: 0–899 represent expressions, 900–999 represent predicates and conditions, 1000–1199 are used for forms that can be either expressions or actions, 1200–1499 represent actions and 1500–200 represent statements. The choice of 908 for says is to make it the same as the built-in predicate operator: in. The main effect of choosing the ‘wrong’ value for the priority of an operator is that expressions don’t parse the way you would like.
Notice that the word says is also an operator! Star allows us to use words as operators as well as using ‘graphical’ symbols like +.
We chose to use a word-style operator here — says — to introduce the triple condition; yet we chose graphical operators for the triples themselves. One of the ways that taste shows up in designing DSLs is in areas like this: what names do we use.
As it happens, part of the texture of Star is to use keywords frequently. There are graphical operators of course, but one of the hallmarks of Star is its liberal use of keywords. While this undoubtably makes for more typing it also makes for a more readable language.
Picking the right priority for operators is one of the most subtle aspects of designing syntactic extensions to Star. The language definition has a table of all the standard operators and their priorities.</Text>
        </Document>
        <Document ID="D52D46B3-BB78-479F-82E7-542FDD9A0A70">
            <Title>Going Even Further</Title>
            <Text>We have focused so far on generalizing the visitor from the perspective of the tree type. But there is another sense in which we are still architecturally entangled: from the perspective of the check and count functions themselves.
In short, they are both tied to our tree type. However, there are many possible collection data types; Star for instance has some 5 or 6 different standard collection types. We would prefer not to have to re-implement the check and count functions for each type.
The good news is that, using contracts, we can write a single definition of check and count that will work for a range of collection types.
Let us start by defining a contract that encapsulates what it means to visit a collection:
contract visitor[c-&gt;&gt;t] ::= {
  visit:all a ~~ (c,(a,t)=&gt;a,a)=&gt;a
}
This visitor contract defines a single function that embodies what it means to visit a collection structure. There are quite a few pieces here, and it is worth examining them carefully.
A contract header has a template that defines a form of contract constraint. The clause
visitor[c -&gt;&gt; t]
is such a constraint. The sub-clause
        c -&gt;&gt; t
refers to two types: c and t. The presence of the -&gt;&gt; term identifies the fact that t depends on c.
The visitor contract itself is about the collection type c. But, within the contract, we need to refer to both the collection type and to the type of elements in the collection: the visit function is over the collection, it applies a function to elements of the collection.
Furthermore, as we design the contract, we do not know the exact relationship between the collection type and the element type. For example, the collection type may be generic in one argument type — in which case the element type is likely that argument type; conversely, if the type is not generic (like string say), then we have no direct handle on the element type.
We do know that within the contract the element type is functionally determined by the collection type: if you know the collection type then you should be able to figure out the element type.
We express this dependency relationship with the the c -&gt;&gt; t form: whatever type c is, t must be based on it.
The body of the contract contains a single type annotation:
visit:all a ~~(c,(a,t)=&gt;a,a)=&gt;a
This type annotation has three type variables: the types c and t come from the contract header and a is local to the signature. What the signature means is
Given the visitor contract, the visit function is from the collection type c, a function argument and an initial state and returns a new accumulation state.
It is worth comparing the type of visit with the type of tVisit:
tVisit:all t,a ~~(tree[t],(a,t)=&gt;a,a)=&gt;a
The most significant difference here is that in tVisit the type of the first argument is fixed to tree[t] whereas in visit it is left simply as c (our collection type).
Given this contract, we can re-implement our two check and count functions even more succinctly:
check(T,S) =&gt; visit(T, (A,X)=&gt;A || X==S,false)
count(T) =&gt; visit(T, (A,X)=&gt;A+1,0)
These functions will apply to any type that satisfies — or implements — the visitor contract. This is made visible in the revised type signature for count:
count:all c,t ~~ visitor[c-&gt;&gt;t] |: (c)=&gt;integer
This type is an example of a constrained type. It is generic in c and t but that generality is constrained by the requirement that the visitor contract is appropriately implemented. The eagled-eyed reader will notice that count does not actually depend on the type of the elements in the collection: this is what we should expect since count does not actually care about the elements themselves.
The type signature for check, however, does care about the types of the elements:
check:all c,t ~~
    visitor[c-&gt;&gt;t], equality[c] |: (c,t)=&gt;boolean
This type annotation now has two contract constraints associated with it: the collection must be something that is visitable and the elements of the collection must support equality.
Given the work we have done, we can implement the visitor contract for our tree[t] type quite straightforwardly:
implementation visitor[tree[t]-&gt;&gt;t] =&gt; {
  visit = tVisit
}
Notice that header of the implementation statement provides the connection between the collection type (which is tree[t]) with the element type (t). The clause
visitor[tree[t]-&gt;&gt;t]
is effectively a declaration of that connection.
Now that we have disconnected visit from tree types, we can extend our program by implementing it for other types. In particular, we could also implement the visitor for the sTree type:
implementation visitor[Tree -&gt;&gt; string] =&gt; {
  visit = sVisit
}
however, we leave the definition of sVisit as a simple exercise for the reader.
Our final versions of count and check are now quite general: they rely on a generic implementation of the visit function to hide the recursion and are effectively independent of the actual collection types involved.
If we take a second look at our visitor contract we can see something quite remarkable: it counts as a definition of the famous visitor pattern. This is remarkable because although visitor patterns are a common design pattern in OO languages, it is often hard in those languages to be crisp about them; in fact, they are called patterns because they represent patterns of use which may be encoded in Java (say) whilst not necessarily being definable in them.
The combination of contract and implementation represents a quite formal way of defining patterns like the visitor pattern.
There is something else here that is quite important too: we are able to define and implement the visitor contract without having to modify in any way the type definition of tree or sTree. From a software engineering point of view this is quite important: we are able to gain all the benefits of interfaces without needing to entangle them with our types. This becomes critical in situations where we are not able to modify types — because they don’t belong to us and/or we don’t have access to the source.</Text>
        </Document>
        <Document ID="FC786A6F-01F7-4C9F-8FB1-A9DFB8FCFA20">
            <Title>Channels of Communication</Title>
            <Text>In many cases the requirement for synchronization between tasks is based on needing reliable communication between them. Instead of building communication on top of a model of ‘shared resources’, we use direct ‘message passing’: one task sends a message to another. The mechanism we use is called the channel.
Channel
A channel is a means by which two (or more) tasks can pass data in a type-safe way between themselves.
Channels are synchronous: in order for data to be passed between tasks they must both be paused. There are rendezvous associated with receiving messages from a channel and for placing messages on a channel — actual communication only occurs when both the sender and the receiver have waited for the event to occur.
One might ask ‘why is communication synchronous?’ Why not asynchronous? The basic answer is that synchronous communication seems to be more basic than asynchronous communication.
It is straightforward to implement a more asynchronous communication pattern based on synchronous primitives. It is harder to do the converse: to implement synchronous communication using asynchronous primitives.
However, it is also true that synchronous communication is essentially impossible when that communication involves multiple computers. The concurrency features in Star support multiple threads of activity within a single application but do not directly support networked applications. We believe that, for other reasons, such networked applications are better served with different features — features that directly address issues involving agenthood. We shall see some of these in our treatment of [Concurrent Actors][concurrent-actors].
The channel function creates such a synchronous channel:
channel:for all t ~~ () =&gt; channel[t]
There are two rendezvous operators for sending and for receiving data on a channel:
sendRv:for all t ~~
  (channel[t], t) =&gt; rendezvous of ()
recvRv:for all t ~~
  (channel[t]) =&gt; rendezvous of t
Notice that the sendRV operator returns a void-valued rendezvous; whereas the recvRv operator returns a rendezvous with a value.
Type safety is ensured by the type carried by the channel value: the sender and receiver of a channel must also agree on the type of the data being communicated. Channels have no fixed concept of direction. A task can send a message over a channel in one step, and receive a message over the same channel in the next.
Here is a simple scenario involving a transmission over a channel:
def ch is channel()
ignore background task {
  def msg is valof (wait for recRv(ch))
}
ignore background task {
  perform wait for sendRv(ch,15)
}
The communication occurs when both backgrounded tasks reach their appropriate rendezvous: one is waiting for the message to arrive and the other waiting to be able to send it.
In our message passing sequence, we actually do not want the task arguments to complete in a sequential order — so ignoreing (sic) the task is the appropriate command.</Text>
        </Document>
        <Document ID="16A36ADD-AD2E-4196-9F9E-A8C5E538F412">
            <Title>A Meeting of Philosophers</Title>
            <Text>We can model a philosopher as a task function that iteratively acquires a left and right fork, ‘eats’ for a random period, and then stops eating after relinquishing its two forks (so another philosopher can eat).
The classic Hoare solution to the deadlock problem is to invoke an additional element: a central ‘table’. The role of the table is to ensure that only one philosopher at a time is requesting forks; this, in turn, prevents a deadlock situation where two or more philosophers can start the process of acquiring their forks but are unable to complete because the another philosopher ‘has’ the other fork.
The table can be modeled straightforwardly as a semaphore with a limit of 1: only one philosopher can be starting to eat at any given time. Note that this does not mean that only one philosopher is eating at any one time: as soon as a philosopher has gotten two forks, another philosopher can start the process. Of course, given that there are four forks, only two philosophers can actually be eating simultaneously.
The main cycle for the philosopher task then looks like:
sleep(random(10L))  -- chat some
perform T.grab()    -- get permission first
perform L.grab()    -- get left fork
perform R.grab()    -- get right fork
perform T.release() -- release the table

sleep(random(15L))  -- eat some

perform L.release() -- let go of left fork
perform R.release() -- let go of right fork
The sleep calls in this sequence stand for some arbitrary activity — such as chatting or eating — that does not involve forks and/or the central table.
The complete phil function returns a task that performs this cycle a fixed number of times. The program below shows the phil function that takes the semaphores for the left and right forks as argument, together with an identifying philosopher number for debugging purposes.
fun phil(n,L,R) is task{
  for Ix in range(0,Count,1) do{
    sleep(random(10L))  -- chat some
    perform T.grab()    -- get permission first
    perform L.grab()    -- get left fork
    perform R.grab()    -- get right fork
    perform T.release() -- release the table
    sleep(random(15L))  -- eat some
    perform L.release() -- let go of left fork
    perform R.release() -- let go of right fork
  }
}
The eagled-eyed reader will notice that while L and R are parameters of the phil function, the T variable is not. Indeed, it is a free variable that must be bound in an outer scope.
The complete setup for the dining philosophers involves creating separate semaphores for each of the forks and for the table, spinning off background tasks for the philosophers, and then forcing them to run with a sequence of perform actions is given in:
prc dining(Count) do let{
  def T is semaphore(1)  -- The table

  fun phil(n,L,R) is task{ ... }
} in {
  def fork1 is semaphore(1)
  def fork2 is semaphore(1)
  def fork3 is semaphore(1)
  def fork4 is semaphore(1)

  def phil1 is background phil(1,fork1,fork2)
  def phil2 is background phil(2,fork2,fork3)
  def phil3 is background phil(3,fork3,fork4)
  def phil4 is background phil(4,fork4,fork1)

  perform phil1
  perform phil2
  perform phil3
  perform phil4
}
This program will run the simulation for four philosophers for a Count number of times. We have not added any trace information, so there will not actually be any output; however, that is relatively straightforward to do.
The dining philosophers is a toy example; nevertheless it highlights many of the issues found in regular parallel applications where there is some shared resource.</Text>
        </Document>
        <Document ID="B3B0B724-F387-41BC-9806-84D42A91B9E3">
            <Title>Philosophers Have to Eat Too</Title>
            <Text>The analog of the ‘hello world’ example in concurrent programming is probably the in-famous dining philosophers problem. Made famous in this form by C.A.R. Hoare (also of QuickSort fame), the set up for this puzzle is quite straightforward: there are four philosophers sitting at a table wanting to eat. The catch being that there are only four forks on the table and a philosopher needs to use two forks to eat something from the table.
#
Four Dining Philosophers
Of course, it is only a little eccentric that the philosophers eat with two forks rather than a knife and fork!
Being philosophers, they don’t eat constantly: they spend some time talking too; which means that it is possible for them to share the forks. Each philosopher has access to the two forks nearest him; each fork is shared by two philosophers; but a given fork can not be in use by more than one philosopher simultaneously.
This problem is one of resource contention and the underlying issue is to avoid the different kinds of contention issues — such as deadlock (where everyone is stuck and no progress can be made), livelock (where no effective progress is made) and starvation where one or more of the philosophers receives an unfair allocation of the forks — preventing others from using the forks.
There are two well-known solutions to the dining philosophers problem; one is due to Hoare and the other is due to Mani Chandy. We will show how to realize Hoare’s solution as it introduces some additional features of our concurrency platform.
In essence, the Hoare solution depends on a combination of semaphores to manage the status of the various forks and an additional arbitrator to ensure deadlock-free execution.</Text>
        </Document>
        <Document ID="797CBF11-BBA9-4ABB-B871-E1AD5D67CAFD">
            <Title>Using Channels and Rendezvous to Implement Semaphores</Title>
            <Text>A semaphore is a general structure for implementing mutual exclusive access to some resource.
Semaphore
A semaphore is a variable, with associated grab and release operations, that can be used to manage access to a shared resource by multiple concurrent activities.
It is traditional in designing semaphores to allow an arbitrary — but controlled — number of clients access to the resource. A non-recursive lock is equivalent to a semaphore with a count of 1.
Given a semaphore, it is used in a sequence that looks like:
S.grab()
... -- access the shared resource
S.release()
The idea is, of course, that if the resource is not currently available then activities that attempt to grab the semaphore will be blocked. When the resource becomes available (by someone else performing the semaphore’s release action) then the grab may be re-attempted.
Semaphores are useful, but they are prone to mis-programming. Use of a semaphore relies on the programmer ensuring that the release is invoked after a grab and, furthermore, any connection between the semaphore and the governed resource is purely implicit. Both of these are common sources of errors in concurrent programs.
To build a semaphore we have to arrange for a unique resourceNot to be confused with the resource that is governed by the semaphore. together with coordinated access to it; preferably without exposing the resource itself (to prevent it from being contaminated by external code).
One way of implementing such a unique resource is to use an internal background task and to map calls to grab and release to communication to the task. This internal task only needs a simple structure: it is simply listening for messages either to grab or to release. The crux is that, at any one time, either a grab or a release message might arrive and we cannot control this ahead of time. In effect, we have to be able to listen to more than one potential message, and the choice rendezvous allows us to do this.
The main loop of the semaphore’s internal task is modeled using a mutual recursion between the grabR function, releaseR function and the semLoop functions. The forms of grabR and releaseR are actually very similar:
grabR(X) =&gt;
  wrapRv(recvRv(grabCh), (_) =&gt; semLoop(X-1).
releaseR(X) =&gt;
  wrapRv(recvRv(releaseCh), (_) =&gt; semLoop(X+1)).
The wrapRv operator is a function that takes a rendezvous and a function as argument; it returns a new ‘wrapped’ rendezvous as its result. If the argument rendezvous fires then the result of that rendezvous is passed to the function argument — which in turn becomes the value returned by the wrapRv rendezvous.
In the case of grabR, the effect is that a message on the grabCh channel is passed to the lambda; which ignores the actual message but recursively invokes the semaphore loop function — with an decremented ‘resource counter’. The releaseR function is similar except that semLoop will be invoked with an incremented counter and that it is listening to the releaseCh channel rather than the grabR channel.
The wrapRv function is a useful way to ‘do something’ during a rendezvous. It is also the best way of converting a rendezvous of one form into that of another — to allow, for example, a combination rendezvous.
There is an analogous operator — guardRv that has the effect of enabling a rendezvous only if some condition is met.
In both cases, the result is a new invocation of semLoop which — depending on whether the counter is zero or not — will either listen for a release or both a releaseR or grabR:
fun semLoop(0) is wait for releaseR(0)
 |  semLoop(X) default is wait for grabR(X) or releaseR(X)
In implementations of Star that do not support tail recursion, this code would need to be rewritten as a loop. However, we leave it as a recursion for expository purposes.
If we focus on the disjunction in:
wait for grabR(X) or releaseR(X)
This is a rendezvous that is composed of two rendezvous; and the wait for operator waits for either one to occur. Disjunctive rendezvous like this are critical for many concurrent applications.
It is important to emphasize that the disjunctive wait for waits for exactly one rendezvous to occur. In the situation that both happen simultaneously then only one will be picked for this choice. The unchosen rendezvous may be ‘picked up’ by another wait for operation.
Notice that the disjunctive rendezvous is only entered when the value passed to semLoop is non-zero (it will actually be positive). If the value was zero then the task will only wait for a releaseR rendezvous. This captures the essential requirement that a rendezvous will limit access to its associated resource to a fix number of clients.
We are now in a position to show the complete workings of the semaphore function in:
semaphore:(integer) =&gt; { grab:()=&gt;rendevous[()], release:()=&gt;rendezvous[()]. }.
semaphore(Count) =&gt; let{
  grabCh:channel[()] = channel().
  releaseCh:channel[()] = channel().

  releaseR:(integer)=&gt;rendevous[()].
  releaseR(X) =&gt;
    wrapRv(recvRv(releaseCh), (_) =&gt; semLoop(X+1)).

  grabR(X) =&gt;
    wrapRv(recvRv(grabCh), (_) =&gt; semLoop(X-1).

  semLoop:(integer) =&gt; rendevous[()].
  semLoop(0) =&gt; wait for releaseR(0).
  semLoop(X) default =&gt; wait for grabR(X) || releaseR(X)

  { ignore background semLoop(Count) }
} in {
  grab() =&gt; wait for send(grabCh,()).
  release() =&gt; wait for send(releaseCh,()).
}
Notice that the value returned by the semaphore function is itself a record — with the two functions grab and release embedded in it. Thus an expression of the form:
S.grab()
is an invocation of the grab function from that record — and denotes an attempt to ‘grab’ the resource managed by the semaphore. Similarly, S.release() counts as releasing the resource.
Because the inner workings of the semaphore are protected by a let expression, they cannot be seen externally by other programs — only the grab and release functions are exposed; hence meeting one of the key requirements of the semaphore implementation.</Text>
        </Document>
        <Document ID="7C2E06C3-26F6-4628-8DF7-57DFBCF60DAF">
            <Title>Wiring up Boxes and Arrows</Title>
            <Text>We started this section with a graphical depiction of an application as boxes and arrows between them. However, not many computers can execute boxes, and so if we want to run the application we have to construct a complete written program that represents the drawn diagram.
The written form of our car part sourcing application is not that hard to follow, given the material we have covered so far:
import boxesNarrows
partSource is application{
  def ordersIn is import ordersInComponent
  def split is import splitComponent
  def db is import dbComponent
  import supplier

  ordersIn.out connect to split.incoming
  split.parts connect to db.queryIn
  split.suppliers publish to {
    def megaWheel is supplier("MegaWheel")
    ...
    def discriminator("Mega") is megaWheel
    ...
  }
}
This sketch shows how we can construct a written version of the boxes-and-arrows diagram in a way that lends itself to executable code.</Text>
        </Document>
        <Document ID="A9055870-84C7-484C-B8C3-C2EE68BB1CC6">
            <Title>Worksheet</Title>
            <Text>We have already seen a number of examples of using worksheets. They represent a modern replacement for the traditional REPL — or Read-Eval-Print-Loop. The main advantages of worksheets over the traditional REPL are that they simultaneously act as their own transcript and they integrate well with editor-centric IDEs.
We prefer worksheets to normal packages for the same situations as one might prefer a REPL over a traditional file: worksheets make it easy to set up simple experiments and ‘see what happens’ when you try something.
The contents of a worksheet may be anything that counts as a definition — including import statements. In addition, there are some special ‘actions’ that may be specified; some of which we have already seen:
perform
perform an action. For example:
worksheet{
  perform task {
    logMsg(info,"This is an action")
  }
}
perform is used to perform a task expression.
ignore
Evaluate an expression for side-effect purposes; ignoring its value.
This is used in those cases where a computation has a value that we wish to ignore – i.e., is evaluated for its effect only.
In many programming languages expressions are automatically ‘promoted’ to statements. For example, if you have the expression 3+X in Java (say) then it is also a statement:
while(true){
  3+X;
}
Of course, in this case, the expression 3+X doesn’t do anything; but the feature is commonly used to allow a function call in a situation where the returned value is thrown away or ‘ignored’.
Star does not have an automatic way of promoting expressions to actions; but you can use the ignore keyword for the same effect. The action
ignore 3+X
is the way that you would write 3+X as an action.
show
Evaluate an expression and show its value.
For example:
worksheet{ show 1+2 }
show is probably the most used special action in a worksheet. After all, the purpose of a worksheet is to help figure out what is going on…
**assert
assert a condition.
For example:
worksheet{ assert 3&gt;2 }
{ … }
Perform a block of actions, enclosed in parentheses.
For example:
worksheet{
  {
    for Ix in range(0,10,1) do{
      logMsg(info,"hello $Ix")
  }
}
In many cases, the actions that are used most often are the show and the assert actions; in conjunction with a set of regular definitions. This reflects one of the main purposes of the worksheet: to give some insight into the behavior of a program.
Unlike a traditional REPL, because the worksheet is in a file, there is much more freedom within a worksheet to order things. In particular the definitions can be in any order: a later definition may be referenced explicitly or implicitly in a show action.
It is possible to include actions in a regular package. However, especially in cases where the package is to be imported into another program this practice is not recommended.</Text>
        </Document>
        <Document ID="50F5AF1C-FAFF-4D46-B605-F99E77DA7482">
            <Title>Opening Up</Title>
            <Text>One of the reasons that we are so interested in establishing a ‘normal’ semantics for modules and ADTs is that we can develop systems where the contents of a module depends on some additional computation; i.e., we can use functions over modules. For example, we can show that aspect oriented programming and dependency injection can be realized just using normal code structuring with functions and let environments.
Techniques like dependency injection are typically applied to large programs; unfortunately that makes constructing small examples a little forced. So, we’ll use a crow-bar to open a soda bottle. Imagine, if you will, that we needed to define a new arithmetic type that supported arbitrary fractions.
Floating point numbers are fractions. But they do not permit the representation of all fractions — e.g., it is not possible to represent 1/3 exactly in IEEE 754.
However, while we want to expose the type, and a set of operator functions, we definitely do not want to expose anything about the implementation of fractional numbers: as far as users are to be concerned, the type fraction is to be completely opaque and might be implemented in any way.
Let us start with an interface; which in this case will take the form of a record type:
fractionals ::= exists fraction ~~ fracts{
  type fraction.
  frPlus:(fraction,fraction)=&gt;fraction.
  frToString:(fraction)=&gt;string.
  frParse:(string)=&gt;fraction.
  fraction:(integer,integer)=&gt;fraction
}
One of the first things to note here is that fraction is existentially quantified; secondly we need to ensure that the set of operators we expose is complete. Our interface is not really complete, but includes two critical operators: a means of constructing fractions – via the fractions and frParse functions – and a means of escaping from the world of fractions to other types (in this case string via frToString).
Here we are mostly concerned with using fractions, so we will assume that we have at least one implementation — courtesy of the FR variable:
FR:fractionals
One way to use our implementation of fractions would be to reference the needed operators via the FR variable:
F0 = FR.frParse("3/4")
F1 = FR.fraction(1,2)
F2 = FR.frPlus(F0,F1)
show FR.frToString(F2)   -- results in 5/4
However, we can do rather better than this in Star. We have already encountered the import statement; there is an analogous statement that allows us to unwrap a record like FR in a binding environment — such as:
let{
  open FR
  F0 = frParse("3/4")
  F1 = fraction(1,2)
  F2 = frPlus(F0,F1)
} do
  show frToString(F2)   -- results in 5/4
The open statement has a similar effect to the package import: it enables the functions, types and other elements that are embedded in a record to be made available as normal identifiers within the normal scope of the let action (or expression).
Of course, this code is still fairly clumsy; since we would like to use normal arithmetic notation over fractions; which we can do by implementing the arith contract:
let{
  open FR
  implementation arith[fraction] =&gt; {
    X+Y =&gt; frPlus(X,Y)
   ... -- more operators needed
  }
} do {
   F0 = frParse("3/4")
   F1 = fraction(1,2)
   F2 = F0+F1
   show frString(F2)
}
We can improve this further by also implementing the coercion contract between strings and fractions:
let{
  open FR
  implementation arith[fraction] =&gt; {
    X+Y =&gt; frPlus(X,Y)
   ... -- more operators needed
  }
  implementation coercion[string,fraction] =&gt; {
    coerce(S) =&gt; frParse(S)
  }
  implementation coercion[fraction,string] =&gt; {
    coerce(F) =&gt; frToString(F)
  }
}
This allows us to use a more natural notation for expressions involving our fractions:
let{
  open FR
  ...
} do {
   F0 = "3/4" :: fraction
   F1 = fraction(1,2)
   show F0+F1
}
While much better than our original, we still have too much code to write to use the fraction type: we have to get the type and then demonstrate the appropriate implementations. We want to be able to combine everything that is important about fractions into a single structure.
There is a straightforward way we can do this. Our original signature for fractionals simply required the presence of the fraction type. What we can do is further require that the arith and appropriate coercion contracts are also implemented; we do this by constraining the type definition for fractionals:
 fractionals ::= exists fraction ~~ arith[fraction], coercion[string,fraction], coercion[fraction,string] |: fracts{
  type fraction.
  frPlus:(fraction,fraction)=&gt;fraction.
  fraction:(integer,integer)=&gt;fraction.
}
Since we are using contracts we do not need the explicit frParse and frToString functions in the signature any more.
When we instantiate a fracts record we must provide — within the record itself — appropriate implementations of arith and coercion:
FX = fracts{
  fraction &lt;~ myFraction.
  implementation arith[myFraction] =&gt; {
    X+Y =&gt; frPlus(X,Y).
   ... — more operators needed
  }
  implementation coercion[string,myFraction] =&gt; {
    coerce(S) =&gt; frParse(S)
  }
  implementation coercion[myFraction,string] =&gt; {
    coerce(F) =&gt; frToString(F)
  }
  ...
}
Notice that we implemented arithmetic for the internal myFraction type. We could have equally implemented the contract for fraction type too; the key requirement is to provide evidence that arithmetic is implemented for the type.
The FX record now has everything we want to expose about fractional numbers. If we open the structure then indeed we can write programs like:
let{
  open FX
} do {
   F0 = "3/4" :: fraction
   F1 = fraction(1,2)
   show F0+F1
}
This is virtually equivalent to the code we might have written if we were importing a package with the definition of the fraction type in it. The difference is that we have access to the full expressive power of the language in computing FX.</Text>
            <Comments>Assuming that we added the missing operators that we would actually need.</Comments>
        </Document>
        <Document ID="439D5074-37F6-44F2-9358-2E379B7BD238">
            <Title>There is More</Title>
            <Text>As we have noted, Star is a rich language and it would be impossible to try to cover it in a short introduction. Later chapters will look at some of the other features such as a deeper look at contracts, queries, actors, concurrency, existential types, and extending Star with domain specific languages. Chapter [2][functional-programming] starts this process by looking at functional programming in Star.</Text>
        </Document>
        <Document ID="A15F7AA4-5D6D-49F9-A22C-813E63D84054">
            <Title>Application = Policy + Mechanism</Title>
            <Text>It should be clear at this point that Star is quite a rich language. There are many features and sub-languages that form the whole. We have seen several of these, including quite complex features like queries. In Chapter  we will also see a sophisticated sub-language based on actors and speech actions. It may surprise you to learn that the core of Star is quite small, with many features implemented in Star itself. Our task now is to introduce you to some of the techniques available to build your own extensions.
One of the most fundamental and surprisingly difficult issues in language design is whether to allow ‘regular’ programmers to extend it. Different language communities have taken radically different stances to this: languages like C/C++ and the LISP family were designed to be extensible by the programmer. On the other hand, the Java designers explicitly chose not to include any form of macro feature — their rationale being that macros make it easy to confuse programmers with strange syntax features.
However, no language that is in active use is fixed. Simply using a programming language is often enough to point to ways in which it might be improved; and language stewards are understandably interested in ensuring the continued relevance of the language in their care by augmenting it with new features.
In the end, it reduces to a question of democracy on the one hand and a curated environment on the other. Language designers can choose to allow evolution to occur ‘in the wild’ or can choose to try to control it with a disciplined process.
Neither strategy is inherently better than the other. Both carry risks. A bottom-up approach risks splintering and difficulties with effective tooling. A top-down approach is often slower and more deliberate; which can mean being made irrelevant in a fast paced world. It also can put users in a difficult situation – if a potentially useful feature is missing from the language.
For better or worse, Star was designed to allow programmers to extend it without requiring changes to the basic compiler — i.e., the language has facilities to support its own evolution and, particularly, the development of Domain Specific Languages.
Although Star permits regular programmers to define extensions, you should be prepared for some difficult and complex topics. Designing and extending programming languages is a subtle art that calls for some obscure concepts at times.
On the other hand, designing for extensibility does not necessarily mean adopting macro language features like those in either C/C++ or LISP. Designing the language extensibility features needs to be done with the same care as the ‘main’ language.
Star’s has a well developed set of language elements that are designed to make it easier to develop reliable extensions to the core language. The primary features involved are:
	•	an extensible grammar — allowing new operators to be defined;
	•	a rule-based system for defining well-formed notations; and
	•	a powerful macro language system that is used to implement extensions by mapping them to core language elements.
Together, there are sufficient facilities for making a huge range of potential extensions and sub-languages.</Text>
            <Comments>With LISP arguably being more successful in that.
We believe better of course.</Comments>
        </Document>
        <Document ID="F62377AA-8944-426E-9BC9-CD265F374AA5">
            <Title>Sequence Notation</Title>
            <Text>A sequence is simply an ordered collection; a sequence expression is an expression involving a complete or partial enumeration of the values in the collection. Star has a simple notation for expressing sequences of any underlying type; for example, a cons sequence of integers from 1 through 5 can be written:
cons of [1, 2, 3, 4, 5]
In situations where we do not know or do not wish to specify the collection type, we can write instead:
[1, 2, 3, 4, 5]
This term — it could be either an expression or a pattern — denotes the sequence without specifying the underlying collection type. The difference in the types of the two terms is telling:
cons[integer]
and
sequence[c-&gt;&gt;integer] |: c
respectively — where c is a type variable. The first is a concrete type expression, the second is a constrained type —  in this case c must implement the sequence contract.
Although the second type expression is longer, and a bit more complex to read, it is also actually less constraining. The type expression cons[integer] does not allow for variation of the underlying collection type; the second type expression allows the term to be used in contexts that require different concrete types.</Text>
        </Document>
        <Document ID="2CB9697E-729D-4BD7-A451-B4FC296117FD">
            <Title>Notation and Contract-Based Semantics</Title>
            <Text>One of the distinctive features of the sequence notation is that it is an example of syntax that is underwritten by a semantics expressed as a contract. This is part of a widespread pattern in Star.
This has a parallel in modern OO languages like Java and C# where important contracts are expressed as interfaces rather than concrete types. However, Star extends the concept by permitting special notation as well as abstract interfaces — as many mathematicians understand, a good notation can make a hard problem easy. In Star we further separate interfaces from types by separating the type definition from any contracts that may be implemented by it.
This is part of a general pattern in Star: there are many sub-languages that are actually underwritten by contracts for their realization. For example, the indexing notation has the same pattern: of a special notation backed by contract.
The merit of this combination of special syntax and contracts is that we can have the special notation expressing a salient concept — in this case the sequence — and we can realize the notation without undue commitment in its lower-level details. In the case of sequence notation, we can have a notation of sequences without having to commit to the type of the sequence itself.</Text>
        </Document>
        <Document ID="355B1EB2-1CDD-4CBC-8DAA-A9411F53E6ED">
            <Title>A Missing Performative</Title>
            <Text>Star does not have a declare performative – currently. It may be instructive to see why not, especially since we introduced speech action theory with a classic declaration. While it is not likely that a software agent will marry couples any time soon, there are legitimate reasons for wanting the ability to make declarations.
A declaration is a speech action whose effect is embedded within the speech action itself. Declarations establish new facts that are shared by the listener and potentially others in the context. Perhaps the best example comes from transaction processing: declaring that a transaction has been committed to is the same as committing to the transaction. Similarly, declaring that a couple is married is the same as marrying them.
The real reason why there is no declare relates to it’s putative argument – which must be a proposition. For example, in signing a contract, each party says to the other:
I agree to be bound by this contract
where contract refers to the normal human interpretation, not Star’s contracts. How, we must ask, might we represent this in a way that is amenable to automated processing? To answer this, we must try to unpack what a contract is.
A legal contract has two elements: it is a statement of constraints on potential behaviors of the parties involved and it defines a value exchange (I exchange my money for your house). Very few programming languages, and essentially no ‘conventional languages’ have any way of representing concepts like value exchange and constraints on behavior.
A more general approach is to use a logical language in which we can encode contracts and the like. At the time of this writing Star does not have ready access to a well developed logical language (but see [our treatment of RDF][rdf]). As a result, there is no appropriate partner for the declare speech action.
Developing a usable logical language is fairly substantial task; however, once it exists, we have exactly the right speech action for it!</Text>
        </Document>
        <Document ID="EC0CFAD9-6006-4644-8754-6CE8CF66B7D5">
            <Title>The cons Type</Title>
            <Text>This is the simplest collection type; and is perhaps the original collection type used in functional programming languages. It is defined by the type declaration:
all t ~~ cons[t] ::= nil | cons(t,cons[t]).
Cons lists have the property that adding an element to the front of a list is a constant-time operation; similarly, splitting a cons list into its head and tail is also a constant time operation. However, almost every other operation is significantly more expensive: putting an element on to the end of a cons list is linear in the length of the list.
The main merit of the cons list is the sheer simplicity of its definition. Also, for small collections, its simple implementation outweighs the advantages that more complex collections offer.</Text>
        </Document>
        <Document ID="3249D51C-DF24-43C7-8E8C-F793196B48FC">
            <Title>Computation Expressions and the M word</Title>
            <Text>Task expressions are instances of a more general Star feature: the computation expression. Task expressions are oriented towards the concurrent execution of computations; other forms of computation expression have different purposes. In particular, the simple valof expression:
valof{
  x = 2;
  y = f(x);
  valis x+y
}
is actually a degenerate case of an action computation expression:
valof action computation {
  x = 2;
  y = f(x);
  valis x+y
}
There are several forms of computation expression in the Star library; they are used for expressing tests for example. In fact, computation expressions are syntactic sugar for monadic expressions — and play the same role in Star as the do notation plays in Haskell.
Monads represent a generalization of the kind of deferred computation we see in task expressions. In effect, monads represent a standard way of composing computations.
The computation expression is syntactic support for using the monad contract — actually called computation. We can see the extent of this support by looking at the ‘raw’ version of the action function:
AA:((integer)=&gt;integer)=&gt;integer.
AA(f) is action computation {
  x = 2
  y = valof f(x)
  valis x+y
}
The AA function becomes the somewhat more complex form:
AA:((integer)=&gt;integer)=&gt;integer.
AA(f) =&gt; _delay(() =&gt; valof{
  x = 2;
  y = _perform(f(x), raiser_fun);
  valis _encapsulate(x+y)
}).
Note how the inner valof expression is transformed to a call to _perform; whereas the result of the action computation is encapsulated in a call to the function _delay. These are all functions in the standard computation contract; and so will actually be further specialized to be specific for the action expression.
It is beyond the scope of this book to explore more deeply the handling of monadic computations. However, if you wish to explore further, a more complete description of computation expressions is given in the Star language definition.</Text>
            <Comments>This is actually a somewhat sanitized version of the raw code. This is because we have hidden a required transformation from action sequences to functional expressions that encode the computation’s ordering as function calls.</Comments>
        </Document>
        <Document ID="07B0D7C6-57CE-4D6F-8AD9-11F06A261A5F">
            <Title>Notes</Title>
        </Document>
        <Document ID="85F49EB4-E3E4-49C7-BBE5-4B6F733FD7B6">
            <Title>The main Program</Title>
            <Text>If a package defines a main procedure then that package may act as a command-line program as well as a package. There are two styles of main program possible; one where command line arguments are automatically converted into regular values and one where you get the arguments as a list[string]s.
If the main procedure is defined then automatic coercion from command line arguments to internal types is performed. For example,
myProgram{
  main:(integer,string)=&gt;().
  main(Count,Name) do {
    ...
  }
}
With this style of main, a command-line invocation of the program is possible:
$ Star myProgram 34 fred
where the run-time verifies verifies that exactly two arguments are passed, the first being an integer. In principle, the arguments of main may be of any type — so long as the type is known to implement the coercion contract:
coercion[string,T]
where T is the type being passed to the main function. I.e., so long as there is a way of parsing a string value we can pass such arguments from the command line to our main program. However, there is another constraint: the ability of the operating system to pass in arbitrary strings to a command line program.
For example, list structures are difficult to get past the typical shell:
$ Star myProgram "list of [1,2,3]"
In practice, this means that most simple types can easily be passed to a main procedure; and, with some difficulty, collection types such as list[integer] may also be used.
This capability is not often provided in programming languages. Normally, you are limited to passing in an array of strings to the top-level main function. Why this is the case is a question best asked of the respective programming language designers.
However, for program safety reasons, it is not permitted to coerce strings into functions or other forms of code (the ability to coerce a string into a function amounts to dynamic compilation).
If you want to manage all the command-line arguments, then define the procedure _main instead:
myProgram{
  _main:(list[string])=&gt;().
  _main(args) do { ... }
}
The _main procedure is given all the command line arguments as a list[string]. This variant of _main (you cannot have both in the same package) is useful if you want to process command line arguments with dash-options.</Text>
            <Comments>A procedure, in Star, is a function that returns the void tuple ().</Comments>
        </Document>
        <Document ID="93BBADB1-462B-4F6B-8087-5508B7C718B3">
            <Title>Filtering for Primes</Title>
            <Text>Let us start with modeling each filter as a concurrent task; which can be implemented using:
fun filter(P,inChannel) is let{
  def outChannel is channel()
  fun loop() is task{
    while true do {
      def I is valof (wait for recvRv(inChannel))
      if I%P!=0 then -- not a multiple, pass it on
        perform wait for sendRv(outChannel,I)
    }
  }

  { ignore background loop() }
} in outChannel
The heart of the filter function is a concurrently executing task that listens for input, checks to see if the input number is a multiple of ‘its’ prime, and if it is not passes it on.
Notice that the filter function is written in such a way that it does not directly know what the recipient of the output messages will be. This is quite common in programs of this type and is the concurrent analog of returning a value from a function. In this case the returned ‘value’ is presented as a sequence of messages on a channel.
The sharp-eyed reader will spot something else that is novel here: the let environment has a local action as well as two definitions. The form:
{ ignore Action }
within a let definition environment denotes an action that is performed as part of ‘constructing’ the let environment.
One might ask, why do we ignore the background task and not perform it? Well, the simple answer is that performing a task implies waiting for the task to finish. In fact, performing a task is equivalent to ignoring the result of taking its value:
ignore valof background loop()
will clearly not finish until the loop finishes; which it will not.
Our filter task will never terminate, indeed the entire Sieve consists of a network of tasks that do not terminate of their own accord.</Text>
        </Document>
        <Document ID="766589D8-9397-4C9F-B7A8-44D6EFC92057">
            <Title>The indexable Contract</Title>
            <Text>The indexable contract captures the essence of accessing a collection in a random-access fashion. There are functions in the contract to access a directly accessed element, to replace and to delete elements from the collection:
contract all s,k,v ~~ indexable[s-&gt;&gt;k,v] ::= {
  _index:(s,k)=&gt;option[v].
  _set_indexed:(s,k,v)=&gt;s.
  _delete_indexed:(s,k)=&gt;s.
}
There are several noteworthy points here:
	•	the form of the contract itself; the signature for _index which accesses elements; and
	•	the signatures for _set_indexed and _delete_indexed which return new collections rather than modifying in-place.
Recall that the sequence contract had the form:
contract all s, e ~~ sequence[s-&gt;&gt;e] ::= ...
the s-&gt;&gt;e clause allows the implementation of the contract to functionally determine (sic) the type of the elements of the collection.
In the case of indexable, the contract form determines two types denoted by k and v. The type k denotes the type of the key used to access the collection and v denotes the type of the elements of the collection. Each individual implementation of indexable is free to specify these types; usually in a way that best reflects the natural structure of the collection.
For example, the implementation of indexable for strings starts:
implementation indexable[string -&gt;&gt; integer,char] =&gt; ...
reflecting the fact that the natural index for strings is integer and the natural element type is char (neither being explicitly part of the string type name).  Note that char is a synonym for integer; but that is not relevant here.
On the other hand, the implementation of indexable for the concrete type dictionary starts:
implementation all k,v ~~
      indexable[dictionary[k,v] -&gt;&gt; k,v] =&gt; ...
reflecting the fact that dictionaries are naturally generic over both the key and value types.</Text>
        </Document>
        <Document ID="40224EA6-46AD-4534-9654-A94129E07813">
            <Title>A Tale of Three Loops</Title>
            <Text>Imagine that your task is to add up a list of numbers. Sounds simple enough: in most procedural or OO languages (such as Java) one would write a fragment of code that looks like:
int total = 0;
for(Integer ix:L)
  total += ix;
However, this code is also full of pitfalls. For one thing we have a lot of extra detail in this code that represents additional commitments beyond those we might be comfortable with:
	•	we have had to fix on the type of the number being totaled;
	•	we had to know about Java’s boxed v.s. unboxed types; and
	•	we had to construct an explicit loop, with the result that we sequentialized the process of adding up the numbers.
We can also write an equivalent loop in Star:
total : integer.
total = valof{
  tot : ref integer;
  tot := 0;
  for ix in L do
    tot := tot+ix;
  valis tot
}
The valof/valis combination is a neat way of segueing from the ‘world of expressions’ into the ‘world of actions’.
This program is essentially equivalent to the Java loop; although there are some subtleties about the nature of valof/valis that go beyond Java. As a result, it has similar architectural issues.
While one loop is not going to hurt anyone; real code in languages like Java typically has many such loops. Especially when nesting loops to any depth, such code quickly becomes impossible to follow.</Text>
        </Document>
        <Document ID="89A5E220-CE38-43E1-BE4E-548CAC8B5966">
            <Title>Types of Collection</Title>
            <Text>Just as there are many uses of collections, so there are different performance requirements for collections themselves. The most challenging aspects of implementing collections revolves around the cost of adding to the collection, the cost of accessing elements of the collection and the cost of modifying elements in the collection.
There is a strong emphasis on persistent semantics for the types and functions that make up Star’s collections architecture. This is manifest in the fact, for example, that functions that add and remove elements from collections do not modify the original collection.
However, even without that constraint, different implementation techniques for collections tend to favor some operations at the cost of others. Hence, there are different types of collection that favor different patterns of use.</Text>
        </Document>
        <Document ID="D6CF1CA6-767F-4E40-9EBE-14BF8EA3E63B">
            <Title>Compressing Collections With a Fold</Title>
            <Text>Another way of using collections is to summarize or aggregate over them. For example, the average function computes a single number from an entire collection of numbers — as do many of the other statistical functions. We can define average using the standard leftFold1 function, which is part of the standard folding contract:
average(C) is leftFold1((+),C)::float/size(C)::float
Notice the use of coercion here — coercing both the result of the leftFold1 and size to float. The reason for doing this is that functions like average are ‘naturally’ real functions. Without the explicit coercion, averaging a list of integers will also result in an integer value — which is likely to be inaccurate.
Of course, in our definition of average we need to coerce both the numerator and denominator of the division because Star does not have implicit coercion.
The leftFold1 function applies a left-associative binary operator to a collection: starting from the first element and successively ‘adding up’ each of the elements in the collection using the supplied operator.
As we noted above, leftFold1 is part of the foldable contract. Like the functor contract, this uses some more subtle type constraints:
contract all c/1 ~~ foldable[c] ::= {
  leftFold1:all e ~~ ((e,e)=&gt;e,c[e])=&gt;e.
  ...
}
Notice that this contract – which we have not fully written down here – uses quantifiers in two places: once in the contract specification and once in the type signature for leftFold1. What we are trying to express here is that any implementation of foldable must allow for a generic accumulator function.
#
Left Folding a Collection
Our definition of the average function is therefore about as close to a specification of average as is possible in a programming language!
While it is convenient for computing averages, there are several restrictions in the signature for leftFold1: the most egregious is that the result type and the types of the elements of the collection must be identical. A more refined function is possible that liberates us from this:
contract all c/1 ~~ folding[c] ::= {
  ...
  leftFold:all e,ac~~((ac,e)=&gt;ac,ac,c[e])=&gt;ac
  ...
}

leftFold is also part of the foldable contract; as are the analogous rightFold and rightFold1 functions.
In this variant of fold, we separate out an ‘accumulator’ (of type ac) from the type of the elements of the collection. We saw something similar with the visitor pattern. The leftFold function applies its argument function in a similar way to leftFold1, except that it does not require that the type of the accumulator is the same as the elements of the collection. Furthermore, it does not use the first element of the collection as a ‘seed’ of the aggregation — instead, the initial seed is explicitly given.
￼
Left Folding a Collection With a Seed
We can still use leftFold in situations where we would use leftFold1, for example our average function is equally well written as:
average(C) =&gt; leftFold((+),0,C)::float/size(C)::float
But we can do much more than computing averages with a fold. Recall that when we realized the sieve of Erastosthenes, we still had a recursive structure to the program. Furthermore, the way our original program was written each filter results in a new list of numbers being produced. Instead of doing this, we can construct a cascade of filter functions.
Consider the task of adding a filter to an existing filter. What is needed is a new function that combines the effect of the new filter with the old one. The cascade function takes a filter function and a prime as arguments and constructs a new function that checks both the prime and the existing filter:
cascade:((integer)=&gt;boolean,integer)=&gt;((integer)=&gt;boolean).
cascade(F,K) =&gt; (X)=&gt;F(X) &amp;&amp; X%K=!=0.
This is a truly higher-order function: it takes a function as argument and returns a another function.
Given cascade, we can reformulate the sieve function itself as a leftFold — at each new prime step we ‘accumulate’ a new cascaded filter function:
step:((integer)=&gt;boolean,integer)=&gt;((integer)=&gt;boolean).
step(F,X) where F(X) =&gt; cascade(F,X).
step(F,X) =&gt; F.
At each step in the fold we want to know whether to continue to propagate the existing filter or whether to construct a new filter.
The sieve function itself is now very short: we simply invoke leftFold using step and an initial ‘state’ consisting of a function that checks for odd numbers:
sieve(C) =&gt; leftFold(step,(K)=&gt;K%2=!=0,C).
This version of sieve is not quite satisfactory as, while it does find prime numbers, it does not report them. A more complete version has to also accumulate a list of primes that are found. We can do this by expanding the accumulated state to include both the cascaded filter function and the list of found primes. The main alteration is to the step function:
prStep:((list[integer],(integer)=&gt;boolean),integer) =&gt;
				(list[integer],(integer)=&gt;boolean).
prStep((P,F),X) where F(X) =&gt; ([P..,X],cascade(F,X))
prStep((P,F),_) =&gt; (P,F)
and the initial state has an empty list:
sieve(C) is fst(leftFold(prStep,([],(K)=&gt;K%2=!=0),C)).
where fst and snd pick the left and right hand sides of a tuple pair:
fst:all s,t ~~ ((s,t))=&gt;s.
fst((L,R)) =&gt; L
snd:all s,t ~~ ((s,t))=&gt;t.
snd((L,R)) =&gt; R
There is one final step we can make before leaving our sieve of Erastosthenes — we can do something about the initial list of integers. As it stands, while the sieve program does not construct any intermediate lists of integers, it still requires an initial list of integers to filter. However, this particular sequence can be represented in a very compact form — as a range term.
range terms are special forms of collections that denote ranges of numeric values. For example, the expression
range(0,100,2)
denotes the sequence of integers starting at zero, not including 100, each succesive integer being incremented by 2.
Using a similar range term, we can denote the list of primes less than 1000 with
primes(Max) =&gt; let{
  cascade:((integer)=&gt;boolean,integer)=&gt;((integer)=&gt;boolean).
  cascade(F,K) =&gt; (X)=&gt;(F(X) &amp;&amp; X%K=!=0).

  prStep:((list[integer],(integer)=&gt;boolean),integer) =&gt;
				(list[integer],(integer)=&gt;boolean).
  prStep((P,F),X) where F(X) =&gt; ([P..,X],cascade(F,X)).
  prStep((P,F),_) =&gt; (P,F).

  sieve:(list[integer])=&gt;list[integer].
  sieve(C) is fst(leftFold(prStep,([],(K)=&gt;K%2=!=0),C)).
} in sieve(range(3,Max,2)).

show primes(1000)
This final program has an important property: there are no explicit recursions in it — in addition, apart from the leftFold function, there are no recursive programs at all in the definition of primes.</Text>
            <Comments>Real as in the Real numbers.</Comments>
        </Document>
        <Document ID="A73083E0-B7EF-4FA9-96EC-5440403F59D6">
            <Title>Validation</Title>
            <Text>Simply defining operators is not enough to design a language extension. We must be able to make sure that the operators are used sensibly in the appropriate context, and that the Star compiler can make sense of the new syntax. To help with this, Star has a standard way of representing the valid syntactic forms of language extensions — using a set of validation rules.
The purpose of a validation rule is two fold: it defines the legal syntactic forms and it enables a language extension designer to report syntax errors in terms of the DSL rather than in terms of operators (which most users of the DSL should not have to understand).
A validation rule uses patterns to define legal instances of syntactic categories. The top-level categories are expression, pattern, statement and action; however, it is often useful to introduce additional categories. For example, one validation rule that we use for defining the legal forms of triple might be:
 # ?S ! ?P $ ?O :: triple :-
    S::concept :&amp; P::concept :&amp; O::concept
This validation rule states that terms such as
:john ! :name $ "John Smith"
are triples providing that :john, :name and "John Smith" are concepts. The operator :: is a standard operator that can be read to mean ‘has syntactic category’. The :- is read as ‘if’ and :&amp; is read as ‘and’.
The meta language features of Star have a completely different texture and style to regular programs. This is deliberate; it allows a clear separation between regular code and meta code.
The validation rules for concept allow for the two basic forms of concept:
 # : identifier :: concept
 # string :: concept
These rules state that the two legal forms of concept are a colon-prefixed identifier and a literal string. However, we might want to allow simple identifiers also — given that such ‘concept variables’ may also appear:
 # : identifier :: concept
 # string :: concept
 # identifier :: concept</Text>
        </Document>
        <Document ID="88E5CD14-82A3-4086-903A-E4F0331A25C3">
            <Title>Waiting for Events</Title>
            <Text>The wait for operator takes a rendezvous and yields a task that waits for the event described by the rendezvous to happen. So, for example:
wait for timeoutRv(2000)
is a task that, when performed, does nothing for 2 seconds, and yields the () value.
Note that wait for returns a task - it does not execute it. To actually wait for an event, the task must be performed. Typically, within a task block, it is used like so:
task {
  ...
  perform wait for timeoutRv(2000)
  ...
}
There are quite a few pieces here: the timeoutRv event itself, the wait for task function and the performance of the task itself.
This separation into distinct phases helps in flexibility of the concurrency features but, of course, can make straightforward scenarios complex to construct. We shall see below that there is a library of concurrency features that are quite high-level and may be more appropriate than ‘rolling your own’ setup.</Text>
        </Document>
        <Document ID="833DF1C1-C636-4034-9053-01C6641E9578">
            <Title>Contents</Title>
        </Document>
        <Document ID="BDC25B8F-9D69-4B56-BFA4-F35EAA7939F0">
            <Title>Performance Characteristics of Actors</Title>
            <Text>Actors are comparatively efficient at processing speech actions; and they are trusting: that is, they do not perform any validation on the speech actions. One resulting limitation is that they are definitely not safe in a concurrent environment. Again, no interlocking checks are performed – which means that if you use a regular actor in background tasks (say) then you will likely get inconsistent results.
Also, actors are somewhat stateful in nature. They are intended to encapsulate the processing of speech actions; and that implies that they normally carry some form of state.
A corollary of actors’ execution profile is that they are re-entrant: multiple tasks can access the same actor concurrently. This can be advantageous in certain circumstances where the actor is actually stateless and performance is critical.
However, in most concurrent situations the normal actor’s execution model is too dangerous. To make speech action processing safer it is necessary to serialize access to the actor – something that is accomplished with concurrent actors.</Text>
        </Document>
        <Document ID="520C484F-808A-4651-B417-C1CF776CEA07">
            <Title>Title Page</Title>
            <Text>










&lt;$PROJECTTITLE&gt;

&lt;$author&gt;</Text>
            <Notes>These tags get replaced with the information set in the Metadata pane of Compile. Alternatively, you can simply replace this text altogether.</Notes>
        </Document>
        <Document ID="0C48D92D-481D-471E-91FE-508C9FF114D9">
            <Title>Free Variables in Speech</Title>
            <Text>Although the body of a query speech action might be any expression, there are some syntactic restrictions on the valid forms of query expression. The primary reason for these restrictions is to make it simpler to determine the scopes of identifiers occurring in the query expression. Specifically, we need to be able to determine for any identifier occurring in the queried expression whether or not it refers to the agent being queried or the outer context.
For example, consider the simple query:
query Ag with quantity("W-S-23")&gt;0
In this query the function quantity is part of the API of Ag; but the function &gt; is not – it is actually a standard function defined as part of the comparable contract.
The Star compiler has to be able to reliably determine the scope of any identifier, including identifiers in embedded query speech actions. We address this in one of two ways: the speech action processor is able to ‘understand’ sufficient of the standard query notation that it can determine which of the identifiers in the query should be part of the remote agent’s API and which should be local. I.e., we can use query speech actions like:
query Ag with list of { all X where ("W-S-23",X) in products }
and the compiler will assume that products refers to something that belongs to Ag.
The second way in which we can inform the compiler which identifiers belong to the agent is explicitly. For example, we can write:
query Ag's quantity with quantity("W-S-23")&gt;0
This is definitely more clumsy than one would like. Sometimes it is not possible to satisfy all requirements in the design of a programming language.
Why, one should ask, cant the compiler simply rely on the type of Ag? The answer is two-fold: of course, if Ag’s type is known then it will rely on it. However, Star’s type system is based on type inference – it is quite possible that the only signal that Ag is a speech action correspondent is the presence of this speech action. In that scenario there is not enough information to also determine the types of all the free variables.</Text>
        </Document>
        <Document ID="D656362C-1141-4D7D-8F24-A14D76F42943">
            <Title>Representing Triple Graphs</Title>
            <Text>We start with representing triple graphs. Reflecting the ontology we constructed above, we can define three types to represent concepts, triples, and graphs.
There are two kinds of concept, and we want to keep them distinct in our representation:
type n3Concept is n3C(string) or n3S(string)
The two kinds of concept are the named concept — identified by the n3C constructor — and the literal string — identified by the n3S constructor.
The triple is similarly represented by a type definition:
type n3triple is n3(n3Concept,n3Concept,n3Concept)
where the three arguments to n3 are the subject, predicate and object respectively of the triple.
Notice that here we are being explicit about the strong connection between subjects, predicates and objects: they are all concepts.
A triple graph may be represented in a variety of ways — it is effectively a collection of triples. However, for the purposes of exposition, we will assume that triple graphs are represented as lists of triples. We can capture this with a type alias statement:
type n3Graph is alias of list[n3Triple]
A more robust implementation of the triple graph store would provide better support for querying triple graphs by the individual subjects, predicates and objects of triples. This would normally involve being able to index the triples by their subjects, predicates and objects.</Text>
        </Document>
        <Document ID="EE1021F0-8DCF-45A2-B179-EBDFE272F771">
            <Title>The list Type</Title>
            <Text>The list type offers a different trade-off to the cons type: where the latter is optimal for ease of constructing and for traversing complete lists, the list type offers constant-time access to random elements within the array — at the potential cost of more expensive construction of lists.
Unlike the cons type, the list type does not have a straightforward definition as an algebraic type. Internally, an list structure consists of an array of locations with a ‘control pointer’ giving the portion of the array block that represents a given list value.
The list type is optimized for random access and for shared storage — recall that Star collection types are persistent: that means that different values can share some or all of their internal structure. The diagram below shows two list values that overlap in their elements and consequently share some of their structure.
#
Two lists Sharing Structure</Text>
        </Document>
        <Document ID="C4F8ECB6-3E66-43A7-9ABB-62B88C2666D1">
            <Title>Partial Sequence Notation</Title>
            <Text>
The sequence notation also allows for the specification of partial sequences; this is particularly useful in writing functions that construct and traverse sequences. The sequence term:
[1,2,..X]
denotes the sequence whose first two elements are 1 and 2 and whose remainder is denoted by the variable X — which must also be a sequence of the correct type. Similarly, the term:
[F..,23]
denotes the sequence obtained by gluing 23 to the back of the sequence F.
There is a strong relationship between the normal sequence notation and the partial sequence notation. In particular, the sequence expression
cons of [1,2]
is equivalent to:
cons of [1,..cons of [2,..cons of []]]
However, we are not permitted to use both of ,.. and .., in the same expression:
[F..,2,3,..B]
is not permitted (since it amounts to a concatenation of two sequences which implies a non-deterministic decomposition when used as a pattern).
The major benefit of general sequence notation is that it allows us to construct programs involving collections that are independent of type and to do so in a syntax which is concise: the only constraint is the sequence contract.
For example, we can use sequence notation to write functions over sequences; such as the concat function that concatenates two sequences:
concat:all c,e ~~ sequence[c-&gt;&gt;e] |: (c,c)=&gt;c.
concat([],X) =&gt; X.
concat([E,..X],Y) =&gt; [E,..concat(X,Y)].
This function will work equally well with cons lists, lists, strings, even your own collection types. All that is required is that there is an implementation of the sequence contract for the actual type being concatenated.</Text>
        </Document>
        <Document ID="F93A7A74-62CB-4006-9DB7-1171F17FD13A">
            <Title>Translating Graphs</Title>
            <Text>Once we have rules for validating graph expressions in place the compiler is able to verify the form of triple graph expressions but is not able to type check or compile them. For this we need to be able to translate triple graphs to something the compiler can understand; for that we use macros.</Text>
        </Document>
        <Document ID="56E076D8-6607-4B95-B2E4-B02659DC3BDE">
            <Title>Technology</Title>
            <Text>The technology platform that programs are written for is also changing. Just a few decades ago most computers were single-core; nowadays most computers are multi-core and are capable of significant parallelism.
Especially spectacular is the parallelism available in modern GPUs; where a high end graphics processor may have thousands of cores capable of processing instructions in parallel. We expect that the days of personal computers with thousands of cores is not too far in the future.
Programming parallel machines with conventional languages is an exercise in frustration. This is because programming models that worked in single core computers do not scale well to highly parallel machines. One of the primary reasons for this is that state – as represented by the changing values of variables – is implicit in procedural and object oriented languages. The implicitness of state is important because it makes many programs easier to express. On the other hand, that implicitness becomes a liability in multi-threaded and parallel situations where state is no longer so well behaved.
However, Star has adopted some of the recent innovations in that make dealing with multi-tasking and parallel execution easier. These innovations layer on top of basic features such as threading and provide simpler models of execution than ‘conventional’ threaded models. Star’s computation expressions combine the best of fork-join queues and map-reduce frameworks whilst enabling a more normal style of programming.</Text>
        </Document>
        <Document ID="04687E64-D5C2-438F-BC81-7037D99BB9C3">
            <Title>Variations on a Theme</Title>
            <Text>Because we want to allow different variations on the legal forms of triple, the complete validation rules for triple are more complex and involve several different categories:
 # ?S ! ?V :: triple :- S::nounPhrase :&amp; V::verbPhrase
 # ?T :: triple :- error("$T is not a valid triple")

 # ?P $ ?O :: verbPhrase :- P::verb :&amp; O::nounPhrase
 # [ ?VP ] :: verbPhrase :- VP::verbPhrases
 # ?VP :: verbPhrase :-
    error("$VP must have at least one predicate and one object")

 # ?A,?B :: verbPhrases :- A::verbPhrase :&amp; B::verbPhrases
 # ?A :: verbPhrases :- A::verbPhrase

 # [?V] :: verb :- V::verbs
 # ?V :: verb :- V::concept

 # #(?V1,?Vr)# :: verbs :- V1::concept :&amp; Vr::verbs
 # ?V :: verbs :- V::concept

 # [ ?NP ] :: nounPhrase :- NP :: nounPhrases
 # string :: nounPhrase
 # ?C :: nounPhrase :- C::concept

 # #(?NP1,?NPr)# :: nounPhrases :- NP1::nounPhrase :&amp;
     NPr::nounPhrases
 # ?NP :: nounPhrases :- NP::nounPhrase
One thing to notice about these validation rules are the rules of the form:
 # ?VP :: verbPhrase :-
    error("$VP must have at least one predicate and one object")
What this rule is saying is that if the earlier rules for verbPhrase don’t apply then the compiler should report an error message. The notable thing is that the error message is in the context of the triple graph notation: i.e., the compiler is able to report syntax errors in terms of a language extension that is itself not known to the compiler! This is important because it helps to prevent abstraction leaks — where the user of a system must understand what triples might compile to before understanding what is wrong with their program.
One other thing to notice can be seen in the rule for verbs:
 # #(?V1,?Vr)# :: verbs :- V1::concept :&amp; Vr::verbs
The term #(?V1,?Vr)# is a special kind of parenthesized term that means the same as (in this case) ?V1,?Vr. We use #()#’s parentheses because the regular parentheses ()’s are not dropped during parsing by the operator grammar, whereas #()#’s are.
This is because of the importance that regular parentheses play in the language: apart from operator precedence overriding (their most common use in programming languages), ()’s are used for type expressions and for tuple terms. In general, terms of the form (E) are not syntactically identical to E.
The compiler applies parentheses reduction later in the compilation process; when it can be proved to be safe to eliminate regular parentheses.
The intention of the verbs rule is to allow multiple verbs in a predicate of a triple — its context is the rule that picks up on the [] form of predicate:
 # [?V] :: verb :- V::verbs
This rule allows multiple verbs in a triple:
:john ! [:in_department, :works_at] $ :accounting
This form is equivalent to two triples with the same subject and object with with different predicates.
Having rules for triple is not sufficient to ensure validation of triple graphs. This is because the compiler has no a priori reason to understand or look for rules about triple. We need to establish the entry point into the validation.
At the top-most level, the compiler attempts to validate the entire input as a sequence of statements. It so happens that packages and worksheets count as statements.
We stated earlier that a triple graph will be treated as an expression in the larger context. So, the natural entry point for validating triple graphs is as expressions; which we can formalize by defining a new rule for validating expressions:
 # graph{?G} :: expression :-
    G;*triple ## {
      # ?S ! ?V :: triple :- S::nounPhrase :&amp; V::verbPhrase
      # ?T :: triple :- error("$T is not a valid triple")
    ...
}
The form G;*triple is a validation condition that states that G must be a sequence of terms (optionally separated by semi-colons) and each term must be a triple. In addition, we nested the set of validation rules for triple within the rule for graph. The ## operator is analogous to the let operator for expressions; except that here we use it to denote that we should look for the rules for triple within the braces.
Since, the natural scope rules for validation rules is one of cascading and augmentation rather than replacement, the meaning of the ## is a little different to the normal let expression. The validation rules that are located within the body of the ## are applied before other validation rules located externally.
Validating Triple Conditions
Recall that we also introduced a new form of condition that is suited for querying triple graphs. We need to validate such conditions too.
In some ways our says condition is simpler than the triple itself because we choose not to allow our complex noun phrase terminology in the condition. This results in a very simple validation rule for a query condition involving triple graphs:
 ?G says ?S ! ?P $ ?O :: condition :-
  S :: concept :&amp; P :: concept :&amp; O :: concept :&amp;
  G :: expression
Notice that this rule simply states that a says condition is valid if G is an expression. We do not require that G is a literal graph; although its type will need to be consistent.
Unlike the rules for triples, this must must not be embedded within a ## condition. The reason is that we must be able to find says conditions anywhere in a program — not just inside a triple graph.</Text>
        </Document>
        <Document ID="69F0AD6D-5CB4-44D5-B5A8-78B06A8522F2">
            <Title>Quantified Types</Title>
            <Text>A generic type is one which has one or more type variables in it. For example, the type expression:
(x,x)=&gt;boolean
is such a generic type (assuming that x is a type variable – see below).
All type variables must be bound by a quantifier in some enclosing scope. If a type variable is not bound, it is considered free in that type expression.
A quantified type is a type that introduces (i.e., binds) a type variable. There are two quantifiers in Star: a universal quantifier and an existential quantifier.
The most common quantifier is the universal quantifier and universally quantified types correspond closely to generic types in other languages.
Universally quantified types are often used to denote function types and collection types. For example, the type
all x ~~ (x,x)=&gt;boolean
is a universally quantified type that denotes the type of a generic binary function that returns a boolean value. The standard type for the equality predicate == is similar to this type.
A universal type should be read as ‘for all possible values’ of the bound variable. For example, this function type should be read as denoting functions that:
for any possible type – x – the function takes two such x’s and returns a boolean.
Star also supports existentially quantified types — these are useful for denoting the types of modules and/or abstract data types. However, we will leave our exploration of existential types for later.</Text>
        </Document>
        <Document ID="ADCA54BB-70E2-4DA4-9C10-E5BDF12623E5">
            <Title>The Flavors of Equality </Title>
            <Text>Equality in programming languages is typically a very subtle topic. The issues can range from the nature of floating point numbers, the difference between integers and long values and the multiple potential concepts of equality for objects.
Equality in Star is always between values of the same type and it is always semantic. So, for example, an equality condition such as:
3==3.0
is not considered type safe — because 3 is an integer literal and 3.0 is a float literal. If you need to compare an integer and a floating point number for equality you will need to first of all decide in which type the comparison will be made (integer or floating point equality) and then coerce the other value into that type:
3 :: float == 3.0
is valid.
This is an important issue because not all integer values can be represented in a float value and vice-versa. So, comparing an integer and a floating point value raises the possibility of spurious accuracy as a result of losing information.
The second principle is that equality is semantic. What that means is that the == symbol is the name of a boolean-valued function. The precise type of == is quite interesting, we shall, however, leave it to later when we have covered some of the core type features around contracts.
In effect, equality is not considered to be privileged; and it is definable by the programmer — albeit with some important useful default implementations.</Text>
            <Comments>The expression 3::float is a coercion expression that converts the integer 3 into a float value.</Comments>
        </Document>
        <Document ID="AC738EB3-2EC4-4EBC-B361-5276F4871E82">
            <Title>About this book</Title>
            <Text>This book acts as an introduction to the language and to its use. The basic features of the language are introduced; however, this is not a reference manual: it is not intended to be a complete description of the language.
That can be found in the Star Language Definition.
Introducing a programming language like Star can be a challenge in presentation. This is because there is a significant amount of mutual support between elements of the language.
Our strategy is to take a layered approach – we start with simple examples, occasionally skipping over certain aspects of the language without explanation. Later chapters focus on deeper, more complex topics.
For the most part, examples in the text of the book are executable. You are encouraged to try to get them running on your own system.</Text>
        </Document>
        <Document ID="DF8A31D9-8748-4B74-9C38-F0D2C458FD9A">
            <Title>Iteration</Title>
            <Text>We now take a look at how we can process collections using actions rather than expressions. Iteration is one of those areas where history has resulted in two quite different traditions: OO-style languages and functional languages have markedly different approaches to iteration; and yet, as we will see, they can be seen as twins of each other.
A classic iteration over a collection, written in Java in this case, looks something like:
int len = 0;
for(String s:myColl){
  len = len+s.length();  // do something with s
}
As we have seen, the functional approach to this kind of computation would be to capture the implicit recursion into a use of leftFold:
leftFold((A,E)=&gt;A+size(E),0,myColl)
Although the Java and Star code fragments are computing the same value — the total length of string data in the collection — and even though they are nearly the same length; there are radical differences between the two, differences that can make a substantial difference in large programs.
The first salient point is that the iteration/recursion is exposed in the Java code and hidden in the Star code. This is potentially very significant in the event that we want to change how the iteration is implemented — replace a sequential iteration by a parallel one, for example. More subtly, if there are any access issues with the collection — if access to it requires special care with locking or related features — these same issues can be dealt with once — in the implementation of leftFold rather than repeatedly for each loop.
The second salient feature is that the relationship between the iteration and the body of the iteration is inverted. We can see this if we unpack the Java loop, which involves an explicit Iterator object:
int len = 0;
for(Iterator&lt;String&gt; it=myColl.iterator();it.hasNext();){
  String s = it.next();
  len = len+s.length();  // do something with s
}
The Iterator object ‘carries’ most of the information needed to process the collection properly. Each call to next results in s being bound to the next element of myColl.
If we squint appropriately, we can see that the body of the Java loop ‘drives’ the iteration — via the call to it.next(). In effect, the body code is the client of the collection, and the Iterator object is the server in the code fragment.
In the case of the functional code, the loop is encapsulated in the leftFold function which drives the loop by calling the ‘body function’ when needed. It is possible to write Java code that approximates to this style; it would be something like:
Folding.leftFold((A,E)-&gt;{return A+E.length()},0,myColl)
assuming that leftFold were a static function in the Folding class. This relies on new syntax introduced in Java 8. The leftFold would have been much clumsier in earlier versions of the language.
The third saliency is not actually obvious from these fragments, but the functional approach has more variability in the loop structures than Java. Java has several forms of loop, but there is only one loop that is oriented to processing collections — the ‘for each’ loop. On the other hand, in Star — like most functional programming languages — we have fmap to transform a collection, filter to remove unwanted elements, leftFold and its relatives to reduce collections.
The range of looping functions reflects an underlying vocabulary of ‘things we do’ to collections.
The final, perhaps most unexpected point, is that the fundamental computational cost of the two styles of iteration is almost identical! Under reasonable assumptions of optimization for both Java and Star, it is possible to show that the number of steps needed to compute the two fragments is very similar. The details are beyond the scope of this book.</Text>
        </Document>
        <Document ID="7AD249BA-6367-4647-815C-64149EB4FBFC">
            <Title>Polymorphic Arithmetic</Title>
            <Text>There are other ways in which programs can be polymorphic. In particular, let us focus for a while on arithmetic. One of the issues in arithmetic functions is that there are many different kinds of numbers. Pretty much every programming language distinguishes several kinds of numbers; for example, Java distinguishes byte, short, int, long, float, double, BigInteger and BigDecimal — and this does not count the wrapped versions. Other languages have even more choice.
One question that might seem relevant is why? The basic answer is that different applications call for different properties of numbers and no one numeric type seems to fit all needs. However, the variety comes at a cost: when we use numbers we tend to have to make too early a choice for the numeric type.
For example, consider the double function we saw earlier:
double(X) =&gt; X+X
What type should double have? In particular, what should the type of + be? Most people would be reluctant to use different arithmetic operators for different types of numbers. This is resolved in Star by relying on contracts for the arithmetic operations.
The result is that the type computed for double is exquisitely tuned:
double:all t ~~ arithmetic[t] |: (t)=&gt;t
This type is precisely the minimal type that double could have. Any further constraints result in making a potentially premature choice for the numeric type.
If we take another look at our factorial function:
fact(0) =&gt; 1
fact(N) =&gt; N*fact(N-1)
this is constrained to be a function from integer to integer because we introduced the literal integers 0 and 1. However, the arithmetic contract contains synonyms for these very common literals. Using zero and one allow us to be abstract in many arithmetic functions:
genFact:all a ~~ arithmetic[a] |: (a)=&gt;a.
genFact(zero) =&gt; one.
genFact(N) =&gt; N*genFact(N-one).
We call out zero and one for special treatment because they occur very frequently in numerical functions. We can introduce other numeric literals without compromising our type by using coercion; although it is more clumsy:
factorialC:all t ~~
    arithmetic[t],coercion[integer,t] |: (t)=&gt;t.
factorialC(N) where N==0 :: t =&gt; 1 :: t.
factorialC(N) =&gt; N*factorialC(N-1 :: t).
The expressions 0 :: t and 1 :: t are coercions from integer to t.
Of course, coercion is also governed by contract, a fact represented in the type signature by the coercion contract constraints on the type of t.
In any case, using these techniques, it is possible to write numeric functions without unnecessarily committing to specific number types. That in turn helps to make them more useful.</Text>
            <Comments>Although some languages — such as ML — do require this.</Comments>
        </Document>
        <Document ID="8F2A5D69-042B-44B4-9E7D-952E34B29B65">
            <Title>Designing Syntax</Title>
            <Text>Once we have at least an approximate conceptualization, our next step is to design the syntax. Syntax is important because it encodes the manner in which different features of the DSL can be expressed and combined.
There is an inevitable requirement for taste in designing the syntax of a language extension. One of the foundations for this taste is knowledge of the existing syntactic patterns in the host language. Another is an awareness of the combinatorial potential in the language — i.e., what can be combined with what.
Notice that we identify a graph as being a sub-class of expression; this represents an important choice point in DSL design: whether we are extending expressions, statements, actions, or types. Of course, in complex projects you may find yourself implementing multiple kinds of extensions. We choose to model graphs as expressions as that gives us the maximum flexibility in using triple graphs to represent policies.</Text>
        </Document>
        <Document ID="52BE1A92-ACB6-4873-9C98-D2BB53D4A3EE">
            <Title>Queries and Maps for Statistical Purposes</Title>
            <Text>We wrap up our exposition on collections with an example that highlights how we can combine many of the collection manipulation features; with a specific goal of statistical processing of data.
Statistics is, of course, one of the key application areas of computers in general. However, there is often a substantial gap between the theoretical aspects of statistical processing and the pragmatics of collecting and processing data. We aim to demonstrate Star’s power in both areas.
One fecund source of statistics is the web; we can even get statistics about the web. The on-line tool Web Page Test can be used to generate a lot of data about how a browser responds to a website. Furthermore, we can down this data as a file of comma separated values (CSV). The first few lines of this file shows the detail of the data collected:
Date,...,URL,Response Code,Time to Load (ms),...,Bytes In,...
9/12/14,...,/,302,397,...1272,...
There are over 70 columns there. Suppose that we wanted to process this to find out how much time is spent loading Javascript data. Our first step is to introduce the CSV file to Star which we can do with a special macro:
import metaCSV.
worksheet{
  generateCSV Wpt from "file:WPT_Sample.csv".

  ...
}
The generateCSV line is a macro that parses the sample CSV file, constructs a type (Wpt) that reflects the entries in the file and a parser that can parse similar CSV files into Wpt entries. The generated Wpt type looks like:
Wpt ::= Wpt{
    Date:string.
    ...
    URL:string.
    'Response Code':integer.
    'Time to Load (ms)':integer.
    ...
    'Bytes In':integer.
    ...
  };
One immediate feature to notice is that some of the field names are quoted. Star allows a variable (and by extension a field) identifier to have any characters in them — so long as the identifier is quoted. This allows us to access ‘foreign’ data structures like this CSV file in a straightforward way.
Given the implicit generation of the type, and of a parser, we can use this to process real data files. For example, we can extract out from the CSV file records containing just the URLs, the file sizes and the load times using a query:
import metaCSV.
worksheet{
  generateCSV Wpt from "file:WPT_Sample.csv".

  wptData:Wpt.
  wptData = WptParser("http:www.webpagetool.org/...csv").

  extracted:list[{URL:string. size:integer. loadTime:integer}].
  extracted = list of { all
    { URL=D.URL
     size=D.'Bytes In'
     loadTime=D.'Time to Load (ms)'} where
    D in wptData}
  ...
}
We used the generated parser to reach out to the website for the data and to parse the resulting content — in a single high-powered statement.
A single run of the web page test may involve many separate browser actions — the purpose of the tool is to test the performance of a website, which needs many trials to achieve statistical significance. So a better organization of the data is to categorize the raw data by URL. We can do this using a group by query:
import metaCSV.
worksheet{
  generateCSV Wpt from "file:WPT_Sample.csv".

  wptData:Wpt.
  wptData = WptParser("http:www.webpagetool.org/...csv").

  extracted:list[{URL:string. size:integer. loadTime:integer}].
  extracted = list of { all
    { URL=D.URL.
     size=D.'Bytes In'.
     loadTime=D.'Time to Load (ms)'} where
    D in wptData} group by ((X)=&gt;X.URL)
  ...
}
The group by operator takes a collection, and a categorization function, are produces a dictionary of collections.
We want to produce average load times and standard deviations of the load times — to smooth out the vagaries of the Internet. For now, we will assume that we have average and stddev functions that have type signatures:
average:all c,e ~~ sequence[c-&gt;&gt;e] |:(c,(e=&gt;float))=&gt;float.
stddev:all c,e ~~ sequence[c-&gt;&gt;e] |:(c,(e=&gt;float))=&gt;float.
I.e., we are assuming functions that take collections, and an ‘accessor’ function, and return the average and standard deviation respectively.
Given these functions, we can compute our statistics using:
import metaCSV.
import stats.
worksheet{
  generateCSV Wpt from "file:WPT_Sample.csv".

  wptData:Wpt.
  wptData = WptParser("http:www.webpagetool.org/...csv").

  extracted:list[{URL:string. size:integer. loadTime:integer}].
  extracted = list of { all
    { URL=D.URL.
     size=D.'Bytes In'.
     loadTime=D.'Time to Load (ms)'} where
    D in wptData} group by ((X)=&gt;X.URL)

  ldTime:(Wpt)=&gt;float.
  ldTime(D) =&gt; D.loadTime::float.

  show list of { all (K,average(V,ldTime),stddev(V,ldTime)) where
        K-&gt;V in extracted }
}
The query condition:
K-&gt;V in extracted
is analogous to the regular search condition:
D in wptData
except that it is used to search within a dictionary-based collection. The pattern K-&gt;V matches the successive key/value pairs in the dictionary.
That is it! The final query should get output along the lines of:
...
("/o/oauth2/auth?...", 337.0, 0.0),
("/main-thumb-58380181-100-yqpttun...", 101.18518518519, 24.41468441456),
("/main-thumb-49759239-100-pcgldxn...",93.48148148148, 19.03285636867),
...
Notice that most of the remaining complexity in this example related to selecting the right parts of the data to pick up and process.</Text>
            <Comments>A standard deviation of 0.0 is likely a signal that the indicated URL only occurred once in the sample data.</Comments>
        </Document>
        <Document ID="9FFBA333-8432-41C1-91F5-1302DFE283CA">
            <Title>Non-Fiction Format</Title>
            <Text>GENERAL NON-FICTION

About This Template
When compiled (File &gt; Compile), this project will generate a document in the standard manuscript format used for many types of non-fiction. Settings are also provided to make it easy to compile to a paperback-style PDF for self-publishing or an EPUB or Kindle ebook.

How To Use This Template
	•	Inside the “Manuscript” folder, create a new folder for each chapter and title each folder with the name of the chapter. (You do not need to—and indeed shouldn’t—title the folders “Chapter One” and so on, because chapter numbering will be taken care of automatically during the Compile process.) The first chapter folder has been created for you with the placeholder title “Chapter”.
	⁃	Note: The “Manuscript” folder is what we refer to in the documentation as the “Draft folder”. It’s just been renamed “Manuscript” in this template.
	•	Create a new text document for each section inside the chapter folders. (Upon export, sections will be separated with the “#” character for standard manuscript format, or with a blank line for other formats.)
	•	If you don’t require a foreword, move the “Foreword” document to the Trash folder. Alternatively, rename it “Preface” or “Introduction” if you prefer.
	•	“Notes” and “Ideas” folders have been provided for your convenience, although you can replace them or create different top-level folders for your research materials if necessary (these are just regular folders that have had custom icons assigned to them using the Documents &gt; Change Icon feature).

Tables and Figures
If you need to use tables or figures in your manuscript, you can label them using the following tags, replacing “KEYWORD” with a unique word that identifies your table or figure.

!fig(KEYWORD)
!table(KEYWORD)

These tags will be replaced with numbers in the compiled document, with the numbering stream for figures being separate from the numbering stream for tables.

You can refer to such tags as follows:

#fig(KEYWORD)
#table(KEYWORD)

Here’s an example:

Table !table(sales): Sales 2011
Table !table(dates): Shipment dates
Figure !fig(skeleton): The skeleton of the gnu.
Figure !fig(malcolmreynolds): The second coolest character in Firefly.

… (For sales figures, see table #table(sales))… For shipment dates, see table #table(dates)… where he discovered the skeleton of a gnarled gnu (see figure #fig(skeleton))… Chronicles providing a similar role, albeit without the Castle actor (see figure #fig(malcolmreynolds)).

In the compiled document, the above text would look like this:

Table 1: Sales 2011
Table 2: Shipment dates
Figure 1: The skeleton of the gnu.
Figure 2: The second coolest character in Firefly.

… (For sales figures, see table 1)… For shipment dates, see table 2… where he discovered the skeleton of a gnarled gnu (see figure 1)… Chronicles providing a similar role, albeit without the Castle actor (see figure 2).

Compiling
	•	Title pages and front matter are all provided in the “Front Matter” folder. Different front matter is used for different formats.
	•	A “Back Matter” folder is also provided. Here you can add any back matter hat is specific to each Compile format. Currently this only contains documents for storying endnotes (see below).
	•	Tip: You can open this document in a Quick Reference panel and have it open alongside the Compile panel if you need to refer to these instructions while compiling.

	•	To compile to standard manuscript format:
	◦	Edit the “Title Page” document inside the Front Matter/Manuscript Format folder to ensure that it contains the correct information (by default it will use information from Contacts).
	◦	To create your table of contents:
	⁃	Holding down the Command key, select all the documents you wish to be included in the contents listing. (Usually this will be “Foreword” and all chapter folders, but not the individual sections inside chapter folders. If you have footnotes and wish them to be added to the end of the manuscript as endnotes, you should also include the “Endnotes” document inside “Back Matter”.)
	⁃	Go to Edit &gt; Copy Special &gt; Copy Documents as ToC.
	⁃	Paste into the “Contents” document in the “Manuscript Format” front matter folder. (Tip: Use Format &gt; Font &gt; Underline to change or remove the dotted underline between the chapter names and page numbers.)
	⁃	The resulting linked text and page number tags will be replaced with the final chapter names and page numbers in the compiled document.
	⁃	Note that you don’t need to do this every time you compile, only when chapters have been added, deleted, renamed or moved around. (Note to Microsoft Word users: If you export to Word, you may need to generate a print preview in Word to force the page numbers to show up correctly - they may appear as question marks before doing so.)
	⁃	You don’t have to use “Copy Documents as ToC” to create a table of contents, by the way. You could equally construct one yourself using document links and placeholders - see “List of All Placeholders…” in the Help menu for more information on the latter.
	◦	Go to File &gt; Compile…
	◦	Next to “Compile for”, select “Print”, “PDF” or one of the rich text file formats such as RTF, Word or OpenOffice.
	◦	Select either “Non-Fiction Manuscript (Courier)” or “Non-Fiction Manuscript (Times)” from the list of formats on the left.
	◦	Ensure that the “Add front matter” button is ticked under the contents list on the right and that the “Manuscript Format” folder is selected in the pop-up button next to it. (This has already been set up for you.)
	◦	If you have footnotes in your text, they can either be included at the end of each page (the default) or as endnotes. To have them included as endnotes:
	⁃	Beneath the list of documents on the right side of the Compile pane, tick “Add back matter”. This will include the “Endnotes” document from the Back Matter/Manuscript Format folder.
	⁃	Click on the gear icon in the right-most header bar in Compile.
	⁃	Tick “Export inspector footnotes as endnotes” and “Export inline footnotes as endnotes”.
	◦	Click on “Compile”.

•	To compile to paperback PDF format:
	◦	Edit the front matter pages contained inside the Front Matter/Paperback folder.
	◦	Follow the instructions under compiling for standard manuscript format for creating a table of contents, but this time paste the contents list into the Front Matter/Paperback/Contents document.
	◦	Go to File &gt; Compile…
	◦	Next to “Compile for”, select “PDF”.
	◦	Select “Paperback (5.06” x 7.81”)” from the list of formats on the left.
	◦	Ensure that the “Add front matter” button is ticked under the contents list on the right and that the “Paperback” folder is selected in the pop-up button next to it if it. (This has already been set up for you.)
	◦	If you have footnotes in your text, they can either be included at the end of each page or as endnotes. See the instructions under compiling for standard manuscript format for information on how to include footnotes as endnotes (in this case, however, clicking “Add Back Matter” will include the “Paperback” back matter folder).
	◦	Click on “Compile”.

	•	To compile to ebook format:
	◦	Edit or remove the dedication page contained inside the Front Matter/Ebook folder. Feel free to add any other front matter documents as required.
	◦	Import a cover image (preferably in JPG or PNG format).
	⁃	You can store this anywhere, but it makes sense to place it in the “Ebook” front matter folder. A placeholder cover image is already provided—you will want to delete that once you have imported your own.
	⁃	Be sure to check online for recommended image sizes, because the recommendations are constantly changing. The sample cover image provided is 2,500 x 1,563 pixels, based on Amazon’s recommended size and proportions for a Kindle cover at the time this template was created.
	◦	Go to File &gt; Compile…
	◦	From the “Compile for” menu, select one of the ebook formats.
	⁃	To create an ePub file, choose the “ePub 3” format.
	⁃	To create a Kindle file, choose the “Kindle KF8/Mobi” format.
	⁃	“ePub 2” and “Kindle Mobi” are older versions of the formats. They are mainly provided for supporting projects created in older versions of Scrivener.
	◦	Select “Ebook” from the list of formats on the left.
	◦	If you have front matter documents other than the cover image, ensure that the “Add front matter” button is ticked under the contents list on the right and that the “Ebook” folder is selected in the pop-up button next to it. If you don’t have any front matter documents, you can un-tick “Add front matter”. (Un-ticking this does not affect the cover image.)
	◦	Above the contents list in the rightmost header bar are six buttons. Click on each of them in turn to go through the various available settings, changing what you need. In particular:
	⁃	Fill in the metadata such as author name and book title.
	⁃	Ensure your cover image is selected and shown.
	◦	Click on “Compile”.

Adding Sub-Heads
If you want your chapters divided into sections that each have their own sub-headings, simply use blank file groups for each heading, with each section under that heading a subdocument of the file group, like this:

#

Customising the Ebook Table of Contents
When exporting to ebook format, Scrivener automatically generates a table of contents. If you would like to customise what appears in the contents, follow these instructions:
	1.	Create a document for your table of contents inside the Front Matter/Ebook folder.
	2.	Name the document “Contents”.
	3.	In the binder, select the documents you would like to appear in the table of contents (hold down Command to select more than one document).
	4.	For a simple flat list, hit Cmd-C or use Edit &gt; Copy. Alternatively, if you would like the table of contents indented to match the binder structure, go to Edit &gt; Copy Special &gt; Copy Documents as Structured Link List.
	5.	Click into the text of the “Contents” document and hit Cmd-V or use Edit &gt; Paste. The documents you wish to appear in the table of contents will now appear as a list of links.
	6.	If you intend to export to an older ebook format (ePub 2 or non-KF8 Mobi), select all of the text and change it to use a 12-point font.
	7.	If you wish to centre the table of contents, select the text and centre it.
	8.	In the Inspector, change the “Section Type” of the “Contents” document to “Table of Contents”. Now, when you Compile, your custom “Contents” document will be used instead of the automatically-generated one. The titles in the links of the “Contents” document will automatically be updated to match those of the final compiled ebook.

Sample Document
See the “Sample MS” PDF file in the Research folder for an example of a document that has been created using this template.

Final Note
Scrivener project templates are flexible and are not intended to restrict you to a particular workflow. You can change, delete or move the files and folders contained in the template to suit how you work.

Like all templates in Scrivener, this project was originally created from the “Blank” template. We’ve simply added a few folders and set everything up in ways that should be useful to non-fiction authors. Everything you can do with this project, you could equally do by creating a “Blank” project and setting it up yourself.

You can create your own templates by setting up a skeletal project with the files, folders and settings you would like to use for new projects and using File &gt; Save As Template.</Text>
        </Document>
        <Document ID="4A4528B5-7AC1-4C6B-9B4C-6AC91E7F35F8">
            <Title>Why Be a Star Programmer</Title>
            <Text>This book is about the programming language called Star. Why, you may ask, do you need to learn yet another language? We hope to answer that question and more in the course of this book. We also aim to show you how to program effectively in Star to solve real world problems.</Text>
        </Document>
        <Document ID="3F025EB6-8DDE-4ADE-A283-F7F4E19F12CA">
            <Title>The dictionary Type</Title>
            <Text>Unlike the cons or list type, the dictionary type is oriented for access by arbitrary keys. The dictionary is also quite different to hash trees as found in Java (say), the dictionary type is persistent: the functions that access dictionaries such as by adding or removing elements return new dictionaries rather than modifying a single shared structure. However, the efficiency of the dictionary is quite comparable to Java’s HashMap.
The template for the dictionary type is:
all k,v ~~ equality[k] |: dictionary[k,v]
Notice that there is an implied constraint here: the dictionary assumes that the keys in the dictionary can be compared for equality.
A dictionary value can be written using the sequence notation, using tuple pairs for the key-value pairs:
dictionary of [(1,"alpha"),(2,"beta”)]
Dictionaries also have a special variant of the sequence notation; instead of writing the pairs as tuples we can use an arrow notation for dictionary terms:
dictionary of [1-&gt;"alpha", 2-&gt;"beta"]
Dictionaries also have their own special variant of a query search condition. A condition of the form
K-&gt;V in D
where D is a dictionary will be satisfied if there is a key/value pair in D corresponding to K and V. For example, the condition:
K-&gt;V in dictionary of [1-&gt;"alpha", 2-&gt;"beta"] &amp;&amp; V=="alpha"
is satisfied for only one pair of K and V: namely 1 and "alpha" respectively.
For the curious, dictionaries are implemented using techniques similar to Ideal Hash Trees, as described by Bagwell [#Bagwell01idealhash]. This results in a structure with an effective O(1) cost for accessing elements and for modifying the dictionary — all the while offering an applicative data structure.</Text>
        </Document>
        <Document ID="A9FE36BE-4894-4EE9-A729-A0E03E82D7A5">
            <Title>Nominative Types</Title>
            <Text>A nominative type is normally defined using an algebraic type definition. This both introduces a type and defines all the legal values that belong to the type. For example, we might introduce a Person type with the type definition:
Person ::= noOne
         | someOne{
              name : string.
              dob : date.
            }
This statement tells us that there are two kinds of Person: a someOne who has a name and date of birth (dob) associated with them; and a distinguished individual we identify as noOne. The no-one individual does not have a name or date of birth.
Notice how the type annotation statement we saw for declaring the type of fact is also used for defining the types of fields in the someOne record.
We can make a Person value with a labeled record expression:
S : Person.
S = someOne{
  name = "fred".
  dob = today()
}
The equality symbol is used to introduce a new single-assignment variable. In this case the variable S is defined to be a someOne record.
Recall that top-level names require an explicit type annotation – hence the declaration that S is a Person. However, we do not need to explicitly give types to the name and dob fields because their type is constrained by the type declaration for Person.
An important detail about the someOne record defined above is that the fields within it are not re-assignable. If we want to make a variable reassignable, or if we want to make a field of a record reassignable, we use a special ref type to denote that. For example,
employee ::= employee{
  dept : ref string.
  name : string
}
allows the dept field within the employee record to be modifiable.
Only fields that have a ref type are modifiable in records. This is even true when a record is assigned to a reassignable variable.
A reassignable variable is declared using the := operator:
N : ref employee.
N := employee{
  dept := "your department".
  name = "Anon. Y. Mouse"
}
Since the variable N is declared as being reassignable, we can give it a new value:
N := employee{
  dept := "another".
  name = "some one"
}
We can also modify the dept field of N:
N.dept := "new department".
However, we cannot modify the name field — because it is not re-assignable within the Person type.
Notice that the re-assignability of variables and fields does not ‘inherit’: each field or variable is separate. So, for example, if we declared a single-assignment variable D to be an employee:
D : employee.
D = employee{
  dept := "his department".
  name = "Your Name Here"
}
then, even though D itself cannot be re-assigned to, the dept field of D can be altered:
D.dept := "my department"</Text>
        </Document>
        <Document ID="E2266586-7BB7-4E74-94CA-E1615F0AD4CB">
            <Title>Nouns and Verbs</Title>
            <Text>Natural language grammar often has many kinds of words; but the two most important kinds of words are nouns and verbs. The same is usually true of computer languages: there are things that we want to represent and process in some way and there are actions that we want to be be able to model.
The kind of UML-based analysis that we have used so far often leads naturally to a classification of the things in the DSL. Knowing what you want to do with those things may be harder to clarify. However, we have not finished with our use of UML.
One way of defining action, as illustrated in the UML diagram is:
#
Ontology of Action
Action
An action is the intentional application of force to achieve an objective.
Here, ‘force’ is intended to be broadly interpreted — it can include, for example, calling a function as well as lifting a rock.
Agency – itself a term that can be difficult to define – it core to the notion of action. If a tree in a forest falls on a squirrel and squashes it that was not an action because the tree did not (cannot) intend to fall on the squirrel: it has no agency for this class of action.
On the other hand, the jump the squirrel makes when it sees the tree falling is an action: it intends the application of force on the rock it was sitting on — in order to escape the falling tree.
This analysis of action is important because it gives us all kinds of hooks for designing the verbs in our DSLs. For example, a lot of object-oriented programming can be understood by letting the target of an action be a data value.
More generally, we have:
Target
The target of an action is the thing that is being operated on.
Not all actions have an obvious target; for example, the true target in an e-mail marketing campaign is not the e-mail system but the human readers of e-mail.
Instrument
The instrument is the means by which the action is performed.
In the case of the squirrel above, the instrument is the squirrel itself; in the case of calling an API it is the API method.
Force
The diversion of potential energy into actual work.
This can be difficult to get one’s head around. Without force though, there is no action. In the case of a computer program, force amounts to execution: the action is performed when the corresponding program fragment is executed.
Agent
Agency is what distinguishes action from random events.
There are many possible interpretations of agent; we shall see some of them in our chapter on communicating agents. The simplest form of agent in a programming language is simply the thread of control. However, if your DSL is about multiple threads or parallelism in some way, then you will likely need to focus on agent as a specific concept.
Intention
An internal state which is potentially causal for action.
A key aspect of action is that it is intentional — or to put it counterfactually: an action is not an accident.
When designing a DSL, the verbs in the DSL that correspond to actions are normally some form of command. A command also pre-supposes an agent that issues the order.
What difference does this make? There may be other kinds of verbs that you will need in your DSL. For example, declaratives state something about what is (or should be) true. Declaratives are not actions; if for no other reason than declaratives are not associated with intention.
Objective
It is the objective that often gives the most concrete guidance in designing DSLs. There may be many objectives and any given objective may have many actions associated with it. However, if you list your actions’ objectives, then you may have the beginnings of a list of verbs.
Note that we make a distinction between objectives and goals. Agents have goals, actions have objectives. Goals can be viewed as desirable states of the world, an objective is a measurable state. Goals are often not directly observable; by their nature, objectives observable in principle — though not necessarily by everyone.
Objectives also allow us to distinguish between intended results and unintended effects.
Not all DSLs make all the features of an action explicit. However, the exercise of iterating through the different aspects of action will often lead to a clear conceptualization of the nouns and the verbs that form the core of the DSL.
In the case of our RDF language, the principal actions relate to store and recall. We need to be able to manipulate triple graphs and we need to be able to query them. Here, we focus on querying because that is core to the role of any knowledge store.</Text>
            <Comments>Even in a pure expression language, a similar distinction holds: there is data and there are operators over that data. To be fair, there is an extreme variant of functional programming that only has functions; but even there the distinction between data and operators is still meaningful.
There are actions that tree can be said to have agency in; for example, some trees alert their neighbors when they are attacked by insects.</Comments>
        </Document>
        <Document ID="6B2294B0-E706-47B3-A1C9-8599AB5C74BE">
            <Title>Packages and Worksheets</Title>
            <Text>We have already seen the two basic forms of compilation unit in Star: the package and the worksheet.
Since Star is an extensible language, it is quite possible to create other forms of compilation unit.</Text>
        </Document>
        <Document ID="BC183BD7-3020-4119-B0B1-139B4FAE98A7">
            <Title>Implementing Indexing</Title>
            <Text>Of course, this includes our own types. For example, before, when looking at generic types we saw the tree type:
all t ~~ tree[t] ::= tEmpty | tNode(tree[t],t,tree[t]).
We can define an implementation for the indexable contract for this type — if we arrange for the tree to be a tree of key-value pairs:
implementation all k,v ~~
    comparable[k], equality[k] |:
      indexable[tree[(k,v)]-&gt;&gt;k,v] =&gt; {
  _index(T,K) =&gt; findInTree(T,K).
  _set_indexed(T,K,V) =&gt; setKinTree(T,K,V).
  _delete_indexed(T,K) =&gt; removeKfromTree(T,K).
}
The form of the type expression tree[(k,v)] is required to avoid confusion — tree takes a single type argument that, in this case, is a tuple type. The extra set of parentheses ensures that tree is not interpreted (incorrectly) as a type that takes two type arguments.
With this statement in scope, we can treat appropriate tree expressions as though they were regular arrays or dictionaries:
T : tree[(string,string)].
T = tNode(tEmpty,("alpha","one"),tEmpty)
assert T["alpha"]=="one".
U : tree[(string,string)].
U = T["beta"-&gt;"two"]. -- Add in "beta"
assert U["alpha"]=="one".
assert present U["beta"].
assert \+ present U["gamma"].
The implementation statement relies on another feature of Star’s type system — we need to constrain the implementation of indexable to a certain subset of possible instances of tree types — namely, where the element type of the tree is a pair — a two-tuple — and secondly we require that the first element of the pair is comparable — i.e., it has the comparable contract defined for it.
This is captured in the contract clause of the implementation statement:
implementation all k,v ~~
    comparable[k], equality[k] |:
      indexable[tree[(k,v)]-&gt;&gt;k,v] =&gt; ...
This implementation statement is fairly long, and the type constraints are fairly complex; but it is exquisitely targeted at precisely the right kind of tree without us having to make any unnecessary assumptions.
Implementing the indexable contract requires us to implement three functions: findInTree, setKinTree and removeKfromTree. The findInTree function is quite straightforward:
findInTree:all k,v ~~ (tree[(k,v)],k)=&gt;option[v].
findInTree(tEmpty,_) =&gt; none.
findInTree(tNode(_,(K,V),_),K) =&gt; some(V).
findInTree(tNode(L,(K1,_),_),K) where K1&gt;K =&gt; findInTree(L,K).
findInTree(tNode(_,(K1,_),R),K) where K1&lt;K =&gt; findInTree(R,K).
Notice that each ‘label’ in the tree is a 2-tuple — consisting of the key and the value. This function is also where we need the key type to be both comparable and supporting equality. The comparable constraint has an obvious source: we perform inequality tests on the key.
The equality constraint comes from a slightly less obvious source: the repeated occurrence of the K variable in the second equation. This equation is actually equivalent to:
findInTree(tNode(_,(K,V),_),K1) where K==K1 =&gt; some(V)
We leave the implementations of setKinTree and removeKfromTree as an exercise for the reader.
Along with the implementation of indexable, we should also implement sequence for our trees:
implementation all k,v ~~ comparable[k], equality[k] |:
    sequence[tree[(k,v)]-&gt;(k,v)] =&gt; {
  _nil() =&gt; tEmpty.
  _cons((K,V),T) =&gt; setKinTree(T,K,V).
  ...
}
where, again, we leave the implementation of the remaining part of the contract to the reader. The reason for implementing sequence is that that will allow users of the tree to use the dictionary variant of the sequence notation for writing tree literals; as in:
tree of [1-&gt;”alpha”, 2-&gt;”beta”]
The sequence contract includes a pattern that can be used to match sequences. It is not completely obvious what one should do when matching the head of a tree. The standard library chooses to match against the left-most leaf of the tree and return a new tree containing all the other elements for the tail of the match. Since the tree is ordered by key value, the order in which elements of a tree are written in a tree literal does not affect the finally constructed tree value.</Text>
            <Comments>It is also true that most programmers will not be constructing new implementations of the indexable contract very frequently.
With the possible exception of tree balance.</Comments>
        </Document>
        <Document ID="6A5FAE33-927D-4843-B7A1-6DEDDF975B80">
            <Title>Another Look at Types</Title>
            <Text>Organizing data is fundamental to any programming language. Star’s data types are organized around the algebraic data type. In addition, Star’s supports quantified types of several varieties.</Text>
        </Document>
        <Document ID="C48B7BB0-0971-429E-B874-891B1344F536">
            <Title>Tentative Computation</Title>
            <Text>If we look at the signature for _index we can see that this function does not directly return a value from the collection, but instead returns an option value. This bears further explanation.
The great unknown of accessing elements of a collection is ‘is it there?’. Its not guaranteed of course, and we need to be able to handle failure.
This is where the concept of ‘tentative computation’ becomes important.
Tentative Computation
A tentative computation is denoted by an expression that is inherently plausible to not have a value.
When we want to open a file, access an element of a dictionary, parse a string with a regular expression we need to be able to express the possibility of failure as well as of success. There are also times when ‘no answer’ is a legitimate response.
We encode this tentativeness (sic) in the option type. The type definition for option is straightforward:
all t ~~ option[t] ::= none | some(t).
where none is intended to denote the non-existence of a value and some denotes an actual value.
The option type is intended to be used in cases where functions are known to be partial. The option return type signals that the function may not always have a value.
In the case of the _index function, its responsibility is to either return a value wrapped as a some value — if the index lookup is successful — or the signal none if the index lookup fails. Just to be clear, _index can act both as a lookup and as a test for membership in the collection.
In addition to the option type, there are a series of operators that make tentative computations easier to express: these are the optional field access operator — ?. — the option default operator — ?| — and the ?.= optional binding predicate.
The ?| operator allows one to unpack an optional value but to give a default in the case that the optional value is none.
We can see where the latter may be useful when accessing dictionaries. For example, the fillIn function accesses a dictionary for a key but uses a default value when it is not there:
fillIn:all k,v ~~ (dictionary[k,v],k,v)=&gt;v.
fillIn(Tr,Ky,Def) where Vl ?.= _index(Tr,Ky) =&gt; Vl.
fillIn(Tr,_,Def) =&gt; Def.
The condition Vl ?.= _index(Tr,Ky) is satisfied if the _index call returns a proper value and it also binds the variable Vl to that value (specifically, it matches the value against the variable Vl.
While the ?.= operator is very useful in unpacking an optional value, the ?| operator allows us to handle cases where we always need to be able to give some kind of value. For example, normally a dictionary returns none if an entry is not present. However, a cache is structured differently: if a value is not present in a cache then we must go fetch it:
cacheValue(K) =&gt; cache[K] ?| fetch(K)
There is, clearly, a strong relationship between ?.= and ?| — each can be expressed in terms of the other.
The option type — and the some and none values — play some of the same roles as NULL does in other languages. For someone approaching a functional language from most imperative languages they will be struck — and maybe upset — by the lack of a null (or nil or NULL or undefined). After all, if it’s good enough for Java, why can’t Star have it too?
Perhaps the biggest single reasons for not having a universal NULL value are that it corrupts the type system and that it makes reasoning about programs harder.
In a language which has a universal NULL value, the programmer (and the compiler) must be ever vigilant about references: is the value actually a NULL value? This is true even in those cases where the programmer knows values cannot be NULL. By isolating nullability (sic) into a single concept it allows the programmer to use the feature where it is actually needed.
Overall, the option type is part of an elegant approach to nullability that is easily incorporated into Star’s (and similar) type system.</Text>
            <Comments>A partial function does not have a value across the whole range of its arguments.
Read as ‘has a value’.</Comments>
        </Document>
        <Document ID="4F174B3A-4B94-4F1D-BF6E-E4D780DE6155">
            <Title>Boxes and Arrows</Title>
            <Text>It is a kind of truism that whenever engineers need to explain their systems to each other they tend to resort to drawing pictures with boxes and arrows between them. For example:
#
A System for Splitting Orders
could be used to explain the system for managing the way parts are ordered in a car factory that supported a ‘build-to-order’ model for manufacturing customized vehicles.
The different boxes show the major sub-systems and the arrows between them show the major flows between them: we have an input Orders sub-system that accepts incoming orders for cars, we have a Splitter that breaks up the orders into orders for specific assemblies and parts. We also have a Parts database that ‘knows’ how different order requirements translate into specific assemblies. The outputs of the system are orders for parts that will go to individual suppliers S~0~ through S~n~.
There have been a number of attempts to take the boxes-and-arrows intuition and turn it into something that is more actionable. Here, we take an approach that focuses on boxes representing components that denote areas of responsibility and where arrows represent interactions between the components.
In laying out our platform architecture, we tighten up on one rather important aspect of designing systems: all interactions between components are explicitly identified; and, in keeping with the Star style, all interactions are strongly statically typed.
For example, if we zoom in on the Split component above, and draw it in isolation we might arrive at:
￼
The Split Component
The main new feature in this diagram – compared to the informal version – are the different ports that represent the entry and exit points to components. The SplitComponent above has three different ports on it: a responding port that handles incoming orders, an originating port that will be connected to a database and a publishing port that will be connected to multiple supplier components. Given this diagramming notation, the order processing system would look something like:
￼
An Order Processing System
The main difference between this diagram and the original boxes-and-arrows picture is a slight formalization of the notation: we have crystalized the role of boxes (into processing components) and we have formalized the connections in terms of different kinds of ports.
In addition, we emphasize that the key architectural constraint that we adopt is that all communication between components should be explicit. Another way of stating this is that
wiring between components belongs to the containers in which the components are embedded.
Other aspects of platforms which would be necessary to complete the picture include the market place itself: how can clients select components for use in their applications and how can providers make their offerings known? We will come back to these questions below.
It turns out that, with some support from Star, we can turn diagrams like these into executable code that can be deployed over distributed systems. Let us see how we might go about this…</Text>
            <Comments>There are other systems of boxes and arrows that have utility. The most well-known example is probably the suite of diagrams that make up UML.</Comments>
        </Document>
        <Document ID="B3B24A43-1771-40B9-BE0C-5CFCA7474528">
            <Title>Package</Title>
            <Text>Packages are the foundational unit of compilation; there is also an intended semantics:
Package
A package is a collection of definitions that are intended to represent some coherent functionality or purpose.
This is, of course, a vague and non-actionable definition. There are higher-level notions of what ‘functionality’ might mean: for example, in the context of Service Oriented Architecture, a service is the ‘manifestation of a business functionality’. If that is clearer, then a package is a manifestation of an application functionality. Purpose is a topic we will return to in our chapter on agents.
For all of its vagueness, the basic idea of a package is that it is a container for a collection of definitions. Star’s modularity makes it quite straightforward to refactor your package hierarchy should you need to.
The form of a package is
PackageIdentifier { Definitions }
The PackageIdentifier at the beginning of the package definition is the name of the package. It typically consists of a sequence of identifiers separated by periods. Normally a package identifier has little role within the package itself – it is used when referring to the package externally.
Most of the Definitions are those that we have already seen: function definitions, variable definitions and type definitions. There are some additional forms however, including the import statement — that allows one to import elements from other packages.</Text>
        </Document>
        <Document ID="1E9B6C57-88EB-4377-8EF5-A0E0B4F30590">
            <Title>Queries</Title>
            <Text>We have, in Star, a better, more systematic, approach to describing and implementing public APIs – based on speech actions. For example, assuming that Ag had the appropriate type similar to that of A above, we could issue a speech action against Ag with a very similar query:
query Ag with list of { all y where
    (y,"1in-washer") in products and
    quantity(y)&gt;20 }
One of the most obvious differences here being where the target of the API is mentioned: it is only mentioned at the top of the query. The type of Ag, assuming nothing else is known about it, takes the form:
Ag:speech[t-&gt;&gt;myAPI] |: t
This type annotation highlights two important aspects of speech actions as Star program fragments: the entity being queried must implement the speech contract and that the API of the queried entity is also baked into its type – albeit via the speech contract. Actual concrete implementations of speech tend to show the relationship more directly – as we shall see below when we look at [actors].
As we already noted, in classical speech action theory a speech action is a combination of a performative and content. Star supports three performatives: notify which corresponds to a notification that something has happened, query which corresponds to a question, and request which corresponds to a request to perform an action. We have found that these three performatives are sufficient to cover the vast majority of communication requirements in practical software systems.
The general form of the query speech action is an expression of the form:
query Agent with Expression
where Agent is an entity with a constrained type that implements the speech contract.
The query is an expression – how can it be a speech action? The straightforward response is that it is not the expression that is the action, the speech action as a whole consists of the query performative and the content of the action is an expression. Syntactically, because the query has a value, it makes the most sense for the query to be presented to the programmer as an expression.</Text>
            <Comments>And contracts of course.
This is a simplification of the actual speech contract form.</Comments>
        </Document>
        <Document ID="0B40E167-6808-45A5-8494-1FB4867ECB4C">
            <Title>Triple Notation</Title>
            <Text>The focal structure in a triple graph is the triple. This is described as being a triple of a subject/predicate/object. Our triple graph notation is based on the [N3][#N3-W3C] notation — which is a more human readable version of the standard XML RDF syntax adopted by W3C:
Subject ! Predicate $ Object.
The idea is to make it easy to represent facts like:
john ! likes $ mary.
peter ! in_department $ accounting.
We also allow raw strings in our triples, to enable us to represent information that is external to the graph:
john ! address $ "2 smart place".
john ! name $ "John Smith".</Text>
        </Document>
        <Document ID="09FB7E67-CF76-411A-9FC4-B35E400143F7">
            <Title>Is There a Downside to DSLs?</Title>
            <Text>Nothing comes without risks, for every yin there is a yang. What, we should ask, are the risk factors in having a programming language that encourages programmers to create DSLs?
There are three primary areas that we need to point out:
	1.	Programming DSLs is somewhat more difficult than regular programming. This is certainly true: programming with macros requires a certain facility with meta-language. There is much in common between developing compilers and developing macro packages for a DSL. You have to be able to comprehend how to synthesize the appropriate implementations for your chosen DSL. To be fair, developing a macro package is several orders of magnitude simpler than developing a complete compiler: much of the ‘heavy lifting’ has been done for you.
	2.	Risk of programmer confusion. This is the primary reason that languages like Java and C# do not have macro capabilities: it is easy to develop macro packages that end up causing confusion in the mind of your target audience. Our response to this is that macros are not special in this regard; any time that you build a library intended to be shared by other programmers there is a similar risk of introducing a poorly designed API.
	3.	Tooling is harder for languages that support macro extensions. This is a serious issue; especially given the relatively weak technology often given to enable editors to be customized. 
Overall, as we noted above, it comes down to a choice: of the curated garden or the wild democracy. On balance, we believe that the power of being able to craft your own DSL, your own policy framework; or even to be able to rapidly respond to a changing requirement, outweighs the risks associated with being able to define your own macros.</Text>
            <Comments>Editor customization is almost universally based on using regular expressions which are often too weak to handle Star’s syntactic features. 
However, the author recommends Emacs and Sublime Text; in part for their excellent customization features.</Comments>
        </Document>
        <Document ID="982D8A0F-9BE1-4118-B50F-C391F2E4D8A8">
            <Title>Structural Types</Title>
            <Text> A structural type is, informally, a type that looks like a value. For example, the type
(integer,string,employee)
is a tuple type – it denotes the type of a triple of values, consisting of an integer, a string and an employee in this case.
Star has several forms of structural type, the tuple type is one of them; others include record types and function types.
We shall see more of these as we introduce the rest of the language. However, it is worth pausing to ask the question Why? Briefly, nominative types help the programmer focus on what a value denotes; whereas structural types tend to expose what a value can do.
For example, the employee type clearly points to what an employee value is intended to denote (an employee!), but does not help if we want to know what an employee can do. On the other hand, the function type in the annotation:
f : (integer)=&gt;string
clearly indicates what one can use f to do, but it does not indicate anything about why you would want to (except, perhaps, to convert an integer to a string).
In summary, use nominative types when you are modeling ‘real world’ entities and structural types when the focus is on operations and structure more than on what the intention is. In practice, of course, you will use both in some combination.</Text>
        </Document>
        <Document ID="E17E8BB7-8E12-4469-AFEE-ACD12A954D33">
            <Title>Paperback</Title>
        </Document>
        <Document ID="6252F387-63F2-4436-9AD7-F31D328BA95E">
            <Title>Standard URI Schemes</Title>
            <Text>Although the Star compiler can, in principle, utilize any URI scheme, some schemes are ‘built-in’ — in the sense that there is a standard transducer for them. It is obviously an implementation dependent aspect of the language: different compilers may support different schemes; but all should support at least the following:
star
E.g. Star://foo/bar/gar.star. This is intended to be used as a system dependent but universal way of identifying Star files. I.e., each system is free to choose how to find resources using the Star scheme in its own way. It is also expected that methods would be provided for importing and exporting resources between the system and other systems (e.g., mobile vs desktop computers).
http
E.g., http://www.example.com/foo/bar.star. This is intended to denote resources that are accessible in some way from the web.
std
E.g., std:arithmetic.star. This is intended to represent internally important resources — typically Star packages that are part of the standard language definition
In addition, the compiler may support the file scheme and other schemes as appropriate.</Text>
        </Document>
        <Document ID="A3B48DD7-3163-49AF-B1A2-9A4D0C8FE21C">
            <Title>Contracts and Constraints</Title>
            <Text>The concepts of interface and contract are foundational in modern software engineering. This is because explicit interfaces make it substantially easier to develop and evolve systems. A Star contract goes beyond the traditional concept of interface in important ways: we do not mark the definition of a type with its implemented contracts and we allow contracts to involve multiple types.
A contract defines a collection of signatures and an implementation provides specific implementations for those functions for a specific type (or type combination).
For example, we can imagine a contract for simple four function ‘calculator arithmetic’ containing definitions for the basic four functions of addition, subtraction, multiplication and division:
contract all t ~~ four[t] ::= {
  plus : (t,t)=&gt;t.
  sub : (t,t)=&gt;t.
  mul : (t,t)=&gt;t.
  div : (t,t)=&gt;t.
}
This contract defines — but does not implement — the four functions plus, sub, mul and div. All these functions have a similar type:
plus :  all t ~~ four[T] |: (t,t)=&gt;t.
The clause four[t] |: is a type constraint, specifically a contract constraint. So, these functions are generic (universally quantified) but the bound type (t) has the additional constraint that there is an implementation for it.
The four contract defines a set of functions that can be used without necessarily knowing the type(s) that are involved. For example, we can define the double function in terms of plus:
double(X) =&gt; plus(X,X).
Its type is also interesting:
double : all t ~~ four[t] |: (t)=&gt;t.
I.e., it inherits the same constraint as the function plus has. There are several kinds of type constraint in Star’s type system; but the contract constraint is the most significant of them.</Text>
        </Document>
        <Document ID="A274FFA7-B07B-43A7-9756-C5ACCB43A7C4">
            <Title>Notifications</Title>
            <Text>A notify speech action is intended to communicate the occurrence of an event of some form. In the context of software systems it corresponds to a message being sent on a named channel; however that seems low level in comparison.
The form of a notify is a little different to the query:
notify Agent with Expression on Channel
To notify Ag that there is a new stock item might take the form:
notify Ag with
  ("MS-345","3/4in Machine Screw")
  on stock
The content here is the tuple term
("MS-345","3/4in Machine Screw")
and stock is the ‘channel’ of communication. In order for this to be valid we have to assign a new element to Ag’s API interface – one that defines stock and the type of event it may respond to:
type myAPI is alias of {
  products:list of (string,string)
  quantity:(string)=&gt;integer
  stock:occurrence of (string,string)
}
Here we mark the ability to notify on the stock channel by giving stock an occurrence type. Later, when we look at how agents can be implemented, we will see how notifications are handled. For now, we note that occurrence of (string,string) denotes a program that can consume events whose data consists of values of tuple type (string,string).</Text>
        </Document>
        <Document ID="A093493A-FD06-4459-B286-6AAA14A8FF6B">
            <Title>Re-purposing Components</Title>
            <Text>One of the distinguishing features of the boxes-and-arrows diagram is that it highlights the major sources of input and output in the system. This is in marked contrast with most programming languages where the I/O functionality is buried deep within the code itself. For example, our splitComponent does not directly communicate with a database; instead it poses a query to the partsDB component. Similarly, the components for recovering incoming orders and sending out orders to suppliers are mostly about implementing the appropriate I/O operations.
The partsDB component is interesting for another reason: it can be modeled as an adapter to a normal database engine. In fact, the partsDB is quite an interesting component.
Consider the problem of the partsDB component: if we are to build this component in a robust fashion then it must respond to arbitrary queries about the two relations that are exposed in its responding port. However, we assume that the data it uses to answer those queries is not ‘in’ the component itself but is stored in some actual database; perhaps an SQL database.
In order to see how we can achieve this we need to look a little deeper into how speech actions are actually represented. Recall that we stated that a speech action requires an entity that implements the speech contract. That contract, in simplified form, is:
contract all t,a ~~ speech[t-&gt;&gt;a] ::= {
  _query:all s ~~
    (t,(a)=&gt;s,()=&gt;quoted,()=&gt;dictionary[string,quoted])=&gt;s.
  _request:
    (t,(a)=&gt;(),()=&gt;quoted,()=&gt;dictionary[string,quoted])=&gt;().
  _notify:(t,(a)=&gt;())=&gt;()
}
The actual contract is somewhat more involved and involves the use of the execution contract – which is part of the support for Star’s concurrency features. However, this variant is sufficient for us to expose the required issues.
The salient element here is the entry for _query. A speech action like:
query parts with list of { unique S where
    (A,Ps) in assembly &amp;&amp; P in Ps &amp;&amp;
    (P,Ss) in supplier &amp;&amp;
    S in Ss
  }
is translated – by a standard built-in macro processor – into a call to _query of the form:
_query(parts,
  (Ax)=&gt;list of { unique S where
    (A,Ps) in Ax.assembly &amp;&amp; P in Ps &amp;&amp;
    (P,Ss) in Ax.supplier &amp;&amp; S in Ss },
  ()=&gt;&lt;|list of { unique S where
      (A,Ps) in assembly &amp;&amp; P in Ps &amp;&amp;
      (P,Ss) in supplier &amp;&amp; S in Ss
    }|&gt;,
  ()=&gt;["A"-&gt;A::quoted]
)
This is kind of complicated to follow at first; but is quite straightforward if taken one step at a time.
	•	The first argument to _query is the entity being queried with the query speech action. The structure of the speech contract allows a responder to a speech action to ‘pass on’ the action to another responder.
	•	The second argument is the expression that must be evaluated by the actual responding entity. It is encapsulated in a single argument function – the argument being a record that implements the speech action API. Evaluating this function in the appropriate context has the effect of computing the response to the query speech action – and the result of the function is returned as the value of the query speech action itself.
	•	The third and final arguments are used when the responder cannot or does not wish to use the ‘compiled query’. The third argument is a function that returns the original ‘text’ of the query – as a quoted value. The expression: &lt;|list of { unique S where (A,Ps) in assembly &amp;&amp; P in Ps &amp;&amp; (P,Ss) in supplier &amp;&amp; S in Ss }|&gt; has, as its value, an AST term – of type quoted. This form allows a receiver of the request to inspect the query and determine for itself how to interpret it. This is analogous to applications processing queries expressed in terms of Json or XML; except here we allow Star’s syntax.
	•	The fourth argument is also a function, one that returns the values of any free variables in the query – variables whose values are determined by the context of the speech action itself. In this case there is only one free variable – A – which was the part that our splitComponent needed to find suppliers for.
This arrangement allows for two kinds of speech action processing: if the respondent trusts the originator of the speech action then a very rapid response to the query is possible – by evaluating the embedded function. However, in the case where the respondent does not want to simply execute the query function, or it cannot, then the respondent has access to the original text of the query – together with the values of any free variables that appear in the query.
In the case of the partsDB component it cannot simply trust the query; even if it wanted to. This is because the query must be mapped into a form that the attached SQL database can understand. In effect, the query must be translated from Star’s query language to SQL.
#
The PartsDB Component
I.e., the query expression is translated by the partsDB component into SQL:
select S from table assembly as As, supplier as Sp, Ps, Ss where
  As.part="Alloy Wheel" and Ps.partno=As.partno and
  Ps.supplier = Sp.id and
  Ps.supplier=Ss.id and Ss.supplier=S
The details of how this translation are achieved are beyond the scope of this book; but it involves similar techniques to those we saw in [our chapter on DSLs][application-policy-mechanism].
One important point to note here is that the machinery for translating queries into SQL is quite general; and not at all restricted to our parts database. In fact, if we provide the actual database component with a URL of the database we are interested in, the DSL processor is able to dynamically inspect the actual database, construct the appropriate responder port for the partsDB component as well as being able to answer queries by mapping them to SQL.
As a result, we can construct a general purpose adapter component that is able to be used for any database, not just the one we are using in this application. The result of which is that the application is populated with a mixture of standard components that are configured and specifically written components that implement the specific functionality of the application.</Text>
            <Comments>This is not the final form, we have chosen to not macro-process the query expression itself. In practice, the compiler would also convert the query into an executable form – see Chapter 4.</Comments>
        </Document>
        <Document ID="42BCB097-B93B-405B-BC9E-4ED21CEBE1CC">
            <Title>Resource Definition Framework</Title>
            <Text>RDF semantics is extremely simple: a RDF graph is a set of triples consisting of a subject, predicate and object; each of which is a concept. A graph is constructed by having triples linking to each other: the object of one triple being the subject of another. RDF is particularly flexible here as even predicates may be the subjects and objects of other triples.
RDF is good for representing the simple ‘facts’ that one often sees in applications that have to model aspects of the real world. A classic example of this is in modeling things like giraffes: if we have a Giraffe called Joe, then we typically want to be able to say things like:
Joe isa giraffe
which means
Joe is a Giraffe
RDF is not especially powerful — which is actually one of the key design points in the language. Its simplicity means that it is easily manipulated and processed.
Another nice feature of RDF is its extensibility by means of special vocabularies. One of the basic standard vocabularies is RDFS which introduces a languages of classes, sub-classes and instances to basic RDF. In our giraffe example, the RDFS vocabulary allows us to talk about categories as well as individuals:
Joe isa giraffe
giraffe isa class
giraffe subclass mammal
giraffe has long-neck
mammal isa class
mammal subclass animal
mammal has four-legs
RDF stores that understand this vocabulary can automate many simple inferences. Another standard vocabulary is OWL which is the standard for representing Ontologies in the Semantic Web. For example, from these facts, we can also infer:
Joe has long-neck
Joe has four-legs
These inferences are part of the semantics of RDFS; they are added to the graph as a consequence of the other facts. A graph simply consists of a set of such ‘triples’; often there are many millions of triples when modeling complex domains.
Just having a graph is much like having a flat data structure; its utility for describing policies comes from being able to process it effectively, specifically to be able to query it. There are many possible ways of querying triple graphs; will look at a simple one which can be easily integrated with Star’s standard query notation.
It is popular to use a meta-syntax such as XML and JSON to encode policy expressions. The fundamental problem with using these is that they do not easily support type consistency and programming languages typically do not give good integration between XML/JSON strings and program code. The net effect is one of clumsiness and needless complexity.</Text>
        </Document>
        <Document ID="3712FE62-6826-4887-86D1-35C85322C9FD">
            <Title>A Message Queue</Title>
            <Text>We start by defining a type that denotes the interface to the queue. For convenience, we use a type alias to a record with a post function and a poll function within it:
type messageQ of a is alias of {
  post:(a)=&gt;task of ()
  poll:()=&gt;task of a
}
In effect, the record type is a specification of an API for accessing a message queue.
One might ask why do we use task oriented types like task of () and task of a types? I.e., why wrap the type of the message in a task type?
The messageQ is likely to be used in the context of task expressions; and certainly its implementation requires tasks. By exposing this task-nature we actually improve the potential responsiveness of the message queue and of functions that use it.
Like the semaphore that we saw earlier, we will build the message queue using an internal background task that manages the actual queue of messages.
msgQ:for all a ~~
  ()=&gt;messageQ of a
fun msgQ() is let{
  def postMsgChnl is channel()
  def grabMsgChnl is channel()

  fun qLoop(Q) where isEmpty(Q) is wait for postM(Q)
   |  qLoop(Q) default is wait for postM(Q) or pollM(Q)

  fun postM(Q) is
    wrapRv(recvRv(postMsgChnl), (A) =&gt; qLoop([Q..,A]))

  fun pollM([A,..Q]) is let{
    fun reply(R) is valof{
      perform send(R,A)    -- reply on the one-time channel
      valis qLoop(Q)
    }
  } in wrapRv(recvRv(grabMsgChnl), reply)

  { ignore background qLoop(queue of []) }
} in {
  fun post(A) is wait for sendRv(postMsgChnl,A)
  fun poll() is task{
    var ReplyChnl is channel()
    perform wait for sendRv(grabMsgChnl,ReplyChnl)
    valis valof (wait for recvRv(ReplyChnl))
  }
}
This code is quite complex in places. However, if we break it down and examine it piece by piece its secrets will be exposed. Let us start with the qLoop/postM/pollM triumvirate. These three functions form the heart of the message queue’s mechanism.
The qLoop function has a similar structure to the semLoop function we saw in the semaphore. This time, instead of deciding what branch to take based on a resource counter, we decide based on the size of the actual queue of data.
The queue data type is a builtin type to Star. It implements a queue functionality allowing elements to be posted at one end and removed from the other. Like other collection types, the sequence contract is implemented for it; which means that we can use sequence notation when writing queue expressions and patterns.
The postM function — like its cousin the releaseM function in the semaphore — is a rendezvous-valued function that the qLoop function will wait for. It’s inner core is a recursive call to qLoop with a queue that is enhanced with the new work item.
The pollM function has a similar structure; but it is more complex because — in addition to setting up the next qLoop recursion — we also want to export the entry from the queue itself.
Polling from the message queue is accomplished by sending a special one-time channel to the message Q task. Somewhat paradoxically, the poll function asks for the next element of the message queue by sending the one-time channel to the message queue’s grabMsgChnl:
fun poll() is task{
  def ReplyChnl is channel()
  perform wait for sendRv(grabMsgChnl,ReplyChnl)
  valis valof (wait for recvRv(ReplyChnl))
}
The first two actions in the task expression of the poll function are fairly normal: we create a channel, and send that on the grabMsgChnl channel. The third line bears some more examination: what exactly is that valis valof doing?
The simple answer here is that this is a task-valued function, and the value of the wait for expression is also a task — it happens to be the task that we want to return. But, normally, a task expression that performs a valis E action would have type
task of Et
so, apparently, we take the value of the task returned by the wait for and rewrap it back as a task!
In fact, this particular combination is a part of the overall task notation: instead of performing this unwrap and rewrap, the value returned by the wait for will be returned directly without necessarily waiting for the task to finish. We can do this safely because the overall value of the sequence is also a task.
There is a further complication here. Recall that the task notation is based on monads. There are potentially many kinds of monads and the valis valof combination allows us to change monads. This, however, is a topic beyond the scope of this book.
We can use our message queue to knock up a quick simulation of a system of workers and a work queue:
worksheet{
  ...
  fun sender(Q) is task{
    for i in range(0,1000,1) do
      perform Q.post(i)
    sleep(3000L)
    logMsg(info,"all done")
  }

  fun worker(W,Q) is task{
    while true do{
      def Nx is valof Q.poll()
      logMsg(info,"$W doing $Nx")
      sleep(random(100L))
    }
  }

  def MQ is msgQ()
  ignore background worker("alpha",MQ)
  ignore background worker("beta",MQ)

  ignore valof sender(MQ)
}
This example spins up 1000 work items and expects the "alpha" and "beta" workers to process them. Typical bosses attitude!</Text>
        </Document>
        <Document ID="6F897244-F56F-47D7-8853-7F9B193D15EA">
            <Title>Design is Iteration</Title>
            <Text>As we noted, having a triple graph is only as useful as our ability to use it; especially to examine its contents. To simplify that task we could create a query language specifically for triple graphs. However, a more subtle and integrated approach is to extend the standard built-in query language with conditions that are tailored to work with triple graphs. This has the advantage that we can leverage standard query processing to help process triple graphs.
So, we need an extension to standard query conditions that allow us to:
set of { all X where OrgTree says X ! department $ accounting }
However, we immediately hit a road-block: what about variables? How can we distinguish the occurrence of X above to denote a variable whilst allowing accounting to be the fixed name of a concept: they are both identifiers.
We are pretty much guaranteed to encounter variables in query expressions; but we may also encounter them in graph expressions — especially if we construct graphs dynamically. To ensure that we can reliably distinguish variables from named concepts we have to modify the design of our triple graph language.
So, to resolve this, we modify the syntax for named concepts to have a leading colon operator; our simple graph above becomes:
:john ! :likes $ :mary.
:peter ! :in_department $ :accounting.
and the query becomes:
set of {all X where OrgTree says X ! :department $ :accounting }
Now the occurrence of X above is clearly a variable and not the name of a concept.
There is an additional reason for choosing to lead a concept with a colon: when dealing with multiple graphs it may be useful for one graph to refer to a concept in another graph. However, we will leave this extension for another day.
With this change, we can represent our original giraffe example in our RDF DSL:
:Joe ! isa $ :giraffe.
:giraffe ! isa $ class.
:giraffe ! subclass $ :mammal.
:giraffe ! has $ :long-neck.
:mammal ! isa $ class.
:mammal ! subclass $ :animal.
:mammal ! has $ :four-legs.
With the inferred triples:
:Joe ! has $ :long-neck.
:Joe ! has $ :four-legs.
Designing DSLs, like designing anything, is often an iterative process. It is important to remain flexible when designing language features. For example, one rather glaring limitation to our triple graph notation is that it is limited to symbolic concepts and literal strings. We leave as an exercise to the reader how to modify the notation to allow arbitrary Star values to be referenced in triple graphs.</Text>
        </Document>
        <Document ID="C98C8AA1-5817-4DB6-A596-D82B2E178854">
            <Title>Implementing Contracts</Title>
            <Text> Defining a contract is a big step, but it is not generally sufficient to produce working programs. If we had a worksheet containing only:
worksheet{
  contract all t ~~ four[t] ::= {
    plus : (t,t)=&gt;t.
    sub : (t,t)=&gt;t.
    mul : (t,t)=&gt;t.
    div : (t,t)=&gt;t.
  }
  double : all t ~~ four[t] |: (t)=&gt;t.
  double(X) =&gt; plus(X,X).

  show double(2)
}
we would get a compiler error along the lines of:
2:integer
  which is not consistent with
    pPrint[t_12] , four[t_12] |: t_12
  because four[integer] not known to be implemented
This error message is effectively warning us that we have defined the four contract but we have not implemented it. Until we do, the program is not complete. However, if we do supply an implementation of four over integers:
worksheet{
   contract all t ~~ four[t] ::= {
    plus : (t,t)=&gt;t.
    sub : (t,t)=&gt;t.
    mul : (t,t)=&gt;t.
    div : (t,t)=&gt;t.
  }
  double : all t ~~ four[t] |: (t)=&gt;t.
  double(X) =&gt; plus(X,X).

  implementation four[integer] =&gt; {
    plus(x,y) =&gt; x+y.
    sub(x,y) =&gt; x-y.
    mul(x,y) =&gt; x*y.
    div(x,y) =&gt; x/y.
  }
  show double(2)
}
then everything works as expected.
Notice that the error message above shows that type t_12 actually has two type constraints:
pPrint[t_12] , four[t_12] |: t_12
This is because the show action also results in a type constraint being involved. The pPrint contract is used to display values in a number of circumstances; including the string formatting we saw above.
As may be expected, arithmetic itself is also mediated via the arithmetic contract in Star. This is how we can support multiple numeric types using a common set of operators: there are standard implementations of arithmetic for integers, and floating point numbers.</Text>
        </Document>
        <Document ID="3D707F9E-6E7B-4A2B-BF04-64BD98EC07A8">
            <Title>Notes</Title>
            <Text>&lt;$--ENDNOTES--&gt;</Text>
            <Notes>The &lt;$--ENDNOTES--&gt; tag will be replaced by the footnotes during Compile. Using this tag allows us to have the footnotes inserted wherever we like, without having a separator placed above them.</Notes>
        </Document>
        <Document ID="596015F8-8E60-4786-891E-5D8F26BC0F29">
            <Title>The range Type</Title>
            <Text>The range type is a very particular form of collection type: a range denotes a range of numbers. Its type description says it all:
all t ~~ arithmetic[t],comparable[t] |: range[t] ::= range(t,t,t).
This type description has some special features; in particular it is a constrained type: a type expression of the form:
range[T]
is only valid if T is an arithmetic type; specifically a type that supports arithmetic and is comparable. Thus a type expression such as range[integer] is fine, but range[list[integer]] will result in a syntax error!
Range terms are used to compactly represent regular ranges of numbers; for example the term
range(0,100,1)
denotes the first 100 integers. But, of course, we have already seen the range collection in our exploration of the sieve of Erastosthenes.
There is a specific property of the range type that is difficult to capture with this type definition — specifically, we rely in many places on ranges half closed property: that is, the range of numbers in the range include the first number but does not include the second. This property makes combining ranges much smoother than either a fully closed range (includes both ends) or an open range (includes neither end).
For example the following assertion is expected to hold for range terms:
range(F,I,Ix)++range(I,T,Ix)=range(F,T,Ix)
where ++ is the standard function — i.e., is part of the concatenate contract — for expressing sequence concatenation.</Text>
        </Document>
        <Document ID="302DB081-7E23-4C26-B524-83C54A155EEF">
            <Title>There is Methodology in my Ontology</Title>
            <Text>The most important initial task in designing a DSL or policy framework has nothing to do with operators, macros or any of the facilities of Star — you must identify the appropriate structure and semantics for your language. More formally, this can be defined as identifying the ontology of the language.
Ontology, as a discipline, dates back to Aristotle. He was one of the first people to attempt to systematically classify the known world. Nowadays, Ontology refers to the study of the relationship between concepts and the real world.
Ontology 
A description of a set of concepts and how they relate to each other and to the intended domain.
A systematic, or formal, ontology is often written in a language that allows automated verification of desirable aspects of the ontology — such as its consistency.
Developing an ontology often starts by brainstorming a set of words that seem to be important in the domain. For example, an ontology of pet animals would include the concepts of Dog, Cat, Giraffe and so on. It would likely also include the concepts of caring for a pet and of breeding them.
Our RDF triple language also has an ontology — about the concepts involved in knowledge graphs. The figure below shows such a collection, which, while it is not the complete set needed for conceptualizing triple graphs, makes a good start:
#
A Collection of Concepts Important to Triple Graphs
Once you have a set of concepts it helps to organize them. Common meta-concepts in ontologies include the notions of class, instance, and the sub-class relationship — not to be confused with similar concepts in Object Oriented Programming. For example, your giraffe Joe denotes an instance of the Giraffe class, and Giraffes are a sub-class of the more general concept of Pet.
One simple (sometimes simplistic) tool for figuring out an ontology is UML. UML has the great advantage of being very visual and therefore can often be useful in bringing out the salient parts of an ontology. For example, a UML diagram of our triple graph conceptualization highlighting some of the important classes and their relationships can be seen in:
￼
UML Diagram of Triple Graph Ontology
The role of the UML diagram is to make it clear what the key features and components of your language are — so you don’t miss anything important.
The careful reader will notice that we have actually extended the ontology to include the concepts of class, cub-class and instance; which are actually part of RDFS more than basic RDF.</Text>
            <Comments>The ‘official’ definition of an ontology is a systematic conceptualization of a domain.</Comments>
        </Document>
        <Document ID="030AF59B-1E07-410E-9D10-D5CC52BDDD3E">
            <Title>Private and Public Imports</Title>
            <Text>By default, when a package is imported, it is privately imported – the contents of the package are not automatically re-exported. This means that if a package implicitly re-exports something from a package (such as a type) then when you import the package you must also import the dependent packages.
However, when constructing a library, which itself may be built from more than one package, it can become tedious to require clients to import all the constituent packages of the library.[It also exposes the structure of the library to clients; something that is typically not desirable.]
In order to facilitate the construction of libraries, and larger scale packages, we allow for so-called public imports.
A public import is written:
public import fooPkg.
This will have the effect of re-exporting elements of fooPkg as though they were directly exported (made public) by the current package.
One common pattern for specifying libraries – that are made of multiple packages – is though a stub package that just consists of a sequence of public imports:
libraryPkg{
  public import part1.
  public import part2.
  ...
}</Text>
        </Document>
        <Document ID="8FEA6DDC-BA04-412F-8BEE-F576C93C1387">
            <Title>Getting hold of Star</Title>
            <Text>The easiest way to access the compiler is to pick up the released code from Github.  
Assuming that you have downloaded the release files, assuming that you have your Star files in the current directory, you can compile and run a Star program using:
$ Star fact.star 10
This will compile the file fact.star and run it, passing the integer 10 to the embedded program.
The first time you run the compiler it will be a little slow. This is because it also compiles the standard library into the sub-directory ./starcode/. Subsequently, the compiler will be significantly quicker.
The Star compiler generates Java byte code, which means that it relies on the JVM platform for its execution. It also means that integrating Star code with Java code is straightforward. For details of how this works we recommend the Star Language definition.</Text>
        </Document>
        <Document ID="B9CFA6CB-3E0D-4D62-A2E9-EFFA705EB7DC">
            <Title>A Word About Type Inference</Title>
            <Text>We have seen some powerful forms of types in this chapter: recursive types defined using algebraic type definitions, generic types and even function types. Recall also that Star only requires programmers to explicitly declare the types of top-level variables and functions. It is worth pausing a little to see how this might be done.
Recall our original factorial function:
fact(0) =&gt; 1.
fact(N) =&gt; N*fact(N-1).
The compiler is able to compute the types of the various variables automatically through a process known as type inference. Type inference may seem magical, but is actually (mostly) quite simple. Let us take a look at the expression:
N-1
which is buried within the recursive call in fact. Although it looks like a special operator, Star does not treat arithmetic expressions in a special way; the - function is just a function from numbers to numbers; its type is given by:
  (-) : all t ~~ arithmetic[t] |: (t,t)=&gt;t
However, we should simplify this type a little in order to make the explanation of type inference a little simpler. In what follows, we assume that the type of (-) is:
(-) : (integer,integer)=&gt;integer
Type inference proceeds by using special type inference rules which relate expressions to types, in this case the applicable rule is that a function application is consistent if the function’s parameter types are consistent with the types of the actual arguments. If they are consistent, then the type of the function application is the return type of the function.
The type inference process initially gives every variable an unknown type — represented by a new type variable not appearing anywhere else. For our tiny N-1 example, we will give N the type tN.
The (-) function has two arguments whose types can be expressed as a tuple of types:
(integer,integer)
and the types of the actual arguments are also a tuple:
(tN,integer)
In order for the expression to be type correct, the actual types of the arguments must be consistent with the expected types of the function; which we can do by making them the same. There is a particular process used to do this — called unification.
#
Inferring the Type of N-1
Unification
An algorithm that replaces variables with values in such a way as to make two terms identical.
Unification matches equals with equals and handles (type) variables by substitutions — for example, we can make these two type expressions equal by binding the type variable tN to integer.
We initially picked the type of N to be an arbitrary type variable, but the process of checking consistency leads us to refine this and make the type concrete. I.e., the use of N in a context where an integer is expected is enough to allow the compiler to infer that the type of N is indeed integer and not tN.
Of course, if there are multiple occurrences of N then each of those occurrences must also be consistent with integer; and if an occurrence is not consistent then the compiler will report an error — a given expression may only have one type!
The bottom line is that Star’s types are based on a combination of unification for comparing types and a series of type rules that have the effect of introducing constraints on types based on which language features the programmer uses. The type checker is really a constraint solver: if the constraints are not satisfiable (for example by trying to ‘call’ a variable and add a number to it) then there is a type error in the program.
The magic of type inference arises because it turns out that solving these constraints is sufficient for robustly type checking programs.
A sharp-eyed reader will notice that Star’s type system is different in nature to that found (say) in OO languages. In Star’s type system, types are considered to be consistent in Star if they are equal. This is quite different to the notion of consistency in OO languages where an argument to a function is consistent if its type is a sub-type of the expected type.
However, we would note that the apparent restriction to the type system imposed by type equality is much less severe in practice than in theory — and that OO languages’ type systems also incorporate some of the same restrictions.
We are only able to scratch the surface of the type system here. It is certainly true that — like many modern functional languages — Star’s type system is complex and subtle. The primary motivation for this complexity is to reduce the burden for the programmer: by being able to infer types automatically, and by being able to address many programming subtleties, the type system comes to be seen as the programmer’s friend rather than as an obstacle to be ‘gotten around’.</Text>
            <Comments>We put the (-) in parentheses to highlight the use of an operator as a normal symbol.
This is a slight over-simplification.</Comments>
        </Document>
        <Document ID="B5BAD6CD-B8F5-469F-9D65-288D1D5001DD">
            <Title>Dedication</Title>
            <Text>




For the Family.</Text>
            <Notes>Feel free to delete this document if you don’t need it.</Notes>
        </Document>
        <Document ID="2AAECE44-6809-4086-BC47-48067A961C78">
            <Title>Contract Constrained Types</Title>
            <Text>We noted above that the type of the standard equality predicate was ‘almost’ the same as:
all x ~~ (x,x)=&gt;boolean
This type denotes a universally quantified function type that can be applied to arguments of any given type. However, equality in a normal programming language is not universal: not all values admit to being reliably tested for equality. A great example of such a limitation are functions – equality between functions is not well defined.
To capture this, we need to be able to constrain the scope of the quantifier; specifically to those argument types that do admit equality.
We can do this by adding a contract constraint to the type – the constraint states that equality must be defined for the type. We do this by prepending a constraint clause to the type:
all x ~~ equality[x] |: (x,x)=&gt;boolean
The equality[x] |: clause states that the type variable x must satisfy the equality contract.
What is implied when we state this? This is captured in the definition of the contract itself, in this case the equality contract.
The equality contract is defined using a contract statement:
contract all t ~~ equality[t] ::= {
  (==) : (t,t) =&gt; boolean.
  hash:(t)=&gt;integer
}
In effect, the equality contract states that there must be two functions defined – == and hash.
If this seems a little circular, it is not. The equality contract is effectively saying that the == function must be defined for the type; and we constrain function (and other) types with the equality contract constraint when we need to ensure that == is defined!
We provide evidence for contracts through the implementation statement. This declares that a given type satisfies a contract by providing implementations for the functions in the contract.
For example, we can provide evidence that the equality contract applies to strings using a built-in primitive to actually implement equality for strings:
implementation equality[string] =&gt; {
  s1 == s2 =&gt; _string_eq(s1,s2).
  hash(s) =&gt; _string_hash(s).
}
We shall explore more fully the power of this form of type constraint in later sections and chapters. For now, the core concept is that quantified types can be constrained to allow very precise formulations of types.</Text>
            <Comments>Strictly, not decidable.</Comments>
        </Document>
        <Document ID="4DAE20E1-E8B4-402B-BA96-7067A8A89451">
            <Title>Ebook</Title>
        </Document>
    </Documents>
</SearchIndexes>