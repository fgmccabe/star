Title:	Cafe Programmer  
Author:	Francis G. McCabe  
Bibliography Title:	Bibliography
BibTex:	Cafe.bib

# Why Be a Cafe Programmer

This book is about a new programming language called **Cafe**. Why, you may ask, do you need to learn yet another language? We hope to answer that question and more in the course of this book. We also aim to show you how to program effectively in **Cafe** to solve real world problems.

##Programming has Changed

###Real-time is normal time

Many kinds of business are becoming more and more 'real-time': a 100 millisecond slowdown in loading a web page can mean the loss of 5% of revenue for an e-commerce site; an unrented car, like an unrented hotel room, represents a permanent loss of business and a competitive disadvantage.

For the modern programmer, this means that applications must be engineered from the start to be responsive and multitasking -- aspects that challenge even the most professional of programmers.

###Programming Safely and Effectively
At the same time, safety and security are also critical: no-one likes to have their private information exposed to the bad guys. Most main-stream programming languages were designed in an era when safety was not uppermost in programmers' minds -- usually it was performance. Some seemingly trivial design choices -- such as C's conventions for laying out strings in memory -- turn out to have potentially devastating security implications.

In addition, systems that are built assuming a shielded execution environment, behind closed doors as it were, are often actually expected to perform in the full glare of the Internet. Hardening programs so that they stand up to that glare can often dramatically add to the cost of development -- both in time and in money.

###This Train is Leaving the Station

Perhaps most importantly, we need to be able to do these things _now_ -- time to market is a critical factor in many if not most modern applications. Its no good developing the world's best widget if you run out of 'runway' trying to build it.

A major bottleneck is the relative poor productivity of most modern programming languages. It is simply too hard to produce correct robust code in languages like C/C++, Python etc.

Productivity is an issue for individual programmers but is especially salient for programmer teams.

>Every successful software project involves a team.

The requirements for team-based development tend to put certain aspects of programming language design into sharp focus. For example, strong types and clear interfaces may be excellent aids for individual programmers but they are absolutely paramount for team development.

>More generally, in a competitive environment, the only way to reliably out-perform the competition in reaching the market is to use radically more productive technology.

##Is Cafe for you?

Choosing a programming language -- when you actually have a choice -- is highly personal. Here are some reasons to think about **Cafe**.

###If You Are Already a Functional Programmer

You have many choices for functional programming languages that are excellent. The author considers two languages that are principal sources of inspiration for many of the functional features of **Cafe**: Haskell and Standard ML -- both of which are excellent; but not perfect.

For the functional programmer, the principal benefits of **Cafe** are _readability_, _modernity_ and _predictability_.

One of the major drivers of the design of Haskell and (to a lesser extent) ML is conciseness. However, conciseness is not the same as readability. In modern software development environments there are many stakeholders beyond the developer. Having a language that is easy to follow by non-technical readers is a major benefit in mixed skill teams.

Like Haskell, **Cafe** has a powerful type system. **Cafe**'s type system has many features in common with Haskell's type system -- features that typically go beyond the capabilities of many OO languages. In particular, **Cafe**'s contract system is reminiscent of Haskell's type classes; and **Cafe**'s existential and higher-kinded types give considerable expressive power to the programmer.

**Cafe** does not follow all of Haskell's type features; and some type concepts are rephrased into terminology that is more familiar to OO programmers.

Like ML, **Cafe** has a powerful module system. However, unlike ML's functors, **Cafe** modules are first class values. This means that there is no artificial separation between 'ordinary' programs and 'functor' programs.

The result is a balanced set of type features that provides capabilities that scale well from small programs to large systems.

**Cafe**'s evaluation is, like that of ML but unlike Haskell, strict. We believe that that makes it significantly easier to reason about the actual behavior and performance of programs. However, **Cafe** has a rich set of features that support productive concurrent and parallel programming -- based on a combination of system threads and the features of Concurrent ML.

Like ML, **Cafe** is not a strictly 'pure' language. This was neither an accident nor an afterthought. Computer systems are built to fulfill purposeful activity (although there may be many times when the actual purpose is hard to discern). For example, if I deposit a check into my bank account, I require that the bank's state is updated to reflect my new balance: the world has changed as a result of my action.

However, the converse does not follow: just because the world is stateful does not mean that all our programs should be needlessly stateful. Much, if not most, of a given application program can and should be crafted in a mathematical style -- the merits of functional programming are very great.

Overall, the primary rationale in the design of **Cafe** is to empower the programmer in making obviously correct programs.

###If You Are Already a Java (or C#, or C++) Programmer

Most OO languages are embracing some of the simpler features of functional languages. Even Java 8 with its lambda expressions and stream features represents a nod to the power of functional programming.

However, at the same time, there is a substantial gap in the capabilities of most OO languages compared to modern functional programming languages. This is a problem because the better a language is able to 'understand' your objectives, the better the tools will be able to support those objectives.

Fundamentally, OO languages revolve around _nouns_ rather than _verbs_. Verbs (methods) are relegated to being inside the scope of some noun (object): they are not first class. However, this leads to unnatural representations where functions do not naturally fit inside some class. In functional programming languages, like **Cafe**, there is more of an balance between nouns and verbs.

Functional languages allow a more subtle interplay possible between data and functions. It is possible to have functions that are about data; it is also quite straightforward to have data structures with functions embedded in them. In fact, a simple definition of a _module_ is a record that contains functions in it.

While OO languages like Java provide excellent _data abstraction_ tools, there are much less effective for _control abstractions_. The result is that OO languages are 'stuck in the 1970s' when it comes to control abstractions. However, concepts such as map/reduce, computation expressions, and continuations bring a rich suite of new control possibilities that solve important problems in modern programming.

Similarly, the type systems of languages like Java (or C# or C/C++) make are not as expressive or sensitive as modern type systems in functional languages can be. Professional programmers will recognize the typical response to insufficiently expressive types: lots of casting and dynamic meta-programming. But, while powerful, these techniques amount to giving up on types and their important advantages. Furthermore, contrary to many programmers' expectations, a modern type system is quite capable of dealing with scenarios that require dynamic programming in languages like Java.

In addition, **Cafe**'s extensibility directly addresses a real world requirement: that of supporting a separation of policy from mechanism. Typically OO languages are excellent at describing mechanisms but do not fare so well in describing policies -- which are typically declarative in nature. Hence the tendency in software engineering circles to adopt text frameworks such as XML and JSON to express policies. However, these technologies lose one of the major advantages of Java -- that of type safety.

###Technology

The technology platform that programs are written for is also changing. Just a few decades ago most computers were single-core; nowadays most computers are multi-core and are capable of significant parallelism.

Especially spectacular is the parallelism available in modern GPUs; where a high end graphics processor may have thousands of cores capable of processing instructions in parallel. We expect that the days of personal computers with thousands of cores is not too far in the future.

Programming parallel machines with conventional languages is an exercise in frustration. This is because programming models that worked in single core computers do not scale well to highly parallel machines. One of the primary reasons for this is that state -- as represented by the changing values of variables -- is _implicit_ in procedural and object oriented languages. The implicitness of state is important because it makes many programs easier to express. On the other hand, that implicitness becomes a liability in multi-threaded and parallel situations where state is no longer so well behaved.

However, **Cafe** has adopted some of the recent innovations in that make dealing with multi-tasking and parallel execution easier. These innovations layer on top of basic features such as threading and provide simpler models of execution than 'conventional' threaded models. **Cafe**'s computation expressions combine the best of fork-join queues and map-reduce frameworks whilst enabling a more normal style of programming.

##Design Goals for Cafe

**Cafe** is a multi-paradigm high-level _symbolic_ language. It is designed to be readable, accurate, high performing and extensible.

The syntax of **Cafe** is oriented towards readability rather than strict conciseness. The reason for this is that the programmer is only one of the stake holders in a given program. A readable program is one that is more easily trusted by non-programmers.

>Experience also suggests that readability enhances programmer productivity also: much of team-based development involves comprehending and modifying other programmers' code.

**Cafe** is a strongly, statically typed language. The purpose of a strong type system is to facilitate the communication of intent of the programmer. The purpose of static typing is to ensure that the compiler can rapidly 'fail' incorrect programs without requiring the program to be run. Furthermore, static type checking minimizes any run-time penalty for imposing type constraints.

Although **Cafe** is strongly typed, it uses _type inference_ to eliminate most of the clutter that some type systems impose on the programmer -- which itself is a productivity sink of course.

Generally, the _stronger_ the type system, the more the language system can detect errors before programs are run. In parallel, the more _expressive_ the type system is, the less the temptation to try to subvert or bypass the type system.

**Cafe** has a range of features that make exploiting parallelism easier to manage. For example, it has support for _computation expressions_ and _actors_. Partitioning an application into different _agents_ allows programming to follow a more human approach. Computation expressions allow the programmer to manipulate computations as easily as they do data values; that in turns greatly eases the development of parallel and concurrent applications.

There is no one technology that can solve all problems. This is as true for programming as for other domains. **Cafe** supports a range of programming paradigms that allows the developer to 'use the best tool for the job'. However, we go beyond this 'swiss army knife' stance and make it straightforward to extend the language.

Virtually every non-trivial program can be factored into a combination of general purpose mechanism and specific policy for applying the mechanism. **Cafe** has powerful self-extension features that allow programmers to design their own policy structures (a.k.a. domain specific languages).

Many of **Cafe**'s own features -- such as its query notation and its actor notation -- are built using these extension mechanisms.

##About this book

This book acts as an introduction to the language and to its use. The basic features of the language are introduced; however, this is not a reference manual: it is not intended to be a complete description of the language.

Introducing a programming language like **Cafe** can be a challenge in presentation. This is because there is a significant amount of mutual support between elements of the language.

Our strategy is to take a layered approach -- we start with simple examples, occasionally skipping over certain aspects of the language without explanation. Later chapters focus on deeper, more complex topics.

For the most part, examples in the text of the book are executable. You are encouraged to try to get them running on your own system.

###Getting hold of **Cafe**

The easiest way to access the compiler is to pick up the released code from [Github](https://github.com/fmccabe/Cafe/releases). You can also pick up the source of the compiler from [here](https://github.com/fmccabe/Cafe).

The main files in the Cafe system are:

*  `star100.jar` This is a Java jar file that contains everything needed to compile and run **Cafe** programs. You will need an installed copy of [Java 8](http://www.oracle.com/technetwork/java/javase/downloads/index.html) in order to run the compiler.

*  The `Cafe` shell script is a convenient entry point to using the **Cafe** compiler. You may need to edit this; it assumes that the jar file is located at `~/bin/star100.jar`.

*  The `reference.pdf` file is the official language definition.

Assuming that you have downloaded the release files, assuming that you have your Cafe files in the current directory, you can compile and run a **Cafe** program using:

```
$ Cafe fact.Cafe 10
```
This will compile the file `fact.Cafe` and run it, passing the integer 10 to the embedded program.

>The first time you run the compiler it will be a little slow. This is because it also compiles the standard library into the sub-directory `./starcode/`. Subsequently, the compiler will be significantly quicker.

The **Cafe** compiler generates Java byte code, which means that it relies on the JVM platform for its execution. It also means that integrating **Cafe** code with Java code is straightforward. For details of how this works we recommend the [Cafe Language definition](https://github.com/fmccabe/Cafe/releases/download/v101RC1/reference.pdf).

###Typographical Conventions

Any text on a programming language often has a significant number of examples of programs and program fragments. We show these using a typewriter-like font, often broken out in a display form:

```
P has type integer;
...
```

We use the `...` ellipsis to explicitly indicate a fragment of a program that may not be syntactically correct as it stands.

>As we noted above, **Cafe** is a rich language with many features. As a result, some parts of the text may require more careful reading, or represent comments about potential implications of the main text. These notes are highlighted the way this note is.

##Acknowledgements

No man is an island, and no project of this scale is one person's work. I have had the great fortune to be able to develop **Cafe** in the context of real world applications solving hard problems. Individuals have also played a large role; and it can be hard to ensure that all are properly acknowledged: please forgive any omissions.

Of particular significance, I would like to thank Michael Sperber for our many discussions on the finer topics of language design; and for his not insignificant contributions to the implementation itself.

I would also like to thank my colleagues at Starview inc., in particular Steve Baunach and Bob Riemenschneider who were the world's first **Cafe** programmers! In addition, I would like to thank David Frese and Andreas Bernauer who helped with crucial parts of the implementation of the concurrency features. I would also like to thank Keith Clark, Kevin Cory, Prasenjit Dey, Chris Gray, Mack Mackenzie, and Kevin Twidle for their help and advice. I would like to acknowledge the support of Thomas Sulzbacher who originated the project and Jerry Meerkatz for keeping the faith.

Last, but definitely not least, I would like to acknowledge the love and support of my family; without whom none of this makes sense.
#A Tour of **Cafe**

Our first task is to introduce you to the **Cafe** language. It is difficult to find the right order in which to present a programming language like **Cafe**; this is because many of the features are highly inter-related. For example, types depend on values which depend on functions which depend on types!

Instead, our approach is to take a series of horizontal slices through the whole language; going progressively deeper as you become more comfortable with the language.[^1]

[^1]:Since a layered approach means that a given description may be incomplete or slightly inaccurate, there is a temptation to use footnote annotations which declare '`...` but there is also `...`' Please forgive these pedantic notes when you see them.

##A First **Cafe** Program[a-first-Cafe-program]

It is kind of traditional to introduce a new programming language with something like the hello world example. Which we will do in a moment. However, the `factorial` function often makes a better first example for functional programming languages:

```
factorial is package{
  fact has type (integer)=>integer
  fun fact(0) is 1 -- base case
	| fact(N) where N>0 is N*fact(N-1)
}
```
This is a module which defines a single function — called `fact`. This is not an executable program per se.; however, it does represent a more typical source code unit — most actual programs are built from many smaller modules which come together to make the application. The `factorial` package is small, but already introduces many of the key elements of the **Cafe** language.

##Texture

All programming languages can be said to have a particular  _style_ or  _texture_. This is often so strong that it is often possible to identify a programming language from a single line of source code. In the case of **Cafe**, this line might be:

```
fact has type (integer)=>integer
```

which is a  _type annotation statement_ declaring the type of the function `fact`.

The texture of **Cafe** is designed to enhance the readability of programs; even at the expense of some additional typing. The rationale for this is that many people have to be able to read programs; perhaps more than are involved in writing them.

**Cafe**'s syntax has many keywords. This is quite different from many functional and non-functional programming languages that use many symbolic tokens. However, aggressive use of symbolic tokens can easily result in a large collection of obscure symbols whose meaning is not obvious to the casual reader.

>On the other hand, **Cafe** is not COBOL! Some symbols have an almost universally consistent meaning and **Cafe** uses them where it is appropriate.

###Types

**Cafe** is a strongly, statically typed language. The type annotation statement:

```
fact has type (integer)=>integer
```

declares that the type of `fact` is a function from `integer` to `integer`. However, in nearly all cases explicit type annotations like the one we used here are optional; we could, just as easily have written:

```Cafe
factorial is package {
  fun fact(0) is 1
	| fact(N) where N>0 is N*fact(N-1)
}
```
The compiler uses  _type inference_ to infer the type of `fact`; and can actually infer types in nearly every situation. The result is that a lot of the 'clutter' that can pervade a strongly typed language is just not necessary.

###Rules

Programs are defined using  _rules_. In this case, `fact` is defined using  _equations_. The equations that make up a function definition (or any program definition for that matter) are separated by the `|` operator.

Rule programs are intended to support a case-driven approach to programming: using rules to define programs allows the programmer to focus on each case in relative isolation.

###Patterns

Patterns are ubiquitous in **Cafe**. Patterns can be viewed as a combination of a test — does a value match a particular pattern — and as a way ( _the_ way in **Cafe**) of binding a variable to a value.

The first equation for `fact` above:

```
fun fact(0) is 1
```

has a literal pattern on the left hand side of the `is` keyword. This equation only applies when `fact` is called with zero as argument. The pattern in the second equation:

```
  | fact(N) where N>0 is N*fact(N-1)
```

has a guard on it — between the `where` and the `is` keywords. Guards are additional conditions that constrain patterns. In this case, the equation only applies if the argument is greater than zero.

Any pattern may be qualified with a guard; we could have written the guard  _inside_ the argument pattern:

```
  | fact(N where N>0) is N*fact(N-1)
```

We did not because having the guard outside the arguments is neater.

###Semi-colons are optional
When you have more than one definition in a sequence, such as in a package, they should be separated by semi-colons. However, they are nearly always optional: the compiler is smart enough to distinguish statements without requiring them.

In this book, we will avoid their use — this is part of an overall effort to reduce the visual clutter.

###Packages

The normal compilation unit is a `package`. In this case the package contains just the function `fact`, but packages can contain functions, type definitions, `import` statements and many other elements that we will encounter.

###Worksheets

The other main kind of compilation unit is the `worksheet`. Worksheets are a modern replacement for the REPL[^Read-Eval-Print-Loop] that you see in many functional programming languages.

>We say a  _modern_ replacement for REPLs because worksheets fit much better in the typical environment of an IDE.

A worksheet can be used to implement the infamous hello world example in just a few lines:

```
worksheet{
  show "hello world"
}
```

We can also use a worksheet to display the results of using and testing our `fact` function:

```
worksheet{
  import fact
  show "fact(10) is $(fact(10))"
  assert fact(5) = 120
}
```

###String Interpolation [string-interpolation]
The expression

```
"fact(10) is $(fact(10))"
```

is a  _string interpolation_ expression. It means the string with the substring `$(fact(10))` replaced by the value of the expression `fact(10)`. String interpolation expressions are a convenient way of constructing string values; and, via the use of [contracts][contracts], are also type safe.

Worksheets are like a combination of a module and the transcript of a session. In an IDE, the ideal mode of displaying a `worksheet` is via an interactive editor that responds to edit changes by recomputing the transcript and displaying the results in-line.

The key features of a `worksheet` that we will use are the ability to import packages, define elements, show the results of computations and define assertions.

Note that the original `factorial` package and the worksheet are separate compilation units and would be in different files. We could also have combined the two into a single `worksheet`:

```
worksheet{
  fun fact(0) is 1
   |  fact(N) where N>0 is N*fact(N-1)
  
  show "fact(10) is $(fact(10))"
  assert fact(5) = 120
}
```

##Types, More Types and Even More Types

In many ways, the defining characteristic of programming languages is their approach to types. This is especially true of statically typed languages like **Cafe**. The most basic question to ask about types is

>What is a type?

There is some surprising variability to the answer to this question; for example, in many OO languages, types are conflated with classes. **Cafe** types are terms that denote values.

**Type**
: A type is a term that denotes a set of values. I.e., a type is the _name_ of a set of values.

**Cafe**'s type system can be broken down into a number of dimensions:

*  How legal values of various kinds can be identified with a type;
*  the treatment of type variables and quantifiers; and
*  constraints on types, particularly type variables

**Cafe** distinguishes two basic styles of type: so-called  _structural_ or transparent types and  _nominative_ or opaque types. A structural type term echoes the values it models, whereas a nominative type typically does not.

We have already seen a use of structural types: function types are structural. For example, 

```
(integer,integer)=>string
```

denotes a function from two `integer`s giving a `string` and is effectively transparent. **Cafe** has structural types for functions, tuples, and records.

On the other hand, the standard type `integer` is nominative — its name gives no hint as to the representation, structure or kinds of values that are modeled by `integer`.[^2]

[^2]:I.e., everything you thought you knew about integers may or may not apply to the values denoted by `integer`.

###Nominative Types

A nominative type is normally defined using an  _algebraic type definition_. This both introduces a type and defines all the legal values that belong to the type. For example, we might introduce a `Person` type with the type definition:

```
type Person is noOne
            or someOne{
              name has type string
              dob has type date
            }
```

This statement tells us that there are two kinds of `Person`: a `someOne` who has a `name` and date of birth (`dob`) associated with them; and a distinguished individual we identify as `noOne`.

Notice how the type annotation statement we saw for declaring the type of `fact` is also used for defining the types of fields in the `someOne` record.

We can make a `Person` value with a labeled record expression:

```
def S is someOne{
  name = "fred"
  dob = today()
}
```

The keyword `def` is used to introduce a new single-assignment variable. In this case the variable `S` is defined to be a `someOne` record.

An important detail about the `someOne` record defined above is that the fields within it are not re-assignable. If we want to make a variable reassignable, or if we want to make a field of a record reassignable, we use a special `ref` type to denote that. For example,

```
type employee is employee{
  dept has type ref string
  name has type string
}
```

allows the `dept` field within the `employee` record to be modifiable.

>Only fields that have a `ref` type are modifiable in records. This is even true when a record is assigned to a reassignable variable.

A reassignable variable is declared using the `var` keyword — distinguishing it from the `def` statement — and the `:=` operator:

```
var N := employee{
  dept := "your department"
  name = "Anon. Y. Mouse"
}
```

Since the variable `N` is declared as being reassignable, we can give it a new value:

```
N := employee{
  dept := "another"
  name = "some one"
}
```

We can also modify the `dept` field of `N`:

```
N.dept := "new department"
```

However, we cannot modify the `name` field — because it is not re-assignable within the `Person` type.

Notice that the re-assignability of variables and fields does not 'inherit': each field or variable is separate. So, for example, if we declared a single-assignment variable `D` to be an `employee`:

```
def D is employee{
  dept := "his department"
  name = "Your Name Here"
}
```
then, even though `D` itself cannot be re-assigned to, the `dept` field of `D`  _can_ be altered:

```
D.dept = "my department"
```

###Optional Values
Notice that we identified a special case of `noOne` in our `Person` type. One reason for including this in a type is to be able to cope with non-existent people. However, in languages like Java, a different approach is used: namely variables in Java can be `null`.

However, `null` as found in Java and similar languages causes a great number of problems: for example, `null` must have a special universal type; there are many scenarios where it is not possible for a variable to be `null` but the compiler must discover those for itself; and there is often a consequent tendency in defensive programming to test for `null`.

**Cafe** has no direct equivalent of `null`. However, the [`option` type][the-option-type] allows the equivalent of selective nullability. Any variable that might not have a proper value can be marked with the `option` type rather than the `Person` type. And you can use `none` in those cases to indicate the equivalent of no value. 

So, for example, suppose that a `Person` might have a spouse — who is also a `Person` — but is not guaranteed to have one. Such a type can be described using:

```
type Person is someOne{
   name has type string
   dob has type date
   spouse has type ref option of Person
}
```
Here we have done two things: we have eliminated the `noOne` case for `Person` and we have marked the spouse as being both read-write and optional.

Someone with no spouse would be written:

```
def freddy is someOne{
  name = "Freddy"
  dob = today()
  spouse := none
}
```

whereas someone who has a spouse would be written:

```
someOne{
  name = "Lisa"
  dob = lastYear
  spouse := some(johnny)
}
```
Of course, we can record `freddy`'s marriage to `lisa` using an assignment:

```
freddy.spouse := some(lisa)
lisa.spouse := some(freddy)
```

###The Flavors of Equality
Equality in programming languages is often a very subtle topic. The issues can range from the nature of floating point numbers, the difference between integers and long values and the concepts of equality for objects.

Equality in **Cafe** is always between values of the  _same type_ and it is always  _semantic_. So, for example, an equality condition such as:

```
3=3.0
```
is not considered type safe — because `3` is an `integer` literal and `3.0` is a `float` literal. If you need to compare an integer and a floating point number for equality you will need to first of all decide in which type the comparison will be made (integer or floating point equality) and then  _coerce_ the other value into that type:

```
3 as float = 3.0
```
is valid.

>This is an important issue because not all `integer` values can be represented in a `float` value and vice-versa. So, comparing an integer and a floating point value implies the potential for losing information.

The second principle is that equality is semantic. What that means is that the `=` symbol is the name of a function, a function of type:

```
for all t such that 
  (t,t)=>boolean where equality over t
```

In effect, equality is  _not_ considered to be privileged; and it is definable by the programmer — albeit with some important useful default implementations.

###Quantified Types

A  _quantified type_ is one which has one or more type variables in it, and which is bound by a quantifier. The most common quantifier is the  _universal quantifier_ and universally quantified types correspond closely to generic types in other languages.

Generic types are often used to denote collection types. For example, the type

```
cons of t
```

denotes the generic `cons`-list type. In fact, `cons` is definable via a type definition:

```
type cons of t is nil or cons(t,cons of t)
```

>It is important to bear in mind here that `nil` is not a null-value and should not be confused with `NULL`, `null` or even `none`. It is simply one of the valid forms of a `cons` list.

The definition of the `cons` type contains a  _type variable_ `t` which is implicitly universally quantified, and the type term `cons of t` denotes a generic type.

Overall, the type definition for `cons` models very closely singly-linked lists.

A variable may also have a universally quantified type. A common case is in higher-order functions; for example, the `cons map` function (which applies a function to a list of elements to produce a new list) would have the type:

```
cons_map has type for all s,t such that
	((s)=>t,cons of s)=>cons of t
```

This states that the `cons map` function is generic in two types (`s` and `t`), it takes two arguments (a function from `s` to `t` and a `cons list`) and returns a new `cons list`.

The implementation of `cons_map` is extremely straightforward; we use patterns that follow the type definition for `cons` itself:

```
fun cons_map(_,nil) is nil
 |  cons_map(f,cons(h,t)) is cons(f(h),cons_map(f,t))
```

Note that we do not actually need to be explicit about the type of `cons map`: the compiler is perfectly capable of inferring it.

>However, it is good practice to explicitly annotate a function with its type. Type annotations act as useful documentation; and, furthermore, an explicit type annotation can help with diagnosing certain type errors.

We can use `cons_map` to apply a function to a `cons` list of elements; for example

```
worksheet{
  fun cons_map(_,nil) is nil
  ...
  show cons_map((X)=>X*X,cons(3,cons(2,nil)))
}
```
results in

```
cons(9,cons(4,nil))
```

being displayed.[^3]

[^3]:In fact, since `cons` is a standard type for which there is a special display implemented, what is shown will  be: 

  ```
  cons of [9,4]
  ```


The expression

```
(X)=>X*X
```

that is embedded in the call to `cons map` denotes a lambda expression in **Cafe**. Lambda expressions are simply functions that do not have a name.

**Cafe** also supports  _existentially_ quantified types — these are useful for denoting the types of modules and/or abstract data types. However, we will leave our exploration of existential types for later.

##A Tale of Three Loops

Imagine that your task is to add up a list of numbers. Sounds simple enough: in most procedural or OO languages (such as Java) one would write a fragment of code that looks like:

```
int total = 0;
for(Integer ix:L)
  total += ix;
```

However, this code is also full of pitfalls. For one thing we have a lot of extra detail in this code that represents additional commitments beyond those we might be comfortable with:

*  we have had to fix on the type of the number being totaled;
*  we had to know about Java's boxed v.s. unboxed types; and
*  we had to construct an explicit loop, with the result that we sequentialized the process of adding up the numbers.

We can also write an equivalent loop in **Cafe**:

```
def total is valof{
  var tot := 0;
  for ix in L do
	tot := tot+ix
  valis tot
}
```

The `valof/valis` combination is a neat way of segueing from the 'world of expressions' into the 'world of actions'; this would normally need a separate function to be defined in most languages.

This program is already slightly better than the Java loop because we have not had to be explicit about the types of variables. However, it is fundamentally the identical program with similar architectural issues.

While one loop is not going to hurt anyone; real code in languages like Java typically has many such loops. Especially when nesting loops to any depth, such code quickly becomes impossible to follow.

###A Functional Loop

A more idiomatic way of expressing a computation like the totalizer is to use a function. For example, we can write:

```
let{
  fun total(nil) is 0
   |  total(cons(E,L)) is total(L)+E
 } in total(L)
```

while short, this code too has some of the same drawbacks as the `for` iteration. Even if it is more declarative, there is still a lot of extra detail and architectural commitments here — like the commitment to `cons` lists and the commitment to `integer`s. These result in a function that is needlessly restricted.

Like other functional languages, **Cafe** has a range of higher-order operators that may come to the rescue. For example, we can avoid the explicit recursion altogether by using `leftFold`:

```
leftFold((+),0,L)
```

where `leftFold` means 

>apply an accumulating function to the elements of the data, assuming that the applied operator is left associative.

This is clearly both more concise and higher-level; and it begins to illustrate the productivity gains that are potentially available to the functional programmer.

Using `leftFold` means that we can often abstract away the machinery of loops and recursion completely — instead we can solve the problem at a more holistic level.

###A Totalizer Query

While concise, expressions involving much use of `leftFold` (and the analogous `rightFold`) can be difficult to follow. An even clearer way of adding up numbers is to use a  _query expression_:

```
fold X with (+) where X in L
```

or the even more succinct

```
total X where X in L
```

This query expression frees us from most of the commitments we endured before: it can add up the elements of any kind of collection — not just `cons` lists — and it can add up floating point numbers just as easily as integers. Finally, we have not had to say exactly how the numbers should be added up: the language system is free to use a parallel algorithm for the computation just as much as a sequential one.

The query expression is also very close to the natural specification:

>Add up the numbers in L

**Cafe**'s query expressions — which are similar to but also more expressive than LINQ — can be used to encapsulate a wide range of such computations. We shall look deeper into them in our chapter on [Collections][collections].

Of course, SQL programmers have long had access to this kind of conciseness and declarative expressiveness. However, SQL is constrained by the fact that it is intended to represent queries and actions over a very particular form of data — the relational table.

###The Homunculus in the Machine

Programming is often taught in terms of constructing sequences of steps that must be followed. What does that imply for the programmer? It means that the programmer has to be able to imagine what it is like to be a computer following instructions.

It is like imagining a little person — a homunculus — in the machine that is listening to your instructions and following them literally. You, the programmer, have to imagine yourself in the position of the homunculus if you want to write effective programs in most languages today.

Not everyone finds such feats of imagination easy. It is certainly often tedious to do so. Using query expressions and other higher-order abstractions significantly reduces the programmer's burden — precisely by being able to take a declarative approach to programming.

##Contracts and Constraints [contracts]

The concepts of interface and contract are foundational in modern software engineering. This is because explicit interfaces make it substantially easier to develop and evolve systems. A **Cafe** contract goes beyond the traditional concept of interface in important ways: we do not mark the definition of a type with its implemented contracts and we allow contracts to involve multiple types.

A `contract` defines a collection of signatures and an `implementation` provides specific implementations for those functions for a specific type (or type combination).

For example, we can imagine a contract for simple four function 'calculator arithmetic' containing definitions for the basic four functions of addition, subtraction, multiplication and division:

```
contract four over t is {
  plus has type (t,t)=>t
  sub has type (t,t)=>t
  mul has type(t,t)=>t
  div has type (t,t)=>t
}
```

This contract defines — but does not implement — the four functions `plus`, `sub`, `mul` and `div`. All these functions have a similar type:

```
plus has type for all t such that (t,t)=>t where four over t
```

The clause `where four over t` is a  _type constraint_, specifically a  _contract constraint_. So, these functions are generic (universally quantified) but the bound type (`t`) has the additional constraint that there is an `implementation` for it.

The `four` contract defines a set of functions that can be used without necessarily knowing the type(s) that are involved. For example, we can define the `double` function in terms of `plus`:

```
fun double(X) is plus(X,X)
```

This is a complete, legally valid, function definition. Its type — which is inferred automatically by the compiler — is also interesting:

```
double has type for all t such that (t)=>t where four over t
```

I.e., it inherits the same constraint as the function `plus` has. There are several kinds of type constraint in **Cafe**'s type system; but the  _contract constraint_ is the most significant of them.

###Implementing Contracts
Defining a contract is a big step, but it is not generally sufficient to produce working programs. If we had a `worksheet` containing only:

```
worksheet{
  contract four over t is {
	plus has type (t,t)=>t
	sub has type (t,t)=>t
	mul has type(t,t)=>t
	div has type (t,t)=>t
  }
  fun double(X) is plus(X,X)
  
  show double(2)
}
```

we would get a compiler error along the lines of:

```
2 has type integer
	which is not consistent with t_12 where
		pPrint over t_12 and
		four over t_12
	because four over integer not known to be implemented
```
This error message is effectively warning us that we have defined the `four` contract but we have not implemented it. Until we do, the program is not complete. However, if we do supply an `implementation` of `four` over `integer`s:

```
worksheet{
  contract four over t is {
	plus has type (t,t)=>t
	sub has type (t,t)=>t
	mul has type(t,t)=>t
	div has type (t,t)=>t
  }
  
  fun double(X) is plus(X,X)
  
  implementation four over integer is {
	fun plus(x,y) is x+y
	fun sub(x,y) is x-y
	fun mul(x,y) is x*y
	fun div(x,y) is x/y
  }
  show double(2)
}
```

then everything works as expected.

Notice that the error message above shows that type `t_12` actually has two type constraints:

```
t_12 where pPrint over t_12 and four over t_12
```

This is because the `show` action also results in a type constraint being involved. The `pPrint` contract is used to display values in a number of circumstances; including the string interpolation we saw [above][string-interpolation].

As may be expected, arithmetic itself is also mediated via the `arithmetic` contract in **Cafe**. This is how we can support multiple numeric types using a common set of operators: there are standard implementations of `arithmetic` for `integer`s, `long`, `float`ing point and `decimal` numbers.

###Coercion, not Casting

**Cafe** does not support type casting, as can be found in languages like Java and C/C++. This is for many reasons, not the least of which is safety and predictability of code.

Casting in many languages is really two kinds of operations-in-one which we can refer to as  _casting_ and  _coercion_. Casting is mapping of a value from one type to another without changing the value itself; and coercion involves converting a value from one type to another.

For example, the Java cast expression:

```
(Person)X
```

amounts to a request to verify that `X` is actually a `Person` object. In particular, this only checks the value of `X` to see if it is a `Person`. On the other hand, casting an integer to a double involves changing the value to conform to the floating point representation.

**Cafe** does not support casting, but does support coercion. However, coercion in **Cafe** is never silent or implicit — as it can be in Java and C/C++. An expression of the form:

```
3+4.5
```

will fail to type in **Cafe** — because there is an attempt to add an integer to a floating point number. The reason for raising an error is strongly related to safety and predictability: automatic conversion of integers to floating point can be a common source of errors in languages like C — because the implicit coercion of numeric values is easy to miss when reading arithmetic expressions.

**Cafe** provides a coercion notation that allows programmers to be precise in their expectations:

```
(3 as float)+4.5
```

denotes the explicit coercion of the integer `3` to a `float` and type checks as expected.

In fact, type coercion in **Cafe** is mediated via a `contract` and this expression is equivalent to

```
coerce(3)+4.5
```

where `coerce` is defined in the `coercion` contract involving two types:

```
contract coercion over (s,t) is {
  coerce has type (s)=>t
}
```

The `coercion` contract is an interface, but has no analog in most OO languages: it involves two types — the source type and the destination type. Each implementation of `coercion` specifies both types. For example, the implementation of coercion between integers and floating point is explicitly given:

```
implementation coercion over (integer,float) is { ... }
```

This implementation gives the implementation for coercing `integer`s to `float`s. Other implementation statements give the definitions for other forms of coercion.

Having coercion as a contract makes it straightforward to add new forms of coercion. This is used quite extensively within **Cafe** itself: for example, parsing JSON can be viewed as coercion from `string` values to `json` values. Thus the interface to parsers can be standard across all types and parsers.

##There is More

As we have noted, **Cafe** is a rich language and it would be impossible to try to cover it in a short introduction. Later chapters will look at some of the other features such as a deeper look at contracts, queries, actors, concurrency, existential types, and extending **Cafe** with domain specific languages. Chapter [2][functional-programming] starts this process by looking at functional programming in **Cafe**.
#Functional Programming[functional-programming]

There is a perception of functional programming that it is _weird_ and _difficult_. This is unfortunate for a number of reasons; the most important being that functional programming is _not_ weirder than procedural programming and that all programmers can benefit by programming functionally.

As for being difficult, a more accurate description would be that there is a deeper _range of features_ in functional programming than in most modern programming languages: so a perception of complexity can arise simply because there is more to say about functional programming languages. However, the simplest aspects of functional programming are very simple and the ramp need not be steep.

What may be surprising to the reader who is not familiar with functional programming is that it is _old_: predating the origins of modern computing itself, that there is a huge amount that can be expressed functionally, and that functional programming is often at least as efficient and sometimes more efficient than procedural programming.

In this chapter we will show how we can utilize **Cafe** as a vehicle for functional programming. As a side-goal, we also hope to demystify some of the language and ideas found in functional programming.

##What is Functional Programming?

The foundations of functional programming rest on two principles:

1.  Programs are expressed in terms of functions, where a somewhat mathematical view of functions is taken — functions always produce the same output for the same input.

	This is what people mean when they say that functional programs are _declarative_.[^The term _declarative_ has a technical definition. But this captures much of the essence of declarativeness (sic).]

2. Functions are values: they can be passed as arguments to functions, returned from functions, and they can be put into and retrieved from data structures.

This last principle is what people mean when they refer to functions as being _first class_ values. It is also what is meant when we say that functional programming languages are _higher order_.

**Cafe** is not actually a _pure_ functional programming language; i.e., it is possible to write programs that violate the declarative principle. However, it is a _functional-first_ programming language: a declarative style is strongly encouraged. In this chapter we shall focus on writing pure functional programs in
**Cafe**.

##Basics

There are two variants of function — the anonymous or _lambda_ function and the named function. We have already seen some simple examples both forms; for now, we continue to focus on named functions.

It is often easier to introduce functional programming using numerical examples. [Last chapter][a-first-Cafe-program] we saw, for example, the `factorial` program. This is mostly because most programmers are already familiar with numbers. Continuing that tradition, here is a function that returns the sign of a number:

```
fun sign(X) where X<0 is -1
 |  sign(0) is 0
 |  sign(X) where X>0 is 1
```

Each of these equations applies to different situations: the first equation applies when the input argument is negative, the second when it is exactly zero and the third when it is strictly positive. These represent the three possible cases in the definition of the sign function.

A **Cafe** function may be built from any number of rewrite equations; however, they must all be contiguous and separated by the `|` operator.

Although it is good practice to ensure that equations in a function definition do not overlap, **Cafe** will try the equations in a function definition in the order they are written in. We could have relied on this and written `sign` using:

```
fun sign(X) where X<0 is -1
 |  sign(0) is 0
 |  sign(X) is 1
```

Sometimes it is important to mark a particular equation as the _default_ case: i.e., an equation that should be used if none of the other cases apply:

```
fun sign(X) where X<0 is -1
 |  sign(0) is 0
 |  sign(X) default is 1
```

An explicitly marked `default` equation does not need to be the last equation.

Notice that we have not written any type annotations for the `sign` function; the **Cafe** compiler uses _type inference_ to automatically compute the types of functions and variables.

>The principle of type inference is not very hard: a program often has clues that indicate the types of variables. For example, predicate `X<0` has the clue that `X` is being compared to an `integer`; and so it had better be an `integer` too. The magic of type inference lies in the power and flexibility of the type language: specifically, the clues you leave as a programmer are sufficient to uniquely define the  types of all variables.

Although the compiler does not require type annotations, they can be useful to express the intent of the programmer. We can explicitly declare the type of `sign` with a _type annotation_:

```
sign has type (integer)=>integer
fun sign(X) where X<0 is -1
 |  sign(0) is 0
 |  sign(X) default is 1
```

It is a matter of personal style whether or not to mark functions with their types. In many situations an explicit type annotation represents useful documentation; furthermore, explicit type annotations can make it easier to debug certain kinds of type errors.

>However, particularly with some more complex cases, the type of a function may be too complex for the programmer to reliably write down.

In any case, the **Cafe** compiler computes the _most general_ form of type for functions and variables. This most general type is often more general than that written by human programmers! If you are explicit about the type of a function then that will be treated as the type of the function — even if it is less general than the compiler would have computed.

>We will generally _not_ write down the type of a function unless we are trying to draw your attention to it for some reason.

Of course, one of the potential consequences of manually adding type annotations is that the function's inferred type is not consistent with the declared type; in which case the compiler will print a suitable error message.

###Patterns

A function is defined as a sequence of _rewrite equations_ each of which consist of a _pattern_ and an _expression_. There are three general forms of rewrite equations:

_Pattern_ `is` _Expression_  

or

_Pattern_ `where` _Condition_ `is` _Expression_

or

_Pattern_ `default is` _Expression_

The left hand side of a rewrite equation consists of the pattern which determines the applicability of the equation; and the right hand side represents the value of the function if the pattern matches.

**Pattern**
: A _pattern_ represents a test or guard on a value. Patterns can be said to _succeed_ or _fail_ depending on whether the value being tested matches the pattern.

We also refer to a pattern being _satisfied_ when matching a value.[^This terminology originates from Logic — where a formula can be satisfied (made true) by observations from the world.]

The pattern in a rewrite equation is a guard on the arguments of the function call. For example, given a call

```
sign(34)
```

the patterns in the different equations of the `sign` function will be applied to the integer value `34`.

When the pattern on the left hand side of a rewrite equation succeeds then the equation _fires_ and the value of the expression on the right hand side of the equation becomes the value of the function.

As we noted earlier, patterns are ubiquitous in **Cafe**. They are used in equations, in variable declarations, in queries and in many other places. Here, we shall look at three main kinds of pattern, and in later sections, we look at additional forms of patterns.

**Variable Pattern**
: A _variable pattern_ is denoted by an identifier; specifically by the _first occurrence_ of an identifier.

A variable pattern always succeeds and has the additional effect of _binding_ the variable to the value being matched.

For example, the `X` in the left hand side of

```
fun double(X) is X+X
```

is a variable pattern. Binding `X` means that it is available for use in the right hand side of the equation — here to participate in the expression `X+X`.

The part of the program that a variable has value is called its _scope_.

*  Variables in rewrite equations always have scope ranging from the
  initial occurrence of the variable through to the whole of the right
  hand side of the equation.

*  Variable patterns are the _only_ way that a variable can get a
  value in **Cafe**.

*  Variables are never redeclared within a given scope. It is not
  permitted to hide a variable with a new variable that is defined
  within the natural scope of the variable.

  This is somewhat different to the scope rule for most other functional
  (and non-functional) languages — which allow outer scoped variables
  to be effectively eclipsed or hidden by inner variables.


  >The rationale for this choice is based on the observation that errors
  that arise from mistakenly hiding outer variables are often
  particularly difficult to track down.

**Literal Pattern**
: A _literal pattern_ — such as a numeric literal or a string literal — only matches the identical number or string.

Clearly, a literal match amounts to a comparison of two values: the pattern match succeeds if they are identical and fails otherwise.

Equality is based on _semantic equality_ rather than _reference equality_. What this means, for example, is that two strings are equal if they have the same sequence of characters in them, not just if they are the same object in memory.

There is no automatic coercion of values to see if they _might_ match. In particular, an `integer` pattern will only match an `integer` value and will not match a `float` value — even if the numerical values are the same. I.e., there will be no attempt made to coerce either the pattern or the value to fit.

>This, too, is based on the desire to avoid hard-to-detect bugs from leaking into a program.

**Guard Pattern**
: Sometimes known as a _semantic guard_, a guard pattern consists of a pair of a pattern and a _condition_:


>_Pattern_ `where` _Condition_


Conditions are `boolean`-valued and the guard succeeds if both the pattern matches and if the condition is _satisfied_. **Cafe** has a normal complement of special conditional expressions which we shall explore as we encounter the need. In the case of the equation:

```
 |  sign(X) where X>0 
```

the guard pattern is equivalent to:

```
X where X>0
```

We can put guard pattern anywhere that a pattern is valid; and, for convenience, we can also put them immediately to the left of the rewrite equation's `is` operator.

>Notice that any variables that are bound by the pattern part of a guarded pattern are _in scope_ in the condition part of the guard.

In the pattern above, the variable `X` will be bound in the variable pattern `X` and will then be tested by evaluating the condition `X>0`.

Subsequent occurrences of variables in a pattern 'stand for' `equality` guards. For example, the equation:

```
fun same(X,X) is true
```

is exactly equivalent to:

```
fun same(X,X1) where X=X1 is true
```

###Order of Evaluation

**Cafe** is a so-called _strict_ language. What that means is that arguments to functions are evaluated prior to calling the function. Most programming languages are strict; for two main reasons:

1.  It is easier to implement a strict language efficiently on modern hardware. Suffice it to say that modern hardware was designed for evaluating strict languages, so this argument is somewhat circular.

1.  It is also easier for programmers to predict the evaluation characteristics of a strict language.

There many possible styles of evaluation order; one of the great merits of programming declaratively is that the order of evaluation does not affect the actual results of the computation.

>It may, however, affect whether you get a result. Different strategies for evaluating expressions can easily lead to differences in which programs terminate and which do not.

One other kind of evaluation that is often considered is _lazy_ evaluation. Lazy evaluation means simply that expressions are only evaluated _when needed_. Lazy evaluation has many potential benefits: it certainly enables some very elegant programming techniques.

Essentially for the reasons noted above, **Cafe** does not use lazy evaluation; however, as we shall see, there are features of **Cafe** that allow us to recover some of the power of lazy evaluation.[^Even predominantly lazy languages like Haskell have   features which implement strict evaluation. It reduces to a question   of which is the _default_ evaluation style.]

The other dimension in evaluation order relates to the rewrite equations used to define functions. Here, **Cafe** uses an in-order evaluation strategy: the equations that make up the definition of a function are tried in the order that they are written — with the one exception being any `default` equation which is always tried last.[^There is a theorem — called the _Church Rosser Theorem_ — that guarantees some independence on the order of the rewrite equations provided that the different rewrite equations that make up function definitions do not overlap. Usually, however, it is too fussy to _require_ programmers to ensure that their equations  do not overlap; hence the reliance on ordering of equations.]

##Algebraic Data Types[algebraic-data-types]

Organizing data is fundamental to any programming language. **Cafe**'s data types are organized around the _algebraic data type_. An algebraic data type definition achieves several things simultaneously: it introduces a new type into scope, it gives an enumeration of the legal values of the new type and it defines both constructors for the values and it defines patterns for decomposing values. This is a lot for a single statement to do!

For example, we can define a type that denotes a point in a two-dimensional space:

```
type point is cart(float,float)
```

This kind of statement is called a _type definition statement_ and is legal in the same places that a function definition is legal.

The new type that is named by this statement is `point`; so, a variable may have `point` type, we can pass `point` values in functions and so on.

The constructor `cart` allows us to have expression that allow 'new' `point` structures to be made:

```
cart(3.4,2.1)
```

`cart` is also the name of a _pattern_ operator that we can use to take apart `point` values. For example, the `euclid` function computes the Euclidian distance associated with a `point`:

```
fun euclid(cart(X,Y)) is sqrt(X*X+Y*Y)
```

Of course, the `point` type is based on the assumption that point values are represented in a cartesian coordinate system. One of the more powerful aspects of algebraic data types is that it is easy to introduce multiple alternate forms of data. For example, we might want to support two forms of point: in cartesian coordinates and in polar coordinates. We can do this by introducing another case in the type definition statement:

```
type point is cart(float,float)
           or polar(float,float)
```

Of course, our `euclid` function also needs updating with the new case:

```
fun euclid(cart(X,Y)) is sqrt(X*X+Y*Y)
 |  euclid(polar(R,T)) is R
```

`cart` and `polar` are called _constructor functions_ because, logically, they _are_ functions.

For example, we can give a type to `polar`:

```
polar has type (float,float)=>point
```

In fact, constructor functions are one-to-one functions. Variously known as _free functions_ (in logic), _bijections_ (in Math), one-to-one functions are guaranteed to have an inverse. This is the logical property that makes constructor functions useful for representing data.

>Of course, we are talking of a _logical_ property of constructor functions. Internally, when implementing functional languages like **Cafe**, constructor functions are represented using data laid out in memory — just like any other programming language.

**Cafe** actually employs a special type for constructor functions; so the correct type of `polar` is given by:

```
polar has type (float,float)<=>point
```

The double arrow representing the fact that constructor functions are bijections.

In addition to constructor functions, an algebraic type definition can introduce two other forms of data: _enumerated symbols_ and _record functions_. Enumerated symbols are quite useful in representing symbolic alternatives. The classic example of an enumerated type is `daysOfWeek`:

```
type daysOfWeek is monday
          or tuesday
          or wednesday
          or thursday 
          or friday
          or saturday
          or sunday
```

Another example is the standard `boolean` type which is defined:

```
type boolean is true or false
```

Unlike enumerated symbols in some languages, there is no numeric value associated with an enumeration symbol: an enumerated symbol 'stands for' itself only. The reason for this will become clear in our next type definition which mixes enumerated symbols with constructor functions:

```
type sTree is sEmpty or sNode(sTree,string,sTree)
```

In addition to mixing the enumerated symbol (`sEmpty`) with the `sNode` constructor, this type is _recursive_: in fact, this is a classic binary tree type where the labels of the non-empty nodes are `string`s. (We shall see shortly how to generalize this).

Whenever you have a recursive type, its definition must always include one or more cases that are not recursive and which can form the base case(s). In that sense, an enumerated symbol like `sEmpty` plays a similar role in **Cafe** as `null` does in other languages; except that `sEmpty` is only associate with the `sTree` type.

We can use `sTree` to construct binary trees of `string` value; for example:

```
sNode(sNode(sEmpty,"alpha",sEmpty),
      "beta",
      sNode(sEmpty,"gamma",sEmpty))
```

denotes the tree in:

![A Binary `string` Tree][sTree]

[sTree]:images/sTree.jpg width=200px

>One of the hallmarks of languages like **Cafe** is that _every_ value has a legal syntax: it is possible to construct an expression that denotes a literal of any type.

Just as we can define `sTree` values, so we can also define functions over `sTree`s. For example, the `check` function returns `true` if a given tree contains a particular search term:

```
fun check(sEmpty,_) is false
 |  check(sNode(L,Lb,R),S) is Lb=S or check(L,S) or check(R,S)
```

Here we see several new aspects of **Cafe** syntax:

*  An empty pattern — marked by `_` — matches anything. It is called the _anonymous pattern_ and is used whenever we don't care about the actual content of the data.

*  The `or` disjunction is a _short-circuit_ disjunction; much like `||` in languages like Java.  Similarly, conjunction (`and`) is also short-circuiting like `&&`.

*  Functions can be recursive. **Cafe** permits _mutual recursion_ just as easily: there is no special requirement to order function definitions in a program.

We can use `sTest` to check for the occurrence of particular strings:

```
def T is sNode(sNode(sEmpty,"alpha",sEmpty),
               "beta",
               sNode(sEmpty,"gamma",sEmpty))
show check(T,"alpha")           -- results in true
show check(T,"delta")           -- results in false
```

##Functions as Values

The second principle of functional programming is that functions are first class. What that means is that we can have functions that are bound to variables, passed into functions and returned as the values of functions. In effect, a function is a legal _expression_ in the language. It also means that we can have _function types_ in addition to having types about data.

We can see this best by looking at a few examples. One of the benefits of passing functions as arguments to other functions is that it makes certain kinds of parameterization easy. For example, suppose that you wanted to generalize `check` to apply an arbitrary test to each node — rather than just looking for a particular `string`.

We will first of all define our `fTest` function itself:

```
fun fTest(sEmpty,_) is false
 |  fTest(sNode(L,Lb,R),F) is F(Lb) or fTest(L,F) or fTest(R,F)
```

The substantial change here is that, rather than passing a string to look for, we pass `fTest` a `boolean`-valued function to apply; within `fTest` we replace the equality test `Lb=S` with a call `F(Lb)`.

Given `fTest`, we can redefine our earlier `check` function with:

```
fun check(T,S) is fTest(T,(X)=>(X=S))
```

We have a new form of expression here: the _anonymous function_ or _lambda expression_. The expression

```
(X)=>(X=S)
```

denotes a function of one argument, which returns `true` if its argument is the same value as `S`.

>The parentheses around the equality are needed to avoid confusing the compiler.

Notice that it would be difficult to define a top-level named function that is equivalent to this lambda because of the occurrence of the variable `S` in the body of the lambda. This is an example of a _free variable_: a variable that is mentioned in the body of a function but which is defined outside the function. Free variables are a powerful feature of functional programming languages because they have an _encapsulating_ effect (the lambda encapsulates the free variable so that the `fTest` function does not need knowledge of `S`).

###Functions and Closures

If a function is an expression, what is the value of the function expression? The conventional name for this value is _closure_:

**Closure**
: A _closure_ is a structure that is the value of a function expression and which may be applied to arguments.

It is important to note that, as a programmer, you will never 'see' a closure in your program. It is an implementation artifact in the same way that the representation of floating point numbers is an implementation artifact that allow computers to represent fractional numbers but which programmers (almost) never see explicitly in programs.

Pragmatically, one of the important roles of closures is to capture any free variables that occur in the function. Most functional programming languages implement functions using closure structures. Most functional programming languages (including **Cafe**) do not permit direct manipulation of the closure structure: the only thing that you can _do_ with a closure structure is to use it as a function.

###Let Binding Environments[let-binding-environments]

We noted that it is difficult to achieve the effect of the `(X)=>(X=S)` lambda expression with named functions. The reason is that the free variable `S` is defined as a parameter to the `check` function — in particular, it is _not_ defined in the same way that named functions are defined. If we wanted to define a named function which also captures `S`, we would have to be able to define functions inside expressions.

There is an expression that allows us to do this: the `let` expression. A `let` expression allows us to introduce local definitions anywhere within an expression. We can define our lambda as the named function `isS` using the `let` expression:

```
let{
  fun isS(X) is X=S
} in isS
```

The region between the braces is a _definition environment_ and **Cafe** allows _any_ definition statement to be in such an environment. We can define `check` using a `let` expression:

```
fun check(T,S) is fTest(T,
  let{
    fun isS(X) is X=S
  } in isS)
```

This is a somewhat long-winded way of achieving what we did with the anonymous lambda function. However, there is a strong inter-relationship between anonymous lambdas, `let` expressions and variable definitions. These are all equivalent:

```
let{
  fun isS(X) is X=S
} in isS
```

```
let{
  def isS is (X) => (X=S)
} in isS
```

and

```
(X)=>(X=S)
```

Apart from being long-winded, the `let` expression is significantly more flexible than a simple lambda. It is much easier within a `let` expression to define functions with more than one rewrite equation; or to define multiple functions. We can even define local types within a `let` binding environment.

Conversely, lambda functions are so compact because they have strong limitations: you cannot easily define a multi-rewrite equation function with a lambda and you cannot easily define a recursive function as a lambda.

In short, we would use a `let` expression when the function being defined is at all complex; and we would use a lambda when the function being defined is simple and small.

Assembling functions in this way, either by using anonymous lambdas or by using `let` expressions, is one of the hallmarks of functional programming.

##Generic Programs[generic-programs]

If we take a slightly closer look at `fTest`, we can see that it seems pretty generic: it has a function as argument that it calls in the right places; but otherwise makes few assumptions about the function it calls. However, the type signature of `fTest` is rather specific:

```
fTest has type (sTree,(string)=>boolean)=>boolean
```

which means, for example, that its second argument _must_ be a function from `string` to `boolean`. On the other hand, nothing in the actual definition of `fTest` seems to depend on `string`s.

###Generic Types[generic-types]

What actually makes `fTest` more constrained than it could be is the type definition of `sTree` itself. It too is unnecessarily restrictive: why not allow trees of any type? We can, using the type definition for `tree`:

```
type tree of t is tEmpty or tNode(tree of t,t,tree of t)
```

Like the original `sTree` type definition, this statement introduces a new type: `tree of t` which can be read as 'tree of something'. The name `tree` is not actually a type identifier — although we often refer to the `tree` type — but is a _type constructor_.

>In an analogous fashion to constructor functions, a type constructor constructs types from other types. Type constructors are even bijections — one-to-one functions from types to types.

The identifier `t` in the type definition for `tree` denotes a _type variable_. Again, similarly to regular variables and parameters, a type variable denotes a single unspecified type. The role of the type variable `t` is like a parameter in a function: it identifies the unknown type and its role.

The `tree` type is a _generic_ or universal type. What that means is that instead of defining a single type it defines a family of related types: for example:

```
tree of string
tree of integer
...
```

are `tree` types. We can even have `tree`s of `tree`s:

```
tree of tree of string
```

We capture this genericity of the `tree` type by using a _universal quantifier_: a more accurate expression of the type introduced by the `tree` type statement would be:

```
for all t such that tree of t
```

What this type expression denotes is a set of possible types: for any type `t`, `tree of t` is also a type. There are infinitely many such types of course.

The `for all` quantifier is important: as in logic, there are two kinds of quantifiers in **Cafe**'s type system: the _universal_ `for all` and the _existential_ `exists`. We will take a deeper look at the latter in a later chapter on modular programming.

> To eliminate some syntactic clutter **Cafe** uses context to determine whether an identifier is a type variable or a type name. For example, in a type definition, only one type is being introduced, hence the `t` must be a type variable.

  In some cases, you will be required to use explicit quantifiers like `for all` and `exists`.

The types of the two constructors introduced in the `tree` type definition are similarly quantified:

```
tEmpty has type for all t such that
           ()<=>tree of t
tNode has type for all t such that
           (tree of t,t,tree of t)<=>tree of t
```

The type `()<=>tree of t` on the right hand side of `tEmpty`'s type annotation raises a couple of interesting points:


1.  `()` is the empty tuple — a tuple of zero elements. Normally,  **Cafe** will distinguish the expression `foo` from `foo()`.[^Normally, without the parentheses, the expression `foo` would refer to the function `foo` itself, whereas `foo()` denotes the _result_ of calling the function `foo`. This discrimination is one of the consequences of **Cafe**'s _strict evaluation_ rule.]

	But in the case of enumerated symbols they are conflated. This is an instance where more regularity would lead to less readability — because most programmers would think that having to write `true()` instead of `true` would be pretty bizarre.

	In the case of enumerated symbols however, there is no distinction between the _function_ `tEmpty` and the _value_ `tEmpty`.

2.  The type of a literal `tEmpty` expression — assuming that no further information is available — will be of the form `tree of t34` where `t34` is a 'new' type variable not occurring anywhere else in the program. In effect, the type of `tEmpty` is `tree of` _some type_ `t34` where we don't know anything more about `t34`.

###Generic Functions

Given this definition of the `tree` type, we can construct a more general form of the tree test function; which is almost identical to `fTest`:

```
fun test(tEmpty,_) is false
 |  test(tNode(L,Lb,R),F) is F(Lb) or test(L,F) or test(R,F)
```

and our original string `check` function becomes:

```
fun check(T,S) is test(T,(X) => X=S)
```

The type that is computed for `test` reflects the inherent generality arising from the generic `tree` type:

```
test has type for all t such that
    (tree of t,(t)=>boolean)=>boolean
```

The type of `check` is also more generic:

```
check has type for all t such that (tree of t,t)=>boolean
```

I.e., `check` can be used to find any type of element in a tree — providing that the types align of course.

##Going Further[going-further]

Although better than the original `sTest` program there is still one major sense in which the `test` program is not general enough. We can see by looking at another example: a function that counts elements in the tree:

```
fun count(tEmpty) is 0
 |  count(tNode(L,_,R)) is count(L)+count(R)+1
```

This code is very similar, but not identical, to the `test` function.

The issue is that `test` is trying to do two things simultaneously: in order to apply its test predicate to a binary tree it has to implement a walk over the tree, and it also encodes the fact that the function we are computing over the `tree` is a `boolean`-value function.

We often need to do all kinds of things to our data structures and writing this kind of recursion over and over again is tedious and error prone. What we would like to do is to write a single _visitor_ function and specialize it appropriately when we want to perform a specific function.

>This principle of separating out the different aspects of a system is one of the core foundations of good software engineering. It usually goes under the label _separation of concerns_. One of the beautiful things about functional programming is that it directly supports such good architectural practices.

Since this visitor may be asked to perform any kind of computation on the labels in the `tree` we will need to slightly generalize the type of function that is passed to the visitor. Specifically, the type of function should look like:

```
F has type (a,t)=>a
```

where the `a` input represents accumulated state, `t` represents an element of the `tree` and the result is another accumulation.

Using this, we can write a `tVisit` function that implements tree walking as:

```
fun tVisit(tEmpty,_,A) is A
 |  tVisit(tNode(L,Lb,R),F,A) is tVisit(R,F,F(tVisit(L,F,A),Lb))
```

Just as the accumulating function acquires a new 'state' parameter, so the `tVisit` function also does. The `A` parameter in the two equations represents this accumulated state.

The second rewrite equation for `tVisit` is a bit dense so let us open it out and look more closely. A more expanded way of writing the `tVisit` function would be:

```
fun tVisit(tEmpty,_,A) is A
 |  tVisit(tNode(L,Lb,R),F,A) is let{
      def A1 is tVisit(L,F,A)
      def A2 is F(A1,Lb)
    } in tVisit(R,F,A2)
```

where `A1` and `A2` are two local variables that represent the result of visiting the left sub-tree and applying the accumulator function respectively. We have used the `let` expression form to make the program more obvious, rather than to introduce new functions locally; but this is a legitimate role for `let` expressions.

The `tVisit` function knows almost nothing about the computation being performed, all it knows about is how to walk the tree and it knows to apply functions to labels in the `tree`.

Given `tVisit`, we can implement our original `check` and `count` functions as one-liners:

```
fun check(T,S) is tVisit(T,(A,X)=>(A or X=S),false)
fun count(T) is tVisit(T,(A,X)=>A+1,0)
```

> Notice that we have effectively hidden the recursion in these function definitions — all the recursion is encapsulated within the `tVisit` function. One of the unofficial mantras of functional programming is _hide the recursion_.

The reason we want to hide recursions that this allows the designer of functions to focus on _what_ is being computed rather than focusing on the structure of the data and, furthermore, this allows the implementation of the visitor to be _shared_ by all users of the `tree` type.

Notice that, while `a` and `t` are type variables, we did not put an explicit quantifier on the type of `F`. This is because the quantifier is actually put on the type of `tVisit` instead:

```
tVisit has type for all a,t such that
    (tree of t,(a,t)=>a,a)=>a
```

Just like regular variables, type variables have scope and points of introduction. Also like regular variables, a type variable may be _free_ in a given type expression; although it must ultimately be _bound_ by a quantifier.

###Going Even Further[going-even-further]

We have focused so far on generalizing the visitor from the perspective of the `tree` type. But there is another sense in which we are still _architecturally entangled_: from the perspective of the `check` and `count` functions themselves.

In short, they are both tied to our `tree` type. However, there are many possible collection data types; **Cafe** for instance has some 5 or 6 different standard collection types. We would prefer not to have to re-implement the `check` and `count` functions for each type.

The good news is that, using contracts, we can write a single definition of `check` and `count` that will work for a range of collection types.

Let us start by defining a contract that encapsulates what it means to `visit` a collection:

```
contract visitor over c determines t is {
  visit has type for all a such that (c,(a,t)=>a,a)=>a
}
```

This `visitor` contract defines a single function that embodies what it means to _visit_ a collection structure. There are quite a few pieces here, and it is worth examining them carefully.

A contract header has a template that defines a form of _type constraint_. The clause

```
visitor over c determines t
```

is such a constraint. The sub-clause

```
        c determines t
```

refers to two types: `c` and `t`.

The `visitor` contract itself is about the collection type `c`. But, within the contract, we need to refer to both the collection type and to the type of elements in the collection: the `visit` function is over the collection, it applies a function to elements of the collection.

Furthermore, as we design the contract, we _do not know_ the exact relationship between the collection type and the element type. For example, the collection type may be generic in one argument type — in which case the element type is likely that argument type; conversely, if the type is _not_ generic (like `string` say), then we have no direct handle on the element type.

We _do know_ that within the contract the element type is _functionally determined_ by the collection type: if you know the collection type then you should be able to figure out the element type.

We express this dependency relationship with the the `c determines t` form: whatever type `c` is, `t` must be based on it.

The body of the contract contains a single type annotation:

```
visit has type for all a such that
    (c,(a,t)=>a,a)=>a
```

This type annotation has three type variables: the types `c` and `t` come from the contract header and `a` is local to the signature. What the signature means is

> Given the `visitor` contract, the `visit` function is from the collection type `c`, a function argument and an initial state and returns a new accumulation state.

It is worth comparing the type of `visit` with the type of `tVisit`:

```
tVisit has type for all t,a such that
    (tree of t,(a,t)=>a,a)=>a
```

The most significant difference here is that in `tVisit` the type of the first argument is fixed to `tree of t` whereas in `visit` it is left simply as `c` (our collection type).

Given this contract, we can re-implement our two `check` and `count` functions even more succinctly:

```
fun check(T,S) is visit(T, (A,X)=>A or X=S,false)
fun count(T) is visit(T, (A,X)=>A+1,0)
```

These functions will apply to _any_ type that satisfies — or implements — the `visitor` contract. This is made visible in the revised type signature for `count`:

```
count has type for all c,t such that
    (c)=>integer where
      visitor over c determines t
```

This type is an example of a _constrained type_. It is generic in `c` and `t` but that generality is constrained by the requirement that the `visitor` contract is appropriately implemented. The eagled-eyed reader will notice that `count` does not actually depend on the type of the elements in the collection: this is what we should expect since `count` does not actually care about the elements themselves.

The type signature for `check`, however, does care about the types of the elements:

```
check has type for all c,t such that 
  (c,t)=>boolean where
    visitor over c determines t
```

Given the work we have done, we can implement the `visitor` contract for our `tree of t` type quite straightforwardly:

```
implementation visitor over tree of t determines t is {
  visit = tVisit
}
```

Notice that header of the `implementation` statement provides the connection between the collection type (which is `tree of t`) with the element type (`t`). The clause

```
visitor over tree of t determines t
```

is effectively a declaration of that connection.

Now that we have disconnected `visit` from `tree` types, we can extend our program by implementing it for other types. In particular, we could also implement the `visitor` for the `sTree` type:

```
implementation visitor over sTree determines string is {
  visit = sVisit
}
```

however, we leave the definition of `sVisit` as a simple exercise for the reader.

Our final versions of `count` and `check` are now quite general: they rely on a generic implementation of the `visit` function to hide the recursion and are effectively independent of the actual collection types involved.

If we take a second look at our `visitor` contract we can see something quite remarkable: it counts as a definition of the famous _visitor pattern_. This is remarkable because although visitor patterns are a common design pattern in OO languages, it is often hard in those languages to be crisp about them; in fact, they are called patterns because they represent patterns of use which may be encoded in Java (say) whilst not necessarily being definable in them.

The combination of `contract` and `implementation` represents a quite formal way of defining patterns like the visitor pattern.

There is something else here that is quite important too: we are able to define and implement the `visitor` contract _without_ having to modify in any way the type definition of `tree` or `sTree`. From a software engineering point of view this is quite important: we are able to gain all the benefits of interfaces without needing to entangle them with our types. This becomes critical in situations where we are not able to modify types — because they don't belong to us and/or we don't have access to the source.

###Polymorphic Arithmetic[polymorphic-arithmetic]

There are other ways in which programs can be polymorphic. In particular, let us focus for a while on arithmetic. One of the issues in arithmetic functions is that there are many different kinds of numbers. Pretty much every programming language distinguishes several kinds of numbers; for example, Java distinguishes `byte`, `short`, `integer`, `long`, `float`, `double`, `BigInteger` and `BigDecimal` — and this does not count the wrapped versions. Other languages have even more choice.

One question that seems relevant is why? The basic answer is that different applications call for different properties of numbers and no one numeric type seems to fit all needs. However, the variety comes at a cost: when we _use_ numbers we tend to have to make too early a choice for the numeric type.

For example, consider the `double` function we saw earlier:

```
fun double(X) is X+X
```

What type should `double` have? In particular, what should the type of `+` be? Most people would be reluctant to use different arithmetic operators for different types of numbers.[^Although some languages — such as ML — do require this.] This is resolved in **Cafe** by relying on contracts for the arithmetic operations.

The result is that the type computed for `double` is exquisitely tuned:

```
double has type for all t such that
    (t)=>t where arithmetic over t
```

This type is precisely the minimal type that `double` could have. Any further constraints result in making a potentially premature choice for the numeric type.

If we take another look at our factorial function:

```
fun fact(0) is 1
 |  fact(N) is N*fact(N-1)
```

this is constrained to be a function from `integer` to `integer` because we introduced the literal integers `0` and `1`. However, the `arithmetic` contract contains synonyms for these very common literals. Using `zero` and `one` allow us to be abstract in many arithmetic functions:

```
fun genFact(N) where N=zero is one;
 |  genFact(N) is N*genFact(N-one)
```

>We call out `zero` and `one` for special treatment because they occur very frequently in numerical functions.

The type of `genFact` reflects this dependency on `arithmetic` and also emphasizes precision:

```
genFact has type for all t such that
  (t)=>t where arithmetic over t
```

We can actually introduce other numeric literals without compromising our type by using _coercion_; although it is more clumsy:

```
factorialC has type for all t such that
  (t)=>t where arithmetic over t and coercion over (integer,t)
fun factorialC(N) where N=0 as t is 1 as t
 |  factorialC(N) is N*factorialC(N-1 as t)
```

The expressions `0 as t` and `1 as t` are coercions from `integer` to `t`.

>The clumsiness stems from the requirement to have an explicit type signature for `factorialC` — in order for the coercions to be sound; as well as all the coercions themselves.

This is one of the few places where explicit type annotations are required; this, in turn, is required because we actually _mention_ the type in the program.

Of course, coercion is also governed by contract, a fact represented in the type signature by having two contract constraints on the type of `t`.

In any case, using these techniques, it is possible to write numeric functions without unnecessarily committing to specific number types. That in turn helps to make them more useful.

##A Word About Type Inference

We have seen some powerful forms of types in this chapter: recursive types defined using algebraic type definitions, generic types and even function types. Recall also that **Cafe** does not require programmers to explicitly declare the types of variables and functions. It is worth pausing a little to see how this might be done.

Recall our original `factorial` function:

```
fun fact(0) is 1
 |  fact(N) is N*fact(N-1)
```

The type that is inferred from this definition is given by:

```
fact has type (integer)=>integer
```

On the other hand, the type inferred from

```
fun tVisit(tEmpty,_,A) is A
 |  tVisit(tNode(L,Lb,R),F,A) is tVisit(R,F,F(tVisit(L,F,A),Lb))
```

is

```
tVisit has type for all a,t such that
    (tree of t,(a,t)=>a,a)=>a
```

The compiler is able to compute these types automatically through a process known as _type inference_. Type inference may seem magical, but is actually (mostly) quite simple. Let us take a look at the expression:

```
N-1
```

which is buried within the recursive call in `fact`. Although it looks like a special operator, **Cafe** does not treat arithmetic expressions in a special way; the `-` function is just a function from numbers to numbers.

The type of arithmetic subtraction is given by:[^3]

[^3]: The handling of arithmetic functions is more subtle than we admit to here. As we saw before, all arithmetic in **Cafe** is actually mediated via the `arithmetic` contract. The actual type of `(-)` is given by

  ```
  (-) has type for all t such that (t,t)=>t where arithmetic over t^^J
  ```
  However, we have simplified this in order to make the explanation of type inference a little simpler.

```
(-) has type (integer,integer)=>integer
```

>This is a legal **Cafe** type annotation: the parentheses around the `-` operator are used to signal a use of the operator as a stand-alone identifier; and to suppress its use as an infix operator.

Type inference proceeds by using special _type inference rules_ which relate expressions to types, in this case the applicable rule is that a function application is consistent if the function's parameter types are consistent with the types of the actual arguments. If they are consistent, then the type of the function application is the return type of the function.

The type inference process initially gives every variable an unknown type — represented by a new type variable not appearing anywhere else. For our tiny `N-1` example, we will give `N` the type t~N~.

The `(-)` function has two arguments whose types can be expressed as a tuple of types:

```
(integer,integer)
```

and the types of the actual arguments are also a tuple:

`(`t~N~ `,integer)`

In order for the expression to be type correct, the actual types of the arguments must be consistent with the expected types of the function; which we can do by making them _the same_. There is a particular process used to do this — called _unification_.

![Inferring the Type of `N-1`][minusType]

[minusType]:images/minustype.jpg width=350px

**Unification**
: An algorithm that replaces variables with values in such a way as to make two terms _identical_.

Unification matches equals with equals and handles (type) variables by substitutions — for example, we can unify these two type expressions by _binding_ the type variable t~N~ to `integer`.

We initially picked the type of `N` to be an arbitrary type variable, but the process of checking consistency leads us to refine this and make the type concrete. I.e., the use of `N` in a context where an `integer` is expected is enough to allow the compiler to infer that the type of `N` is indeed `integer` and not t~N~.

Of course, if there are multiple occurrences of `N` then each of those occurrences must also be consistent with `integer`; and if an occurrence is not consistent then the compiler will report an error — a given variable may only have one type!

The bottom line is that **Cafe**'s types are based on a combination of unification for comparing types and a series of type rules that have the effect of introducing _constraints_ on types based on which language features the programmer uses. The type checker is really a constraint solver: if the constraints are not satisfiable (for example by trying to 'call' a variable and add a number to it) then there is a type error in the program.

The magic of type inference arises because it turns out that solving these constraints is sufficient for robustly type checking programs.

>A sharp-eyed reader will notice that **Cafe**'s type system is different in nature to that found (say) in OO languages. In **Cafe**'s type system, types are considered to be consistent in **Cafe** if they are _equal_.[^This is a slight over-simplification.] This is quite different to the notion of consistency in OO languages where an argument to a function is consistent if its type is a _sub-type_ of the expected type.

However, we would note that the apparent restriction to the type system imposed by type equality is much less severe in practice than in theory — and that OO languages' type systems also incorporate some of the same restrictions.

We are only able to scratch the surface of the type system here. It is certainly true that — like many modern functional languages — **Cafe**'s type system is complex and subtle. The primary motivation for this complexity is to reduce the burden for the programmer: by being able to infer types automatically, and by being able to address many programming subtleties, the type system comes to be seen as the programmer's friend rather than as an obstacle to be 'gotten around'.

##Are We There Yet?[are-we-there-yet]

The straightforward answer to this is no. There is a great deal more to functional programming than can be captured in a few pages. However, we have covered some of the key features of functional programming — particularly as it applies to **Cafe**. In subsequent chapters we will take a closer look at collections, at modular programming, at concurrency and even take a pot shot at Monads.

If there is a single idea to take away from this chapter it should be that functional programming is natural. If there is a single piece of advice for the budding functional **Cafe** programmer, it should be to _hide the recursion_. If there is a single bit of comfort to offer programmers it should be that _Rome was not built in a day_.
#Collections[collections]

Modern programming — whether it is OO programming, functional programming or just plain C programming — relies on a rich standard library. Given that nearly every program needs to be able to manage collections of _things_, the central pearl of any standard library is the _collections_ library. Recalling our mantra of hiding recursion; a well designed collections library can make a huge difference to the programmer's productivity, often by hiding a lot of the recursions and iterations required to process collections.

The collections architecture in **Cafe** has four main components:

1.  a range of standard collection types — including array-like lists, cons lists, red-black trees, first-in first-out queues, and dictionaries;

1.  a range of standard functions — mostly defined in contracts — that define the core capabilities of functions over collection;

1.  special notations that make programming with collections in a type independent way more straightforward; and

1.  the final major component of the collections architecture is _queries_. **Cafe** has a simple yet powerful set of features aimed at simplifying querying collections.

##Sequence Notation[sequence-notation]
A sequence is simply an ordered collection; a _sequence expression_ is an expression involving a complete or partial enumeration of the values in the collection. **Cafe** has a simple notation for expressing sequences of any underlying type; for example, a `cons` sequence of integers from 1 through 5 can be written:

```
cons of [1, 2, 3, 4, 5]
```

In situations where we do not know or do not wish to specify the collection type, we can write instead:

```
[1, 2, 3, 4, 5]
```

This term — it could be either an expression or a pattern — denotes the sequence _without_ specifying the underlying collection type. The difference in the types of the two terms is telling:

```
cons of integer
```

and

```
c where sequence over c determines integer
```

respectively — where `c` is a type variable. The first is a concrete type expression, the second is a constrained type — a type variable that may only be instantiated with a type that is known to satisfy one or more constraints, in this case to implement the `sequence` contract.

>Although the second type expression is longer, and a bit more complex to read, it is also less constraining. The type expression `cons of integer` is concrete and does not allow for variation of the underlying collection type; the second type expression allows the term to be used in contexts that require different concrete types.

The sequence notation also allows for the specification of partial sequences; this is particularly useful in writing functions that construct and traverse sequences. The sequence term:

```
[1,2,..X]
```

denotes the sequence whose first two elements are `1` and `2` and whose remainder is denoted by the variable `X` — which must also be a sequence of the correct type. Similarly, the term:

```
[F..,23]
```

denotes the sequence obtained by glueing `23` to the back of the sequence `F`.

There is a strong relationship between the 'regular' sequence notation and the partial sequence notation. In particular, the sequence expression

```
cons of [1,2]
```

is equivalent to:

```
cons of [1,..cons of [2,..cons of []]]
```

However, we are not permitted to use both of `,..` and `..,` in the same expression:

```
[F..,2,3,..B]
```

is not permitted (since it amounts to a concatenation of two sequences which implies a non-deterministic search when used as a pattern).

The major benefit of general sequence notation is that it allows us to construct programs involving collections that are independent of type _and_ to do so in a syntax which is concise.

For example, we can use sequence notation to write functions over sequences; such as the `concat` function that concatenates two sequences:

```
fun concat([],X) is X
 |  concat([E,..X],Y) is [E,..concat(X,Y)]
```

This function will work equally well with `cons` lists, `list`s, `string`s, even your own collection types. All that is required is that there are implementations of the `sequence` contract for the actual type.

###Types and Sequence Notation[types-and-sequence-notation]

However, there is a small failure of type inference with the `concat` program as written. Our compiler is very careful to construct the most general type when inferring the types of functions; in this case the compiler is a little too careful! The equations as written do not constrain the type of the function in the way that you might expect. The type as computed by the compiler is:

```
concat has type for all e,s,t such that
    (s,t)=>t where
        sequence over s determines e and
        sequence over t determines e
```

I.e., the types of the two input sequences are _not_ constrained to be the same, and the output type is gotten only from the second argument sequence. In some situations this would be exactly what was needed — if the programmer wanted to concatenate a `cons` list to a `list` value (say) to produce a result `list`. However, in most situations this is too general and can cause problems when two or more `concat` expressions are used together; especially with general sequences:

```
concat(cons of [1],concat([2,3],[3,4]))
```

The issue is that the type of the intermediate expression — `concat([2,3],[3,4])` — is _too_ under-specified and the compiler does not have enough information to go on. It could literally be any type of collection and there is no way to constrain it.

>Notice that this is a _small_ failure; if non-literal sequence expressions, or simply variables, had been used in the intermediate `concat` expression, the compiler would have had no issues with type inference.

A human programmer noticing this situation would observe that the type of the intermediate sequence _does not matter_ and we could just pick any type — most likely the type of the sequence being consumed. However, compilers are not as smart (generally) as human programmers and are definitely much more literal-minded. The result is an error along the lines of

```
unresolved variable: concat
```

A more intuitive type for `concat` that also fixes the problem would be:

```
concat has type for all e,s such that
    (s,s)=>s where
        sequence over s determines e
```

This is a more restrictive type, but fits actual usage better. Furthermore, by tying the types of the two arguments together it means that the compiler will be able to type check the program properly.

The net effect is that the `concat` function, as written, needs an explicit type annotation for it to be generally useful.

###The Sequence Contract[the-sequence-contract]

Underlying the sequence notation is the `sequence` contract. This contract contains type signatures in it that can be used to construct and to match against sequence values. The sequence notation is realized by the compiler translating sequence terms to a series of calls to those functions.

The actual `sequence` contract is

```
contract sequence over t determines e is {
  _nil has type ()=>t      — empty sequence
  _cons has type (e,t)=>t  — add to front
  _apnd has type (t,e)=>e  — add to back
  _empty has type ()<=t    — match empty sequence
  _pair has type (e,t)<=t  — match front
  _back has type (t,e)<=t  — match back
}
```

The first three entries in this contract should be fairly self-evident:

* `_nil` is a function that returns an empty sequence;
* `_cons` is a function that 'glues' a new element to the front of
the collection; and
* `_apnd` appends elements to the _back_ of the collection.

The compiler uses these three functions to transform sequence expressions into function calls. For example, the sequence expression:

```
[1,2,3]
```

is transformed into

```
_cons(1,_cons(2,_cons(3,_nil())))
```

If a sequence expression has an explicit type marker on it, then its translation is slightly different — to allow the type checker to make use of the type information. For example, `cons of [1,2]` is translated:

```
_cons(1,_cons(2,_nil())) has type cons of %_
```

This annotation is all that is needed to force the compiler to treat the result as a concrete `cons` list.

> The type expression `%_` is a special type that denotes an anonymous type: each occurrence of the type expression denotes a different unknown type. It is useful in situations, like this one, where only some of the type information is known.

###Sequence Patterns[sequence-patterns]

The complete `sequence` contract has six signatures in it — the latter three signatures play an analogous role to the first three but for sequence _patterns_ rather than sequence _expressions_. They also introduce a new form of type expression — the _pattern type_. For example, the signature for `_pair` — which is used to decompose sequences into a head and tail — is:

```
_pair has type (e,t)<=t
```

Notice the direction of the arrow: this is a new form of type, and relates to a new form of capability that we have not encountered yet in this book — pattern abstractions.

**Pattern Abstraction**
:	A _pattern abstraction_ is an expression that denotes a pattern, allowing its re-use.

Pattern abstractions are exactly analogous to functions — another name for which is _expression abstraction_. Pattern abstractions allow patterns to be encapsulated and reused in the same way that functions allow expressions to be encapsulated and reused.

>In this case, the pattern abstraction is critical because general sequence notation is independent of the types of the collections involved — and so we have no way of knowing what concrete patterns to apply.

Pattern abstractions are applied using the same application notation as for function application; for example, the `_pair` pattern in

```
fun first(_pair(H,T)) is H
```

is a pattern that is applied to the argument of `first`. What may be a little surprising initially is that the arguments to a pattern application are also patterns! So, here, the variables `H` and `T` in the call to `_pair` will be bound to the first element of the collection and the remainder respectively.

For example, applying `_pair` to `[1,2]` binds `H` to the value `1` and binds `T` to the sequence `[2]`. The value returned by `first` will be `1`.

Obviously, there must also be a way of defining pattern abstractions. We can define a `cPair` pattern abstraction that applies to `cons` lists thus:

```
ptn cPair(H,T) from cons(H,T)
```

This takes a little careful reading, but is ultimately straightforward: the right hand side is the pattern that is being abstracted, the left hand side is an application template.

We also have a new keyword here: `ptn`. Just as `fun` introduces a function, `def` introduces a single-assignment variable, so `ptn` introduces a pattern abstraction.

The general form of a pattern abstraction is:

`ptn` _name_(_E~1~_`,..,`_E~n~_`) from` _Pattern_

where the various _E~i~_ are _expressions_ that represent the values 'read off' the _Pattern_ — should the pattern be satisfied. This is quite analogous to the situation for rewrite equations — except that the roles of patterns and expressions are reversed.

>Like functions, pattern abstractions may be defined with multiple pattern rules; and the pattern abstraction is satisfiable exactly when one of its pattern rules is.

>Pattern abstractions are not as ubiquitous as functions; however, they certainly play a vital role in the overall design of **Cafe**; and are indispensable in the right circumstances.

  One special use for pattern abstractions is to give higher-level names to particular patterns. This mimics the use of functions naming expressions, and has a similar importance for program design.

Given that we have seen how sequence expressions are transformed into function calls from the `sequence` contract, we can now straightforwardly give the equivalent translation for sequence patterns. Syntactically, there is no distinction between sequence expressions and sequence patterns — what distinguishes them is context: sequence patterns show up as patterns in functions and sequence expressions show up in the expression context.

A sequence pattern, as in the pattern `[E,..X]` for the non-empty case in `concat`:

```
fun concat([E,..X],Y) is [E,..concat(X,Y)]
```

is transformed into the pattern:

```
_pair(E,X)
```

and the entire rewrite equation becomes:

```
fun concat(_pair(E,X),Y) is _cons(E,concat(X,Y))
```

We can combine multiple pattern abstraction applications; for example, the function:

```
fun single([H]) is H
```

which is a function that only matches singleton sequences requires two pattern applications from the `sequence` contract:

```
fun single(_pair(H,_empty())) is H
```

###Notation and Contract-Based Semantics[notation-and-contract-based-semantics]

One of the distinctive features of the sequence notation is that it is an example of _syntax_ that is underwritten by a semantics expressed as a _contract_. The merit of this approach is that we can have a special notation expressing a salient concept — in this case the sequence — and we can realize the notation without undue commitment in its lower-level details. In the case of sequence notation, we can have a notation of sequences without having to commit to the type of the sequence itself.

This has a parallel in modern OO languages like Java and C# where important contracts are expressed as interfaces rather than concrete types. However, **Cafe** extends the concept by permitting special notation as well as abstract interfaces — as many mathematicians understand, a good notation can make a hard problem easy. In **Cafe** we further separate interfaces from types by separating the type definition from any contracts that may be implemented by it.

This is part of a general pattern in **Cafe**: there are many _sub-languages_ that are actually underwritten by contracts for their realization. For example, the _indexing_ notation has the same pattern: of a special notation backed by contract.

##Indexing[indexing]

Accessing collections conveniently is arguably more important than a good notation for representing them. There is a long standing 'traditional' notation for accessing arrays:

```
L[ix]
```

where `L` is some array or other collection and `ix` is an integer offset into the array. **Cafe** uses a notation based on this for accessing collections with random indices; suitably generalized to include dictionaries (collections accessed with non-numeric indices) and _slices_ (contiguous sub-regions of collections).

Before we explore **Cafe**'s indexing notation it is worth looking at the contract that underlies it — the `indexable` contract.

###The `indexable` Contract[the-indexable-contract]

The `indexable` contract captures the essence of accessing a collection in a random-access fashion. There are functions in the contract to access a directly accessed element, to replace and to delete elements from the collection:

```
contract indexable over s determines (k,v) is {
  _index has type (s,k)=> option of v
  _set_indexed has type (s,k,v)=>s
  _delete_indexed has type (s,k)=>s
}
```

There are several noteworthy points here:

* the form of the contract itself; the signature for `_index` which accesses elements; and
* the signatures for `_set_indexed` and `_delete_indexed` which return new collections rather than
modifying in-place.

Recall that the `sequence` contract had the form:

```
contract sequence over s determines e is ...
```

the `determines e` clause allows the implementation of the contract to functionally determine (sic) the type of the elements of the collection.

In the case of `indexable`, the contract form is:

```
contract indexable over s determines (k,v) is ...
```

The `indexable` contract determines two types `k` and `v`. The type `k` denotes the type of the key used to access the collection and `v` denotes the type of the elements of the collection. Each individual implementation of `indexable` is free to specify these types; usually in a way that best reflects the natural structure of the collection.

For example, the implementation of `indexable` for `string`s starts:

```
implementation indexable over string determines (integer,char) ...
```

reflecting the fact that the natural index for `string`s is `integer` and the natural element type is `char` (neither being explicitly part of the `string` type name). On the other hand, the implementation for dictionaries starts:

```
implementation for all k,v such that
      indexable over dictionary of (k,v) determines (k,v)
```

reflecting the fact that dictionaries are naturally generic over both the key and value types.

>Notice that we mark `k` and `v` as type variable via the explicit `for all` quantification in the `implementation` statement.

###Tentative Computation[the-option-type]

If we look at the signature for `_index` we can see that this function does not directly return a value from the collection, but instead returns an `option` value. This bears further explanation.

The great unknown of accessing elements of a collection is 'is it there?'. Its not guaranteed of course, and we need to be able to handle failure. 

This is where the concept of 'tentative computation' becomes important. 

**Tentative Computation**
:	A tentative computation is denoted by an expression that is inherently plausible to not have a value.

When we want to open a file, access an element of a dictionary, parse a string with a regular expression we need to be able to express the possibility of failure as well as of success. There are also times when 'no answer' is a legitimate response.

We encode this tentativeness (sic) in the `option` type. The type definition for `option` is straightforward:

```
type option of t is some(t) or none
```
where `none` is intended to denote the non-existence of a value and `some` denotes an actual value.

The `option` type is intended to be used in cases where functions are known to be partial.[^A partial function does not have a value across the whole range of its arguments.] The `option` return type signals that the function may not always have a value.

In the case of the `_index` function, its responsibility is to either return a value wrapped as a `some` value — if the index lookup is successful — or the signal `none` if the index lookup fails. Just to be clear, `_index` can act both as a lookup _and_ as a test for membership in the collection.

In addition to the `option` type, there are a series of operators that make tentative computations easier to express: these are the optional field access operator — `?.` — the option default operator — `or else` — and the `has value` binding predicate.

The `or else` operator allows one to unpack an optional value but to give a default in the case that the optional value is `none`.

We can see where the latter may be useful when accessing dictionaries. For example, the `fillIn` function accesses a dictionary for a key but uses a default value when it is not there:

```
fun fillIn(Tr,Ky,Def) where _index(Tr,Ky) has value Vl is Vl
 |  fillIn(Tr,_,Def) is Def
```

The condition
```
_index(Tr,Ky) has value Vl
```
is satisfied if the `_index` call returns a proper value and it also binds the variable `Vl` to that value (specifically, it _matches_ the value against the variable `Vl`.

While the `has value` operator is very useful in unpacking an optional value, the `or else` operator allows us to handle cases where we always need to be able to give the optional a value. For example, normally a `dictionary` returns `none` if an entry is not present. However, a _cache_ is structured differently: if a value is not present in a cache then we must go fetch it:

```
fun cacheValue(K) is cache[K] or else fetch(K)
```

There is, clearly, a strong relationship between `has value` and `or else`: each can be expressed in terms of the other.

>The `option` type — and the `some` and `none` values — play some of the same roles as NULL does in other languages.

For someone approaching a functional language from most imperative languages they will be struck — and maybe upset — by the lack of a `null` (or `nil` or `NULL` or `undefined`). After all, if it's good enough for Java, why can't **Cafe** have it too?

Perhaps the biggest single reasons for not having a universal NULL value are that it corrupts the type system and that it makes reasoning about programs harder.

In a language which has a universal NULL value, the programmer (and the compiler) must be ever vigilant about references: is the value actually a NULL value? This is true even in those cases where the programmer knows values cannot be NULL. By isolating nullability (sic) into a single concept it allows the programmer to use the feature where it is actually needed.

In a language like Java which has a universal NULL value, the type assigned to NULL turns out to be a little strange. Under most circumstances, a value can be assigned a single unique type; but a universal NULL can be literally of any type. So, universal NULL is a value that denotes no actual value, yet it can be of any type.

This distorts the logic of the type system by introducing a bottom value into the type lattice. **Cafe**'s type system is not based on the concept of sub-types; which makes a universal NULL value even more difficult to accommodate. 

Overall, the `option` type is part of an elegant approach to nullability that is easily incorporated into **Cafe**'s (and similar) type system.

###Adding and Removing Elements From a Collection[adding-and-removing-elements-from-a-collection]

The function `_set_indexed` is used to add an element to a collection associating it with a particular index position; and the function `delete_indexed` removes an identified element from the collection.

Both of these functions have a property often seen in functional programming languages and not often seen elsewhere: they are defined to return a complete new collection rather than simply side-effecting the collection. This is inline with an emphasis on _persistent data structures_[^A _persistent_ structure is one which is never modified.] and on _declarative programming_.

One might believe that this is a bit wasteful and expensive — returning new collections instead of side-effecting the collection. However, that is something of a misconception: modern functional data structures have excellent computational properties and approach the best side-effecting structures in efficiency. At the same time, persistent data structures have many advantages — including substantially better correctness properties and behavior in parallel execution contexts.

>It should also be stressed that the `indexable` contract allows and encourages persistence but does not _enforce_ it. It is quite possible to implement indexing for data structures that are not persistent.

###The Index Notation[the-index-notation]

Given the `indexable` contract we can now show the specific notation that **Cafe** has for accessing elements of a collection.

Accessing a collection by index follows conventional notation:

```
C[ix]
```

will access the collection `C` with element identified by `ix`. For example, given a dictionary `D` of strings to strings, we can access the entry associated with `alpha` using:

```
D["alpha"]
```

Similarly, we can access the third character in a string `S` using:

```
S[2]
```

As might be expected, given the discussion above, the type of an index expression is `option`al. This is because the element may not be there; i.e., it is an example of a _tentative computation_.

The most natural way of making use of an index expression is to use it in combination with a `has value` condition or an `or else` expression — which allows for smooth handling of the case where the index fails. For example, we might have:

```
fun nameOf(F) where names[F] has value N is N
 |  nameOf(F) is ...
```

>We will take a deeper look at exceptions and more elaborate management of tentative computation in the section on [Computation Expressions](computation-expressions).

**Cafe** also has specific notation to represent modified collections. For example, the expression

```
D[with "beta"->"three"]
```

denotes the dictionary `D` with the entry associated with `"beta"` replaced by the value `"three"`. Note that the value of this expression is the updated `dictionary`.

For familiarity's sake, we also suppose a form of assignment for the case where the collection is part of a read-write variable. The action:

```
D["beta"] := "three"
```
is entirely equivalent to:

```
D := D[with "beta"->"three"]
```

always assuming that the type of `D` permits assignment.

Similarly, the expression:

```
D[without "gamma"]
```

which denotes the dictionary `D` where the value associated with the key `"gamma"` has been removed has an action equivalent:

```
remove D["gamma"]
```
is the same as:

```
D := D[without "gamma"]
```

In addition to these forms, there is also a test expression:

```
present D["delta"]
```

which is a predicate that is true if the dictionary `D` contains an entry for `"delta"`.

>Although, in these examples, we have assumed that `D` is a `dictionary` value (where `dictionary` is a standard type in **Cafe**); in fact the index notation does not specify the type. As with the sequence notation, the only requirement is that the `indexable` contract is implemented for the collection being indexed.

In particular, index notation is supported for the built-in `list` types, and is even supported for the `string` type.

###Implementing Indexing

Of course, this includes our own types. For example, before, when looking at [generic types][generic-types] we saw the `tree` type:

```
type tree of t is tEmpty or tNode(tree of t,t,tree of t)
```

We can define an implementation for the `indexable` contract for this type — if we arrange for the tree to be a tree of key-value pairs:

```
implementation for all k,v such that
    indexable over tree of ((k,v)) determines (k,v) where
      comparable over k and
      equality over k is {
  fun _index(T,K) is findInTree(T,K)
  fun _set_indexed(T,K,V) is setKinTree(T,K,V)
  fun _delete_indexed(T,K) is removeKfromTree(T,K)
}
```

>The form of the type expression `tree of ((k,v))` is required to avoid confusion — `tree` takes a single type argument that, in this case, is a tuple type. The extra set of parentheses ensures that `tree` is not interpreted (incorrectly) as a type that takes two type arguments.

With this statement in scope, we can treat appropriate `tree` expressions as though they were regular arrays or dictionaries:

```
def T is tNode(tEmpty,("alpha","one"),tEmpty)
assert T["alpha"]="one"
def U is T[with "beta"->"two"]
assert U["alpha"]="one"
assert present U["beta"]
assert not present U["gamma"]
```

The implementation statement relies on another feature of **Cafe**'s type system — we need to constrain the implementation of `indexable` to a certain subset of possible instances of `tree` types — namely, where the element type of the `tree` is a _pair_ — a two-tuple — and secondly we require that the first element of the pair is comparable — i.e., it has the `comparable` contract defined for it.

This is captured in the contract clause of the `implementation` statement:

```
implementation for all k,v such that
    indexable over tree of ((k,v)) determines (k,v) where
      comparable over k and
      equality over k
```

This implementation statement is fairly long; but it is exquisitely targeted at precisely the right kind of `tree` without us having to make any unnecessary assumptions.

Implementing the contract requires us to implement three functions: `findInTree`, `setKinTree` and `removeKfromTree`. The `findInTree` function is quite straightforward:

```
fun findInTree(tEmpty,_) is none
 |  findInTree(tNode(_,(K,V),_),K) is some(V)
 |  findInTree(tNode(L,(K1,_),_),K) where K1>K is findInTree(L,K)
 |  findInTree(tNode(_,(K1,_),R),K) where K1<K is findInTree(R,K)
```

Notice that each 'label' in the tree is a 2-tuple — consisting of the key and the value. This function is also where we need the key type to be both `comparable` and supporting `equality`. The `comparable` constraint has an obvious source: we perform inequality tests on the key.

The `equality` constraint comes from a slightly less obvious source: the repeated occurrence of the `K` variable in the second equation. This equation is actually equivalent to:

```
fun findInTree(tNode(_,(K,V),_),K1) where K=K1 is some(V)
```

We leave the implementations of `setKinTree` and `removeKfromTree` as an exercise for the reader.

###Index Slices[index-slices]

Related to accessing and manipulating individual elements of collections are the _indexed slice_ operators. An indexed slice of a collection refers to a bounded subset of the collection. The expression:

```
C[fx:tx]
```

denotes the subsequence of `C` starting — and including — the element indexed at `fx` and ending — but _not_ including the element indexed at `tx`.

As might be expected, the index slice notation is also governed by a contract — the `sliceable` contract. This contract defines the core functions for slicing collections and for updating subsequences of collections:

```
contract sliceable over s determines k is {
  _slice has type (s,k,k)=>s
  _tail has type (s,k)=>s
  _splice has type (s,k,k,s)=>s
}
```

The `_slice` function is used extract a slice from the collection, `_tail` is a variant that returns the 'rest' of the collection, and `_splice` is used to replace a subset of the collection with another collection.

Like the indexing notation, there is notation for each of the three cases:

```
C[fx:]
```

denotes the tail of the collection — all the elements in `C` that come after `fx` (including `fx` itself); and

```
C[with fx:tx->D]
```

denotes the result of splicing `D` into `C`. This last form has an additional incarnation — in the form of an assignment statement:

```
C[fx:tx] := D
```

This action is equivalent to the assignment:

```
C := _splice(C,fx,tx,D)
```

which, of course, assumes that `C` is correctly defined as a read/write variable.

>**Cafe** encourages declarative programming but we fully recognize that side-effecting behavioral code is often the most effective solution to the problem.

The slice notation is an interesting edge case in domain specific languages. It is arguably a little obscure, and, furthermore, the use case it represents is not all that common. On the other hand, without specific support, the functionality of slicing is hard to duplicate with the standard indexing functions.

##Doing Stuff With Collections[doing-stuff-with-collections]

One of the most powerful features of collections is the ability to treat a collection as a whole. We have already seen a little of this in our analysis of the [visitor pattern](going-even-further). Of course, the point of collections is to be able to operate over them as entities in their own right. As should now be obvious, most of the features we discuss are governed by contracts and it is paradigmatic to focus on contract specifications rather than specific implementations.

The number of things that people want to do with collections is only limited by our imagination; however, we can summarize a class of operations in terms several patterns:

* Filtering
* Transforming into new collections
* Summarizing collections
* Querying collections

Each of these patterns has some support from **Cafe**'s standard repertoire of functions.

###Filtering With `filter`[filtering-with-filter]

The simplest operation on a collection is to subset it. The standard function `filter` allows us to do this with some elegance. For example, to remove all odd numbers from a collection we can use the expression:

```
filter((X)=>X%2=0,Nums)
```

For example, if `Nums` were the `list`:

```
list of [1,2,3,4,5,6,7,8,9]
```

then the value of the `filter` expression would be

```
list of [2,4,6,8]
```

The first argument to `filter` is a _predicate_: a function that returns a `boolean` value. The `filter` function (which is part of a standard contract) is required to apply the predicate to every element of its second argument and return a _new_ collection of every element that satisfies the predicate.[^The original collection is unaffected by the `filter`.]

Note that the `%` function is arithmetic remainder, and the expression `X%2=0` amounts to a test that `X` is even (its remainder modulo `2` is `0`).

By using a function argument to represent the predicate it is possible to construct many filtering algorithms whilst not making any recursion explicit. However, not all filters are easily handled in this way; for example, a prime number filter _can_ be written

```
filter(isPrime,N)
```

but such an expression is likely to be very expensive (the `isPrime` test is difficult to do well).

###The Sieve of Erastosthenese[original-sieve]

One of the classic algorithms for finding primes can be expressed using filters however — the so-called sieve of Erastosthenes. This algorithm works by repeatedly removing multiples of primes from the list of natural numbers. We cannot (yet) show how to deal with infinite lists of numbers but we can capture the essence of this algorithm using a cascading sequence of `filter` operations.

The core of the sieve algorithm involves taking a list of numbers and removing multiples of a given number from the list. This is very similar to our even-number finding task, and we can easily define a function that achieves this:

```
fun filterMultiples(K,N) is filter((X)=>X%K!=0,N)
```

The overall Erastosthenes algorithm works by taking the first element of a candidate list of numbers as the first prime, removing multiples of that number from the rest, and recursing on the result:

```
fun sieve([N,..rest]) is [N,..sieve(filterMultiples(N,rest))]
```

There is a base case of course, when the list of numbers is exhausted then we have no more primes:

```
fun sieve([]) is []
```

The complete prime finding program is hardly larger than the original filter specification:

```
fun primes(Max) is let{
  fun sieve([]) is []
   |  sieve([N,..rest]) is [N,..sieve(filterMultiples(N,rest))]
  fun filterMultiples(K,N) is filter((X)=>X%K!=0,N)
  
  fun iota(Mx,St) where Mx>Max is list of []
   |  iota(Cx,St) is list of [Cx,..iota(Cx+St,St)]
} in list of [2,..sieve(iota(3,2))]
```

The `iota` function is used to construct a list of numbers, in this case the integer range from `3` through to `Max` with an increment of `2`. We start the sieve with `2` and the list of integers with `3` since we are making use of our prior knowledge that `2` is prime.

>It should be emphasized that the sieve of Erastosthenes hardly counts as an efficient algorithm for finding primes. For one thing, it requires that we start with a list of integers; most of which will be discarded. In fact, each 'sweep' of the list of numbers results in a new list of numbers; many of which too will eventually be discarded.

We might ask wether the `sieve` function can also be expressed as a `filter`. The straightforward answer is that it cannot: the `sieve` _is_ a kind of filter, but the predicate being applied depends on the entire collection; not on each element. The standard `filter` function does not expose the entire collection to the predicate. However, we will see at least one way of achieving the sieve without any explicit recursion below when we look at folding operations.

###Mapping to Make New Collections[mapping-to-make-new-collections]

One of the limitations of the `filter` function is that it does not create new elements: we can use it to subset collections but we cannot transform them into new ones. The `map` function can be used to perform many transformations of collections.

For example, to compute the lengths of strings in a list we can use the expression:

```
map(size,list of ["alpha","beta","gamma"])
```

which results in the list:

```
list of [5,4,5]
```

The `map` function is actually defined via the `mappable` contract — thus allowing different implementations for different collection types:

```
contract mappable over c is {
  map has type for all e,f such that
    ((e)=>f,c of e)=>c of f
}
```

Notice how the contract specifies the collection type — `c` — without specifying the type of the collection's element type. We are using a different technique here than we used for the `sequence` contract. Instead of using functional dependency to connect the type of the collection to the type of the element, we denote the full type of the input and output collections using a type variable as the _type constructor_ as in `c of e` and `c of f`.

The reason for this is that _mapping_ implies creating a new collection from an old collection; with a possibly different element type. This is only possible if the collection is generic and hence the type expressions `c of e` and `c of f`.

>One might ask whether we could not have used a contract of the form:

>```
  contract mappable2 over c determines (e,f) is {
    map has type ((e)=>f,c)=>c
  }
  ```

  However, _this_ contract forces the types of the result of the map to be identical to its input type, it also allows the implementer of the `mappable2` contract to fix the types of the collection elements — not at all what we want from a `map`.

It is not all that common that we need to construct a list of sizes of strings. A much more realistic use of `map` is for _projection_. For example, if we wanted to compute the average age of a collection of people, which is characterized by the type definition:

```
type person is someOne{
  name has type string;
  age has type ()=>float;
}
```

Suppose that we already had a function `average` that could average a collection of numbers; but which (of course) does not understand people. We can use our `average` by first of all projecting out the ages and then applying the average function:

```
average(map((X)=>X.age(),People))
```

In this expression we project out from the `People` collection the ages of the people and then use that as input to the `average` function.

There is something a little magic about the lambda function in this expression: how does the type checker 'know' that `X` can have a field `age` in it? How much does the type checker know about types anyway?

In this particular situation the type checker could infer the type of the lambda via the linking between the type of the `map` function and the type of the `People` variable. However, the type checker is actually capable of giving a type to the lambda even without this context. Consider the function:

```
fun nameOf(R) is R.name
```

This function takes an arbitrary record as input and returns the value of the `name` field. Notice that we don't, and do not need to, explicitly annotate the `nameOf` function — this function will work with _any_ record that has a `name` field.

The `nameOf` function _is_ well typed, and the type inference system is able to assign a quite interesting type to it:

```
nameOf has type for all r,n such that
   (r)=>n where r implements { name has type n }
```

This is another example of a constrained type: in this case, the constraint on `r` is that it has a field called `name` whose type is the same as that returned by `nameOf` itself.

With this type signature, we can use `nameOf` with any type that that a `name` field. This can be a record type; it can also be a type defined with an algebraic type definition.

###Compressing Collections With a Fold[compressing-collections-with-a-fold]

Another way of using collections is to summarize or aggregate over them. For example, the `average` function computes a single number from an entire collection of numbers — as do many of the other statistical functions. We can define `average` using the standard `leftFold1` function:

```
fun average(C) is leftFold1((+),C) as float/size(C) as float
```

>Notice the use of coercion here — coercing both the result of the `leftFold1` and `size` to `float`. The reason for doing this is that functions like `average` are 'naturally' real functions.[^Real as in the Real numbers.] Without the explicit coercion, averaging a list of integers will also result in an `integer` value — which is likely to be inaccurate.

  Of course, we needed to coerce both the numerator and denominator of the division because **Cafe** does not have implicit coercion.

The `leftFold1` function applies a left-associative binary operator to a collection: starting from the first element and successively 'adding up' each of the elements in the collection using the supplied operator. 

The type signature for `leftFold1` details this:

```
leftFold1 has type for all c,e such that
  ((e,e)=>e,c of e)=>e
```

![Left Folding a Collection][folding]

[folding]: images/folding.jpg width=240px

Our definition of the `average` function is therefore about as close to a specification of average as is possible in a programming language!

While it is convenient for computing averages, there are several restrictions in the signature for `leftFold1`: the most egregious is that the result type and the types of the elements of the collection must be identical. A more refined function is possible that liberates us from this:

```
leftFold has type for all c,e,ac such that
  ((ac,e)=>ac,ac,c of e)=>ac
```

In this variant of fold, we separate out an 'accumulator' (of type `ac`) from the type of the elements of the collection.[^We saw something similar with the [visitor pattern][going-even-further].] The `leftFold` function applies its argument function in a similar way to `leftFold1`, except that it does not require that the type of the accumulator is the same as the elements of the collection. Furthermore, it does not use the first element of the collection as a 'seed' of the aggregation — instead, the initial seed is explicitly given.

![Left Folding a Collection With a Seed][leftFolding]

[leftFolding]: images/leftfold.png width=270px

We can still use `leftFold` in situations where we would use `leftFold1`, for example our `average` function is equally well written as:

```
fun average(C) is leftFold((+),0,C) as float/size(C) as float
```

But we can do much more than computing averages with a fold. Recall that when we realized the [sieve of Erastosthenes][original-sieve], we still had a recursive structure to the program. Furthermore, the way our original program was written each filter results in a new list of numbers being produced. Instead of doing this, we can construct a cascade of filter functions.

Consider the task of adding a filter to an existing filter. What is needed is a new function that combines the effect of the new filter with the old one. The `cascade` function takes a filter function and a prime as arguments and constructs a new function that checks both the prime _and_ the existing filter:

```
fun cascade(F,K) is (X)=>(F(X) and X%K!=0)
```

This is a truly higher-order function: it takes a function as argument and returns a another function.

>Notice the use of parentheses in the returned lambda. This is because the operator priority of `and` is higher than that of the lambda operator `=>`.

Given `cascade`, we can reformulate the sieve function itself as a `leftFold` — at each new prime step we 'accumulate' a new cascaded filter function:

```
fun step(F,X) where F(X) is cascade(F,X)
 |  step(F,X) is F
```

At each step in the fold we want to know whether to continue to propagate the existing filter or whether to construct a new filter — the conditional expression allows us to achieve that.

The `sieve` function itself is now very short: we simply invoke `leftFold` using `step` and an initial 'state' consisting of a function that checks for odd numbers:

```
fun sieve(C) is leftFold(step,(K)=>K%2!=0,C)
```

The initial 'state' is a function that filters out even numbers

This version of `sieve` is not quite satisfactory as, while it does find prime numbers, it does not report them. A more complete version has to also accumulate a list of primes that are found. We can do this by expanding the accumulated state to include both the cascaded filter function and the list of found primes. The main alteration is to the `step` function:

```
fun prStep((P,F),X) where F(X) is (list of [P..,X],cascade(F,X))
 |  prStep((P,F),_) is (P,F)
```

and the initial state has an empty list:

```
fun sieve(C) is first(leftFold(prStep,(list of [],(K)=>K%2!=0),C))
```

where `first` and `second` pick the left and right hand sides of a tuple pair:

```
fun first((L,R)) is L
fun second((L,R)) is R
```

There is one final step we can make before leaving our sieve of Erastosthenes — we can do something about the initial list of integers. As it stands, while the program does not construct any intermediate lists of integers, it still requires an initial list of integers to filter for primes. However, this particular sequence can be represented in a very compact form — as a `range` term.

`range` terms are special forms of collections that denote ranges of numeric values; using a `range` term, we can denote the list of primes less than 1000 with

```
fun primes(Max) is let{
  fun cascade(F,K) is (X)=>(F(X) and X%K!=0)
  fun prStep((P,F),X) where F(X) is (list of [P..,X],cascade(F,X))
   |  prStep((P,F),_) is (P,F)
  fun sieve(C) is first(leftFold(prStep,(list of [],(K)=>K%2!=0),C))
  fun first((L,R)) is L
} in sieve(range(3,Max,2))

show primes(1000)
```

This final program has an important property: there are no explicit recursions in it — in addition, apart from the `leftFold` function, there are no recursive programs at all in the definition of `primes`.

As before, the `leftFold` function is not defined directly but is part of a `foldable` contract. This contract defines both `leftFold` and `leftFold1` and the corresponding `rightFold` and `rightFold1` functions for right associative operations. The `foldable` contract has a further surprise for us:

```
contract foldable over c determines e is {
  leftFold has type for all st such that ((st,e)=>st,st,c)=>st;
  leftFold1 has type ((e,e)=>e,c) => e;
  rightFold has type for all st such that ((e,st)=>st,st,c)=>st;
  rightFold1 has type ((e,e)=>e,c)=>e;
}
```

If you look closely at the type signature for `leftFold` (and also `rightFold`) you will see that there is an explicit quantifier. Up until now, any time that we have had a quantifier _inside_ a type; it has always been at the outermost — or left most — level.

What the special quantification means is that any implementation of the `foldable` contract must allow for a generic transition function — from state to state. Exactly what we would expect of a contract for folding sequences: the implementation must not constrain the accumulator function unnecessarily.

The `rightFold` and `rightFold1` functions process collections 'from the right' rather than 'from the left'. In most cases, you will be using the left oriented functions as most collections are optimized for accessing in order.

##Iteration[iteration]

We now take a look at how we can process collections using actions rather than expressions. Iteration is one of those areas where history has resulted in two quite different traditions: OO-style languages and functional languages have markedly different approaches to iteration; and yet, as we will see, they can be seen as twins of each other.

A classic iteration over a collection, written in Java in this case, looks something like:

```
int len = 0;
for(String s:myColl){
  len = len+s.length();  // do something with s
}
```

As we have seen, the functional approach to this kind of computation would be to capture the implicit recursion into a use of `leftFold`:

```
leftFold((A,E)=>A+size(E),0,myColl)
```

Although the Java and **Cafe** code fragments are computing the same value — the total length of string data in the collection — and even though they are nearly the same length; there are radical differences between the two, differences that can make a substantial difference in large programs.

The first salient point is that the iteration/recursion is exposed in the Java code and hidden in the **Cafe** code. This is potentially very significant in the event that we want to change how the iteration is implemented — replace a sequential iteration by a parallel one, for example. More subtly, if there are any _access_ issues with the collection — if access to it requires special care with locking or related features — these same issues can be dealt with once — in the implementation of `leftFold` rather than repeatedly for each loop.

The second salient feature is that the relationship between the iteration and the body of the iteration is inverted. We can see this if we unpack the Java loop, which involves an explicit `Iterator` object:

```
int len = 0;
for(Iterator<String> it=myColl.iterator();it.hasNext();){
  String s = it.next();
  len = len+s.length();  // do something with s
}
```

The `Iterator` object 'carries' most of the information needed to process the collection properly. Each call to `next` results in `s` being bound to the next element of `myColl`.

If we squint appropriately, we can see that the body of the Java loop 'drives' the iteration — via the call to `it.next()`. In effect, the body code is the _client_ of the collection, and the `Iterator` object is the server in the code fragment.

In the case of the functional code, the loop is encapsulated in the `leftFold` function which drives the loop by calling the 'body function' when needed. It is possible to write Java code that approximates to this style; it would be something like:

```
Folding.leftFold((A,E)->{return A+E.length()},0,myColl)
```

assuming that `leftFold` were a static function in the `Folding` class.[^This relies on new syntax introduced in Java 8. The `leftFold` would be much clumsier in earlier versions of the language.]

The third saliency is not actually obvious from these fragments, but the functional approach has more variability in the loop structures than Java. Java has several forms of loop, but there is only one loop that is oriented to processing collections — the 'for each' loop. On the other hand, in **Cafe** — like most functional programming languages — we have `map` to transform a collection, `filter` to remove unwanted elements, `leftFold` and its relatives to reduce collections.

The range of looping functions reflects an underlying vocabulary of 'things we do' to collections.

The final, perhaps most unexpected point, is that the fundamental computational cost of the two styles of iteration is almost identical! Under reasonable assumptions of optimization for both Java and **Cafe**, it is possible to show that the number of steps needed to compute the two fragments is very similar. The details are beyond the scope of this book.

##Queries[queries]
Consider, if you will, the problem of finding a set of grandparent-grandchild pairs — given information about parent-child relationships. For example, suppose that we had a `list` of parents and children:

```
def parent is list of [("john","peter), ("peter","jane"), ... ]
```

and that we wanted to construct a result along the lines of:

```
def GC is list of [("john","jane"),...]
```

This involves searching the `parent`s for pairs or pairs that satisfy the grandparent relationship. Based on the collection operators we have seen so far, we can build such a search using two `leftFold` operations:

```
leftFold(
  (SoFar,(X,Z)) => leftFold(
    let {
      fun acc(gp1,(ZZ,Y)) where Z=ZZ is list of [gp1..,(X,Y)]
       |  acc(gp1,_) is gp1
    } in acc,
    SoFar,parent),
  list of [],
  parent)
```

This, rather intimidating, expression uses one `leftFold` to look for the grandparent, for each candidate grandparent a second `leftFold` finds all the grand-children. All without any explicit recursion.

>The `acc` function defined above in the `let` expression implements the logic of deciding what to accumulate depending on whether we had found a grandparent or not.

The various `filter`, `map` and `leftFold` functions _are_ powerful ways of processing entire collections. However, as we can see, they can be difficult to construct and harder to follow; something that is not helped by the occasional need to construct complex functions in the middle, as in this case.

It turns out that **Cafe** has a special notation that makes this kind of complex computation significantly easier to write and comprehend. **Cafe**'s query notation is a very high level way of expressing combinatorial combinations of collections. We can write the equivalent of the previous grandparent expression in this query notation as:

```
def GC is list of { all (X,Y) where 
                      (X,Z) in parent and (Z,Y) in parent }
```

It may not be obvious, but these expressions compute the same values! What is obvious is that the query is much easier to read and easier to verify that it is correct.

>The syntax and style of **Cafe**'s query notation is similar to SQL's syntax — deliberately so.

  Specifically, we take SQL's _relational calculus_ subset — the language of `where`s and of boolean combinations. **Cafe**'s query expressions do not have the equivalent of explicit relational `join` operators.

There are several variations of query expression, but the most common form is:

_SequenceType_ `of {` _QuantifierTerm_ `where` _Condition Modifier_ `}`

where _SequenceType_ is any type name that implements the `sequence` contract, _QuantifierTerm_ is a form that indicates the form of the result of the query, _Condition_ is a condition and the optional _Modifier_ is used to signal properties of the result — such as whether the result is grouped or sorted.

###Satisfaction[satisfaction]

The foundation for the query notation is the notation for conditions. Conditions are boolean valued — but they are not always expressions. For example, the first condition for grandparent is that there is a parent; this was expressed using the condition:

```
(X,Z) in parent
```

This condition is not evaluated in the way that expressions are normally evaluated — by _testing_ to see if a given pair of `X` and `Z` are in the `parent` collection. Instead, the condition is evaluated by trying to find `X` and `Z` that are in the collection. In effect, the condition becomes a search for suitable candidate pairs.

Technically this is called _satisfying_ the condition — to distinguish what is going on from _evaluating_ the condition. Of course, satisfying and evaluating are close cousins of each other and amount to the same thing when there is no search involved.

In addition to individual _search conditions_ like this, it is also possible to use logical operators — called _connectives_ — to combine conditions. In the case of our grand-parent query, there is a conjunction; which involves a variable `Z` that acts as a kind of glue to the two search conditions.

In database parlance the conjunction amounts to an inner join operation; however, it is also simply logical conjunction.

The available connectives include the usual favorites: `and`, `or`, and `not`. They also include some less familiar connectives: `implies` and `otherwise`.

The `implies` connective is a way of testing complete compliance with a condition; for example, we can define a query capturing the situation that a manager earns more than his/her members by requiring that anyone who works for the manager earns less than they do:

```
fun managerOk(M) is (X,M) in worksFor implies X.salary=<M.salary
```

Notice that we can use conditions' satisfaction-oriented semantics outside of query expressions.

###Query Quantifiers[query-quantifiers]

The _QuantifierTerm_ in a query specifies 'how many' answers we want. There are essentially three forms of _QuantifierTerm_ — if we want all the answers then we use a term of the form:

```
{ all (X,Y) where ... }
```

On the other hand, if we want a fixed number, then we use:

```
{ 5 of (X,Y) where ... }
```

Of course, there might not be five answers, and so this is called a bounded _QuantifierTerm_.

We have only scratched the surface of possibilities of query expressions here. They are, in fact, one of **Cafe**'s most powerful and high-level features.

##Types of Collection[types-of-collection]

Just as there are many uses of collections, so there are different performance requirements for collections themselves. The most challenging aspects of implementing collections revolves around the cost of adding to the collection, the cost of _accessing_ elements of the collection and the cost of _modifying_ elements in the collection.

>There is a strong emphasis on _persistent_ semantics for the types and functions that make up **Cafe**'s collections architecture. This is manifest in the fact, for example, that functions that add and remove elements from collections _do not_ modify the original collection.

However, even without that constraint, different implementation techniques for collections tend to favor some operations at the cost of others. Hence, there are different types of collection that favor different patterns of use.

###The `cons` Type[the-cons-type]

This is the simplest collection type; and is perhaps the original collection type used in functional programming languages. It is defined by the type declaration:

```
type cons of t is nil or cons(t,cons of t)
```

Cons lists have the property that adding an element to the front of a list is a constant-time operation; similarly, splitting a `cons` list into its head and tail is also a constant time operation. However, almost every other operation is significantly more expensive: putting an element on to the end of a `cons` list is linear in the length of the list.

###The `list` Type[the-list-type]

The `list` type offers a different trade-off to the `cons` type: where the latter is optimal for ease of constructing and for traversing complete lists, the `list` type offers constant-time access to random elements within the array — at the potential cost of more expensive construction of lists.

Unlike the `cons` type, the `list` type does not have a straightforward definition as an algebraic type. Internally, an `list` structure consists of an array of locations with a 'control pointer' giving the portion of the array block that represents a given `list` value.

The `list` type is optimized for random access and for shared storage — recall that **Cafe** collection types are persistent: that means that different values can share some or all of their internal structure. The  [diagram](twoArrays) shows two `list` values that overlap in their elements and consequently share some of their structure.

![Two `lists` Sharing Structure][twoArrays]

[twoArrays]:images/twoarrays.jpg width=270px


###The `dictionary` Type[the-dictionary-type]

Unlike the `cons` or `array` type, the `dictionary` type is oriented for access by arbitrary keys. The `dictionary` is also quite different to hash trees as found in Java (say), the `dictionary` type is _persistent_: the functions that access dictionaries such as by adding or removing elements return new dictionaries rather than modifying a single shared structure. However, the efficiency of the `dictionary` is quite comparable to Java's `HashMap`.

The template for the `dictionary` type is:

```
for all k,v such that dictionary of (k,v) where equality over k
```

Notice that there is an implied constraint here: the `dictionary` assumes that the keys in the dictionary can be compared for equality.

A `dictionary` value can be written using the sequence notation, using tuple pairs for the key-value pairs:

```
dictionary of [(1,"alpha"),(2,"beta")]
```

Dictionaries also have a special variant of the sequence notation; instead of writing the pairs as tuples we can use an arrow notation for `dictionary` terms:

```
dictionary of [1->"alpha", 2->"beta"]
```

Dictionaries also have their own special variant of a _query search condition_. A condition of the form

```
K->V in D
```

where `D` is a `dictionary` will be satisfied if there is a key/value pair in `D` corresponding to `K` and `V`. For example, the condition:

```
K->V in dictionary of [1->"alpha", 2->"beta"] and V="alpha"
```

is satisfied for only one pair of `K` and `V`: `1` and `"alpha"` respectively.

>For the curious, dictionaries are implemented using techniques similar to Ideal Hash Trees, as described by Bagwell [][#Bagwell01idealhash]. This results in a structure with an effective O(1) cost for accessing elements _and_ for modifying the dictionary — all the while offering an applicative data structure.

###The `range` Type[the-range-type]

The `range` type is a very particular form of collection type: a `range` denotes a range of numbers. Its type description says it all:

```
type range of t where arithmetic over t and comparable over t is
  range(t,t,t);
```

This type description has some special features; in particular it is a constrained type: a type expression of the form:

```
range of T
```

is only valid if `T` is an arithmetic type; specifically a type that supports `arithmetic` and is `comparable`. Thus a type expression such as `range of integer` is fine, but `range of list of integer` will result in a syntax error!

Range terms are used to compactly represent regular ranges of numbers; for example the term

```
range(0,100,1)
```

denotes the first 100 integers. But, of course, we have already seen the `range` collection in our exploration of the [sieve of Erastosthenes](original-sieve).

There is a specific property of the `range` type that is difficult to capture with this type definition — specifically, we rely in many places on `range`s _half closed_ property: that is, the range of numbers in the range include the first number but does not include the second. This property makes combining ranges much smoother than either a fully closed range (includes both ends) or an open range (includes neither end).

For example the following assertion is expected to hold for `range` terms:

```
range(F,I,Ix)++range(I,T,Ix)=range(F,T,Ix)
```

where `++` is the standard function — i.e., is part of the `concatenate` contract — for expressing sequence concatenation.

##Queries and Maps for Statistical Purposes

We wrap up our exposition on collections with an example that highlights how we can combine many of the collection manipulation features; with a specific goal of statistical processing of data.

Statistics is, of course, one of the key application areas of computers in general. However, there is often a substantial gap between the theoretical aspects of statistical processing and the pragmatics of collecting and processing data. We aim to demonstrate **Cafe**'s power in both areas.

One fecund source of statistics is the web; we can even get statistics about the web. The on-line tool [Web Page Test](http://www.webpagetest.org) can be used to generate a lot of data about how a browser responds to a website. Furthermore, we can down this data as a file of comma separated values (CSV). The first few lines of this file shows the detail of the data collected:

```
Date,...,URL,Response Code,Time to Load (ms),...,Bytes In,...
9/12/14,...,/,302,397,...1272,...
```

There are over 70 columns there. Suppose that we wanted to process this to find out how much time is spent loading Javascript data. Our first step is to introduce the CSV file to **Cafe** which we can do with a special macro:

```
import metaCSV
worksheet{
  generateCSV Wpt from "file:WPT_Sample.csv"

  ...
}
```

The `generateCSV` line is a [macro][macro-rules] that parses the sample CSV file, constructs a type (`Wpt`) that reflects the entries in the file and a parser that can parse similar CSV files into `Wpt` entries. The generated `Wpt` type looks like:

```
type Wpt is Wpt{
    Date has type string
    ...
    URL has type string
    Response Code has type integer
    Time to Load \(ms\) has type integer
    ...
    Bytes In has type integer
    ...
  };
```

One immediate feature to notice is that some of the field names have back-slashes and other decidedly non-identifier-like characters in them! **Cafe** allows a variable (and by extension a field) identifier to have any characters in them — so long as they are escaped. This allows us to access 'foreign' data structures like this CSV file in a straightforward way.

Given the implicit generation of the type, and of a parser, we can use this to process real data files. For example, we can extract out from the CSV file records containing just the URLs, the file sizes and the load times using a query:

```
import metaCSV
worksheet{
  generateCSV Wpt from "file:WPT_Sample.csv"

  def wptData is WptParser("http:www.webpagetool.org/...csv")
  def extracted is list of { all 
    { URL=D.URL
     size=D.Bytes\ In
     loadTime=D.Time to Load \(ms\)} where
    D in wptData}
  ...
}
```

We used the generated parser to reach out to the website for the data and to parse the resulting content — in a single high-powered statement.

A single run of the web page test may involve many separate browser actions — the purpose of the tool is to test the performance of a website, which needs many trials to achieve statistical significance. So a better organization of the data is to categorize the raw data by URL. We can do this using a `group by` query:

```
import metaCSV
worksheet{
  generateCSV Wpt from "file:WPT_Sample.csv"

  def wptData is WptParser("http:www.webpagetool.org/...csv")
  def extracted is list of { all 
    {URL=D.URL
     size=D.Bytes In
     loadTime=D.Time to Load \(ms\)} where
    D in wptData} group by ((X)=>X.URL)
  ...
}
```

The `group by` operator takes a collection, and a categorization function, are produces a dictionary of collections.

We want to produce average load times and standard deviations of the load times — to smooth out the vagaries of the Internet. For now, we will assume that we have `average` and `stddev` functions that have type signatures:

```
average has type for all c,e such that
  (c,(e=>float))=>float where sequence over c determines e
stddev has type for all c,e such that
  (c,(e=>float))=>float where sequence over c determines e
```

I.e., we are assuming functions that take collections, and an 'accessor' function, and return the average and standard deviation respectively.

Given these functions, we can compute our statistics using:

```
import metaCSV
import stats;
worksheet{
  generateCSV Wpt from "file:WPT_Sample.csv"

  def wptData is WptParser("http:www.webpagetool.org/...csv");
  def extracted is list of { all 
    { URL=D.URL;loadTime=D.Time to Load \(ms\)} where
        D in wptData} group by ((X)=>X.URL)
        
  fun ldTime(D) is D.loadTime as float
  show list of { all (K,average(V,ldTime),stddev(V,ldTime)) where
        K->V in extracted }
}
```

The query condition:

```
K->V in extracted
```

is analogous to the regular search condition:

```
D in wptData
```

except that it is used to search within a dictionary-based collection. The pattern `K->V` matches the successive key/value pairs in the dictionary.

That is it! The final query should get output along the lines of:

```
...
("/o/oauth2/auth?...", 337.0, 0.0),
("/main-thumb-58380181-100-yqpttun...", 101.18518518519, 24.41468441456),
("/main-thumb-49759239-100-pcgldxn...",93.48148148148, 19.03285636867),
...
```

Notice that most of the remaining complexity in this example related to selecting the right parts of the data to pick up and process.[^A standard deviation of `0.0` is likely a signal that the indicated URL only occurred once in the sample data.]

##Summary[summary]

Collections form an important part of any modern programming language. The suite of features that make up the collections architecture in **Cafe** consists of a number of data types, contracts and special syntax that combine to significantly reduce the burden of the programmer.
#Boiling the Ocean[boiling-the-ocean]

It is a commonplace in software engineering that you should not try to 'boil the ocean'; which is a synonym for 'biting off more than you can chew'. However, it _is_ possible to build very large scale systems if you approach it in the right way. This is the purview of architecture.

>The 'right' way to boil an ocean is one cup at a time. The 'smart' way to do it is to build a machine that makes cups that boil the ocean.

All software projects are team efforts. Even if you feel you wrote everything in your project yourself, it is unlikely that you also wrote the compiler, run-time library, editor and operating system too! Even within a single project, most of the time you are working in the context of a team of co-developers and other professionals.

Building software in the context of a team is quite different to writing individual functions in a program. It is not enough for your code to compute the correct function; it must also interact properly with the environment it is in. As a result, professional programmers find themselves often more concerned with making sure that all the 'pieces' are in the right place than simply the correctness of an algorithm.[^This is not to deny that correctness is important. It is just that algorithm correctness is _not enough_.] Interfaces, contracts, APIs and integration issues often dominate the system builder's landscape.

>One's attitude to basic language features like types is also different: having to deal with the knowledge that is in your co-worker's head (and not yours) should be enough to convince anyone of the merits of strong static types.


**Cafe**'s modularity features are built from the same functional programming foundation as the other features of the language. This has important implications for the programmability of larger systems.

##Packages and Worksheets[packages-and-worksheets]

We have already seen the two basic forms of compilation unit in **Cafe**: the `package` and the `worksheet`.

>Since **Cafe** is an extensible language, it is quite possible to create other forms of compilation unit.

### Package [package]


**Package**
: A `package` is a collection of definitions that are intended to represent some coherent functionality or purpose.

This is, of course, a vague and non-actionable definition. There _are_ higher-level notions of what 'functionality' might mean: for example, in the context of Service Oriented Architecture, a **service** is the 'manifestation of a business functionality'. If that is clearer, then a `package` is a manifestation of an application functionality. Purpose is a topic we will return to in our chapter on agents.

For all of its vagueness, the basic idea of a `package` is that it is a container for a collection of definitions. **Cafe**'s modularity makes it quite straightforward to refactor your package hierarchy should you need to.

The form of a `package` is

_Identifier_ `is package {` _Definitions_ `}`


The _Identifier_ at the beginning of the package definition is the name of the package. Normally it has little role within the package itself; it is used when referring to the package externally.

Most of the _Definitions_ are those that we have already seen: function definitions, variable definitions and type definitions. There are some additional forms however, including the `import` statement — that allows one to import elements from other packages.

####The `main` Program[the-main-program]

If a package defines a main procedure[^A procedure, in **Cafe**, is a function that returns the void tuple `()`.] then that package may act as a command-line program as well as a package. There are two styles of main program possible; one where command line arguments are automatically converted into regular values and one where you get the arguments as a `list of string`s.

If the `main` procedure is defined then automatic coercion from command line arguments to internal types is performed. For example,

```
myProgram is package{
  main has type (integer,string)=>()
  prc main(Count,Name) do {
    ...
  }
}
```

With this style of `main`, a command-line invocation of the program is possible:

```
$ Cafe myProgram 34 fred
```

where the run-time verifies verifies that exactly two arguments are passed, the first being an integer. In principle, the arguments of `main` may be of _any_ type — so long as the type is known to implement the `coercion` contract:

`coercion over (string,`_T_`)`

where _T_ is the type being passed to the `main` function. I.e., so long as there is a way of parsing a `string` value we can pass such arguments from the command line to our main program. However, there is another constraint: the ability of the operating system to pass in arbitrary strings to a command line program.

>For example, `list` structures are difficult to get past the typical shell:

```
$ Cafe myProgram "list of [1,2,3]"
```

In practice, this means that most simple types can easily be passed to a `main` procedure; and, with some difficulty, collection types such as `list of integer` may also be used.

>This capability is not often provided in programming languages. Normally, you are limited to passing in an array of strings to the top-level main function. Why this is the case is a question best asked of the respective programming language designers.

However, for program safety reasons, it is not permitted to coerce strings into _functions_ or other forms of code (the ability to coerce a string into a function amounts to dynamic compilation).

If you want to manage all the command-line arguments, then define the procedure `_main` instead:

```
myProgram is package{
  _main has type (list of string)=>()
  prc _main(args) do { ... }
}
```

The `_main` procedure is given all the command line arguments as a `list of string`. This variant of `_main` (you cannot have both in the same package) is useful if you want to process command line arguments with dash-options.

###Worksheet[worksheet]

We have already seen a number of examples of using worksheets. They represent a modern replacement for the traditional REPL — or Read-Eval-Print-Loop. The main advantages of worksheets over the traditional REPL are that they simultaneously act as their own transcript and they integrate well with editor-centric IDEs.

>We prefer worksheets to normal packages for the same situations as one might prefer a REPL over a traditional file: worksheets make it easy to set up simple experiments and 'see what happens' when you try something.

The contents of a worksheet may be anything that counts as a definition — including `import` statements. In addition, there are some special 'actions' that may be specified; some of which we have already seen:

**`perform`**
: `perform` an action. For example:

```
worksheet{ 
  perform task { 
    logMsg(info,"This is an action")
  }
}
```

`perform` is used to perform a `task` expression.

**`ignore`**
: Evaluate an expression for side-effect purposes; ignoring its value.

  This is used in those cases where a computation has a value that we wish to ignore – i.e., is evaluated for its effect only. 

  In many programming languages expressions are automatically 'promoted' to statements. For example, if you have the expression `3+X` in Java (say) then it is also a statement:

```
while(true){
  3+X;
}
```

Of course, in this case, the expression `3+X` doesn't do anything; but the feature is commonly used to allow a function call in a situation where the returned value is thrown away or 'ignored'.

**Cafe** does not have an _automatic_ way of promoting expressions to actions; but you can use the `ignore` keyword for the same effect. The action

```
ignore 3+X
```

is the way that you would write `3+X` as an action.

**`show`**
: Evaluate an expression and `show` its value.

For example:

```
worksheet{ show 1+2 }
```

`show` is probably the most used special action in a worksheet. After all, the purpose of a worksheet is to help figure out what is going on...

**`assert`
: `assert` a condition.

For example:

```
worksheet{ assert 3>2 }
```

**`{` ... `}`**
: Perform a block of actions, enclosed in parentheses.

For example:

```
worksheet{
  {
    for Ix in range(0,10,1) do{
      logMsg(info,"hello $Ix")
  }
}
```

In many cases, the actions that are used most often are the `show` and the `assert` actions; in conjunction with a set of regular definitions. This reflects one of the main purposes of the worksheet: to give some insight into the behavior of a program.

Unlike a traditional REPL, because the worksheet is in a file, there is much more freedom within a `worksheet` to order things. In particular the definitions can be in any order: a later definition may be referenced explicitly or implicitly in a `show` action.

>It _is_ possible to include actions in a regular `package`. However, especially in cases where the package is to be imported into another program this practice is not recommended.

###Importing Packages[importing-packages]

There are two basic ways of importing a package[^You cannot import a worksheet.] into your code: the _open import_ and the _named import_. The open import is the simplest; to import a package you just import it (sic):

```
worksheet{
  import myPackage

  -- Use definitions from myPackage
}
```

You can also import packages outside the `worksheet` structure:

```
import myPackage
worksheet{
  -- Use definitions from myPackage
}
```

Any definition that is contained in `myPackage` is available throughout the worksheet (or other package if you are building a
package).

>Like other forms of definition, the `import` statement may appear anywhere at the top-level of the importing `worksheet` or `package`. However, it is normally at the beginning of the package.

The second way of importing a package is to use the _named import_. As suggested, a named import associates a local identifier with the import:

```
worksheet{
  mP is import myPackage

  -- Use definitions from myPackage via mP
}
```

For example, suppose that `myPackage` looked like:

```
myPackage is package{
  type foo of t is foo(t) or bar

  fun unFoo(foo(X)) is X
}
```

To use the `unFoo` function in our second worksheet, we simply reference it as a field in the `mP` variable:

```
worksheet{
  mP is import myPackage

  fun getTheStuff(F) is mP.unFoo(F)
}
```

What may be a little surprising is that this applies to the `foo` type also:

```
worksheet{
  mP is import myPackage

  getTheStuff has type (mP.foo of integer)=>integer
  fun getTheStuff(F) is mP.unFoo(F)
}
```

and also to the `foo` and `bar` constructors:

```
worksheet{
  mP is import myPackage;

  fun wrapF(nonInteger) is mP.bar
   |  wrapF(X) is mP.foo(X)
}
```

One of the benefits of the named import is that it makes it possible to import packages even when there are potential clashes amongst the packages being imported and/or definitions in the importing package itself.

By using named imports the effect is to establish a local _namespace_ for the imported package. Different definitions imported from different places can be reliably distinguished using the normal record field access syntax (i.e., a period).

####Private Imports[private-imports]

In addition to accessing imported definitions, an `open` import has the additional effect that those definitions are automatically re-exported by the importing package.

>This makes sense for two reasons: it makes it straightforward to construct a _library_ in terms of other packages; and, secondly, it allows builders of a library to _hide_ essential aspects of the _implementation_ of the library (the user of a library does not need to know which individual packages the different elements of the library came from).

However, it is not always desirable to automatically re-export the contents of a package. To suppress this, we can mark the import statement with the keyword `private`:

```
private import myPackage
```

Any definitions that are in a privately imported package are not re-exported.

In fact, the `private` keyword is not specific to importing packages. There are other situations where it is required to manage the externally visible definitions. A common case of this is in a record expression; such as:

```
def F is {
  fun fib(N) is fibT(N,1,0,0)

  private
  fun fibT(1,X,Y,Z) is X
   |  fibT(I,X,Y,Z) is fibT(I-1,Y,Z,X+Y)
}
```

The `fibT` function is strictly part of the implementation of `fib` and is not, nor should it be, part of the `F` record. The `private` keyword ensures that `fibT` is not exposed.

In fact, `private` is more for convenience than necessity; it does not introduce any new expressiveness to the language. For example, the above definition for `F` can be rewritten using a `let` instead of `private` definitions:

```
def F is let{
  fun fibT(1,X,Y,Z) is X
   |  fibT(I,X,Y,Z) is fibT(I-1,Y,Z,X+Y)
} in { fun fib(N) is fibT(N,1,0,0) } -- actual record
```

However, such transformations have the potential for distorting the programmer's intention. In particular, if several definitions are being privatized (sic) then the resulting program may be quite distorted.

The same applies to privately importing packages: it is possible to restructure a package so that no imports are explicitly `private`, but the result of adding `let` expressions into the program structure may well be ugly.

>It is probably good practice to always use `private` import of packages; unless you are deliberately designing a library package.

>>If, as we suggest, `private` imports are good practice, then why does the language not make that the default form of import and instead require you to explicitly publish those elements you want to export?

  The answer is that `private` is useful in many other contexts than importing packages. In most of those contexts, the better default is to explicitly mark elements as private than to mark exported ones. Such is the life of a language designer!


##Code Products[code-products]

One important question that must be answered in any scheme that permits importing is "where is the code coming from?". **Cafe** has three architectural elements that are the basis of code management: the _code repository_, a system of _universal resource identifiers_ (URI) to identify packages uniquely, and _catalog_s to reduce the bureaucratic burden.

>The issues that show up when managing resources tend to fall in the 'annoyingly complex' rather than 'rocket science' category. However,that does not make them less important, and addressing them certainly helps with that oceanic problem.

###Code Repository[code-repository]

Apart from simply being a place compiled code can be kept, a _code repository_ has several other responsibilities: it must be possible to access the compiled code from multiple packages — including packages that were imported. In fact, a **Cafe** code repository is able to contain the compiled code of any number of packages — since code repositories are also used to hold compiled libraries as well as applications.

In addition, we have to be able to manage multiple _versions_ of a given compiled package; in some cases an application may be using multiple versions of a package (different libraries may have dependencies on specific versions of a package).

The scale of the repository is also flexible: ranging from a and we must be able to manage different kinds of code repository: from individual packages, libraries of packages and even large-scale collections of libraries.

We will not go into the details of **Cafe**'s code repositories here. However, note that code repositories may store their code in different ways — file systems, memory, in a _Star Archive_. Furthermore, repositories can be combined into composite repositories.

###Universal Resource Identifier[universal-resource-identifier]

Each package is identified by a URI. The intention of a URI is that a given URI identifies exactly one package.

>The URI has an IETF standard specification (RFC 2396) [][#rfc2396].


One important source of confusion with the IETF URI is the distinction between URIs and URLs. Although they share a common syntax a URI is _not_ intended to convey the _location_ of the resource.

For example, the URI

```
file://foo/bar/x/y.Cafe
```

looks like a file-based name: one might be tempted to believe that the file `x.Cafe` resides in the directory `/foo/bar/x`. While it is possible that it might, there is actually no such commitment. Furthermore, the same resource may be someplace else (and _will be_ if the identified file is in a code repository).

####Transducer[transducer]

The connection between a URI and the actual resource must be established by a _transducer_. A transducer is any system that can take a URI and produce a copy of identified resource. If you will, the transducer establishes the link between a URI and a URL.

The **Cafe** compiler has a range of transducers built-in to it; and also has a extensibility mechanism so that you can define your own URI scheme and have it mapped to some physical storage mechanism.

####Standard URI Schemes[standard-uri-schemes]

Although the **Cafe** compiler can, in principle, utilize any URI scheme, some schemes are 'built-in' — in the sense that there is a standard transducer for them. It is obviously an implementation dependent aspect of the language: different compilers may support different schemes; but all should support at least the following:

**Cafe**
: E.g. `Cafe://foo/bar/gar.Cafe`. This is intended to be used as a system dependent but universal way of identifying **Cafe** files. I.e., each system is free to choose how to find resources using the **Cafe** scheme in its own way. It is also  expected that methods would be provided for importing and exporting resources between the system and other systems (e.g., mobile vs desktop computers).

**http**
: E.g., `http://www.example.com/foo/bar.Cafe`. This is intended to denote resources that are accessibe in some way from the web.

**std**
: E.g., `std:arithmetic.Cafe`. This is intended to represent internally important resources — typically **Cafe** packages that are part of the standard language definition

In addition, the compiler may support the **file** scheme and other schemes as appropriate.

####Package URIs[package-uris]

We stated earlier that an `import` statement looks like:

```
import myPackage
```

where `myPackage` was an identifier that named the required package to import. Such simple identifiers hardly qualify as URIs of course. In fact, the full form of an import is:

`import` "_scheme_:/_host_/_path_?_version_"

where _scheme_ is one of the supported URI schemes, _path_ is the name of the resource and _version_ is an optional _version specification_.

It turns out that, most of the time, specifying imported packages with explicit URIs in this way is awkward; and subject to errors when refactoring a code base.[^fn2]

[^fn2]:The linguistically precise version of the `import` statement would be even worse; since strings are not actually the same thing as URIs. The accurate version should have been something like:

	```
	import "http:/foo.com/bar/x/y.Cafe" as uri
	```

However, while correct, this is unnecessarily clumsy. Instead, we allow identifiers to name package URIs (as we have been throughout this book) and use our third mechanism to bridge the gap: the _catalog_.

###Catalogs[catalogs]

A _catalog_ is a mapping from identifiers to package URIs; i.e., it is the missing link between package identifiers and package URIs. Catalogs are sometimes written explicitly — in the form of a file that you can place in the same directory as your source files — but usually the compiler makes an automatic catalog depending on where the source code of a package is actually located.

A `catalog` is a **Cafe** term that looks like:

```
catalog{
  version = "2.3.1";
  baseURI = "http://path";
  content = dictionary of [
    "myPackage" -> "file:/example.com/foo/bar/myPackage"
    "yourLib" -> "http://example.com/foo/lib/youLib?version=3.4"
  ]
}
```

The catalog term is placed in a file — also called `catalog` — that is in the same directory as the source files of the packages that this catalog supports. (I.e., one catalog file supports a whole directory worth of source files.)

Most of the contents of the catalog are optional. The `version` information defines the version that will be appended to any programs that are compiled using this catalog. Similarly, the `baseURI` is used to override the default URI; allowing the compiler to attach special URIs to compiled code — no matter where the source code itself lives.

The `content` field defines a set of mappings from identifiers to URIs. The target URI is denoted as a string that follows the standard RFC2396 syntax for naming URIs.

If a target URI identifies a `version`, then exactly that version will be searched for in the code repository. This is the feature that allows large applications to reference multiple versions of a package.

If a target does not reference a specific version then the repository will be searched for the latest version of the requested package and that version will be used at run-time.

In either case, the compiler will determine the exact version of a package to import — not the run-time.

For most projects you will not need to be very aware of code repositories, catalogs and so on. However, the code product architecture is an important part of **Cafe**'s strategy for helping you building all scales of system.

##Existentialism[existentialism]

If we take a slightly deeper look at the `package` we will see some surprising features; features that are sadly not very common in programming languages.

Lets start with a super simple source `package`:

```
ss is package{
  fun double(X) is X*2
}
```

This package structure is semantically equivalent to:

```
def ss is let{
  fun double(X) is X*2
} in {double=double}
```

Since there is a variable being defined here — the package variable `ss` — we might ask what is its type:

```
ss has type { double has type (integer)=>integer }
def ss is let{
  fun double(X) is X*2
} in {double=double}
```

In effect, a package reduces to a variable declaration whose type is a record type and whose value is an anonymous record of all the functions defined in the package.

What about types though? **Cafe** allows us to export types from packages too:

```
simple is package{
  type foo of t is foo(t) or bar

  fooMe(X) is foo(X)
}
```

Exporting types — and their constructors — is an extremely important part of the package functionality. However, it turns out that we can account for types too in our semantics of packages. Similarly to our previous unfolding, the `simple` package is equivalent to:

```
simple has type {
  foo has kind type of type
  foo has type for all t such that
    (t)<=>foo of t
  bar has type for all t such that
    ()<=>foo of t
  fooMe has type for all t such that (t)=>foo of t
}
def simple is let{
  type foo of t is foo(t) or bar
  fooMe(X) is foo(X)
} in { type foo=foo; foo=foo; bar=bar; fooMe=fooMe}
```

This is clearly quite a bit more complex than our super-simple example; but the basic story is still there: a package consists of a variable definition whose value is an anonymous record. In this case, the anonymous record contains a type, two constructors as well as a regular function.

Having constructors in a type is only a small extension to the conventional notion of a record — while many languages restrict records to containing just data values, most functional programming languages do allow functions in records. A constructor is just a special kind of function.

On the other hand, having a type in a record is quite different.

####Type Variables and their Kind[type-variables-and-their-kind]
We also have a new form of statement about types in this type signature: a `kind` annotation.

In general, types have a _kind_ associated with them; kinds serve the same role for types as types do for expressions and values. In particular, when a type variable is used in a type expression, its kind must be consistent with its use. A significant difference between kind expressions and type expressions is that there are very few different forms of kind:

```
type t has kind type
type u has kind type of type
type v has kind type of (type,type,...,type)
```

The statement about the `foo` type asserts that it is a regular generic type:

```
foo has kind type of type
```

The `kind` annotation is a way of declaring that the `foo` type exists, without being specific about what the type actually is.

###Existential Types[existential-types]
What does it mean to have a type in a record? From a programmer's point of view it is actually quite a natural extension of the concept of a record; there does not seem to be any intrinsic reason why a record shouldn't have types in it. However, the logic of this bears a deeper look.

The type expression in our record type:

```
simple has type {
  foo has kind type of type
  ...
}
```

is itself a kind of macro form. The true meaning of the declaration of the `foo` type involves the use of an _existential_ quantifier:

```
simple has type exists f such that {
  type foo=f
  ...
} where f has kind type of type
```
Notice how the `kind` annotation is still there, it is simply attached to a type variable, specifically an existentially quantified type.

The meaning of an existentially quantified type is complementary to the universally quantified type. An existential type quantification is an assertion that the type exists — with possible constraints as in this case.

####Use and Evidence[use-and-evidence]
To get a better grip on type quantification in general and existentially quantified types in particular it can be helpful to see what the rules and expectations are for a quantified type. Occurrences of a type variable can be classified into two forms — depending on whether the context is one of _using_ a variable or whether one is defining it — or providing _evidence_ for it.

Consider the simple function `d` with its type declaration:

```
d has type for all t such that
  (t)=>t where arithmetic over t
fun d(X) is X+X
```

In the equation that defines `d` we have not explicit referenced `t`; but the rules of type inference lead us to determine that — within the equation — type of `X` is `t`.

If we had forgotten the `arithmetic` constraint:

```
d has type for all t such that (t)=>t 
fun d(X) is X+X
```

the compiler would have complained with a type error. The reason being that, because `t` is universally quantified, we are not permitted to further constrain `t` _within_ the body of the function. Requiring that `t` support addition is such a constraint and we are not permitted to assume it.

On the other hand, when we _call_ `d`, the rules are more generous: calls of the forms `d(2)` and `d(2.4)` are both permitted because we are allowed to _use_ any type — that implements the `arithmetic` contract — when we call `d`.

>The reason we can substitute any type for `t` is a result of the universal quantifier: a `for all` quantifier means we can use any type for `t`.

For existentially quantified types the situation is reversed: within an expression involving an existential type we can use what ever type we want — again, providing that other constraints have been met — but _outside_ the defining expression we can't make any assumptions or additional constraints about the quantified type.

For various reasons, which we will explore further, existentially quantified types are mostly associated with records — like the package record we saw earlier. Let us look again at our simple package type:

```
simple has type exists f such that {
  type foo=f
  ...
  fooMe has type for all t such that (t)=>foo of t
} where f has kind type of type
```

As with universally quantified types, there are two kinds of contexts in which we use existentially quantified type variables: _use_ and _evidence_ contexts. In the former we are using the type and in the latter we are providing evidence that an expression has the right type.

The existential quantifier means is that within an instance of this record we can instantiate `foo` to any type that meets the constraints. The simplest way is to provide a type definition for `foo`:

```
def simple is let{
  type foo of t is foo(t) or bar
  fun fooMe(X) is foo(X)
} in { type foo=foo; foo=foo; bar=bar; fooMe=fooMe}
```

Externally, when we use the record, the opposite happens: we can make use of the existence of `foo` but we cannot further constrain it. We can use `simple`'s `foo` type, as well as the `fooMe` function that relies on it; for example, in:

```
m has type for all t such that 
  (t)=>simple.foo of t
fun m(X) is simple.fooMe(X)
```

There is something a little different going on here: the type of `m` appears to be dependent on a field of the variable `simple`; i.e., the type of `m` apparently depends on the value of `simple`. This is not something that we would normally sanction in a type system because of the potential for disaster.

For example, consider the scenario where '`simple` is not simple'; i.e., suppose that its value were computed; for example suppose that the value of `simple` depended on a conditional computation. In that case the actual type `simple.foo` might also depend on the conditional computation; why does this not cause problems?

Normally such dynamic types do cause substantial problems. The presence of a dynamic type is one of the hallmarks of dynamically typed languages and it makes most type inference impossible at compile time.

However, the type rules for existentially quantified variables are crafted so that _it must not matter_ what the actual type `simple.foo` is. So long as no additional constraints on the `simple.foo` are permitted then the program is provably compile-time type-safe. I.e., uses of an existentially quantified type may not further constrain the type — the exact complement of the situation with universally quantified types.

####Using Existentially Quantified Types[using-existentially-quantified-types]

So, what _are_ the rules for existentially quantified types?

The first is one that we have already been looking at:

>An existential variable may be bound to a type when providing _evidence_ that a value has a certain type, but may not be constrained when _using_ a value with an existential quantifier in its type.

The second is related to this:

>_Each occurrence of an existentially quantified type is potentially different_.

Think about a function with the type:

```
exFn has type for all t such that
  (t)=>exists e such that R of (e,t)
```

For the moment, we don't much care about `R`. Now, consider how we might use `exFn`:

```
def X1 is exFn("alpha")
...
def X2 is exFn("alpha")
```

An important question is "what is the relationship between the type of `X1` and the type of `X2`?". Unfortunately, the fundamental answer is 'we cannot know in general'; which in type terms means effectively there is no relationship: they are different. The reason is that the internal type used within the implementation of `exFn` may result in _different_ instantiations for `e` for each invocation. The result is we cannot assume any link between the types of `X1` and `X2`: they are different.

On the other hand, consider the similar sequence of definitions:

```
def Y1 is exFn("alpha")
...
def Y2 is Y1
```

In this case we do know that the type of `Y1` is identical to the type of `Y2`. This leads us to the third rule:

>Each _use_ of an existential quantification introduces a new type — called a _Skolem type_[^Technically, the type is denoted by a _Skolem Constant_ or a _Skolem Function_.] — that follows the normal inference rules for all types.

I.e., once a type has been introduced as a Skolem type, it behaves just like any regular type and the normal rules of inference apply. This applies equally to the two fragments of code above; but the additional constraint on the immutable values of `Y1` and `Y2` make it easier to propagate type information.

We can see this a little clearly by looking at the effective type annotations of `Y1` and `Y2`:

```
Y1 has type R of (e345,string)
def Y1 is exFn("alpha")
...
Y2 has type R of (e345,string)
def Y2 is Y1
```

where `e345` is the skolemized variant of the existential type `e`.

The effective annotations for `X1` and `X2` will have different skolem constants:

```
X1 has type R of (e235,string)
def X1 is exFn("alpha")
...
X2 has type R of (e678,string)
def X2 is exFn("alpha")
```

>If `Y1` or `Y2` were declared to be re-assignable variables then, once again, we would not be able to connect the types of `Y1` and `Y2` together.

##Abstract Data Types[abstract-data-types]

Abstract data types can be viewed as the mathematics behind object oriented programming.[^Not to be confused with _Algebraic Data Types_ — which represent the mathematical foundation for enumerations and other non-object values.]

**Abstract Data Type**
: An _abstract data type_ is a mathematical model of a set of related values that share a common set of semantics.

In programming, it is the _common_ semantics that defines the structure; but, of course, programming languages are not able to capture the full semantics of a program or type and hence the stand-in for this is usually a type specification.

Perhaps an example is overdue. In our chapter on [Collections][collections] we looked at many operators over collections and not a few example collection types. Although programs using the `sequence` contract are fairly abstract, the type of the collection itself is still visible. Suppose we wanted to build a set structure where only the fact that there is a set, and the set-like operators over the set were visible. The representation type for the set should otherwise be completely opaque.

One might start with a type definition:

```
type genSet is genSet{
  coll has kind type of type
  z has type for all t such that coll of t
  addElement has type for all t such that
    (t,coll of t)=>coll of t
  present has type for all t such that
    (t,coll of t)=>boolean
}
```

The essence of this type declaration is a collection of operators that define set-style operators. By protecting the `coll` type with an existential quantifier, we ensure that the representation of `genSet` values is not accessible externally; whilst at the same time we do allow other programs to _use_ the set operators.

One example implementation of `genSet` might use `list`s to represent the set structure itself:

```
def LS is genSet{
  type list counts as coll
  def z is list of []
  fun addElement(X,L) where X in L is L
   |  addElement(X,L) is list of [X,..L]
  fun present(X,L) is X in L
}
```

The statement:

```
type list counts as coll
```

is one of the ways that we can give evidence for the existence (sic) of the `coll` type. Type inference — of the evidence gathering kind — will link `coll` internally to `list`. The `counts as` statement can be viewed as a more fluent way of writing:

```
type coll = list
```

Given `LS`, we can use it like a set generator — `LS` provides a set of operators over sets:

```
def Z is LS.z
def One is LS.addElement(1,Z)
def Two is LS,addElement(2,One)
```

The type of `LS` gives no clue as to the internal type used to represent sets generated by it:

```
LS has type genSet
```

But `Z`, `One` and `Two` have more interesting types:

```
Z has type collK341 of integer
```

where `collK341` is a Skolem type — a unique type generated when we assign a type to `LS`. In effect, `LS` is a module that exports the set type and associated operators; this module is referenced by name and is used to construct particular sets.

A reasonable question here is 'where is the Abstract Data Type?'. What we have is a record with a type and some functions in it. Recall that an ADT is a 'model of a set of related values that share a common set of semantics'. The semantics in common are the functions in the record; and the type itself is the existentially quantified type in that record — `coll`.

Notice how we index off the `LS` variable to access the operators for this set; even while passing into it instances of sets created and modified by `LS`. This is one of the hallmarks of a module system.

###Opening Up[opening-up]

One of the reasons that we are so interested in establishing a 'normal' semantics for modules and ADTs is that we can develop systems where the contents of a module depends on some additional computation; i.e., we can use _functions over modules_. For example, we can show that _aspect oriented programming_ and _dependency injection_ can be realized just using normal code structuring with functions and `let` environments.

Techniques like dependency injection are typically applied to large programs; unfortunately that makes constructing small examples a little forced. So, we'll use a crow-bar to open a soda bottle. Imagine, if you will, that we needed to define a new arithmetic type that supported arbitrary fractions.

>Floating point numbers are fractions. But they do not permit the representation of all fractions — e.g., it is not possible to represent 1/3 exactly in IEEE 754.

However, while we want to expose the type, and a set of operator functions, we definitely do not want to expose anything about the implementation of fractional numbers: as far as users are to be concerned, the type `fraction` is to be completely opaque and might be implemented in any way.

Let us start with an interface; which in this case will take the form of a record type:

```
type fractionals is fracts{
  fraction has kind type
  frPlus has type (fraction,fraction)=>fraction
  frToString has type (fraction)=>string
  frParse has type (string)=>fraction
  fraction has type (integer,integer)=>fraction
}
```

One of the first things to note here is that `fraction` is existentially quantified; secondly we need to ensure that the set of operators we expose is complete. Our interface is not really complete, but includes two critical operators: a means of constructing `fraction`s and a means of escaping from the world of `fraction`s to other types (in this case `string`).

Here we are mostly concerned with _using_ `fraction`s, so we will assume that we have at least one implementation — courtesy of the `FR` variable:

```
FR has type fractionals
```

One way to use our implementation of fractions would be to reference the needed operators via the `FR` variable:

```
def F0 is FR.frParse("3/4")
def F1 is FR.fraction(1,2)
def F2 is FR.frPlus(F0,F1)
show FR.frToString(F2)   -- results in 5/4
```

However, we can do rather better than this in **Cafe**. We have already encountered the `import` statement; there is an analogous statement that allows us to unwrap a record like `FR` in a binding environment — such as a `let`:

```
let{
  open FR
  def F0 is frParse("3/4")
  def F1 is fraction(1,2)
  def F2 is frPlus(F0,F1)
} do 
  show frToString(F2)   -- results in 5/4
```

The `open` statement has a similar effect to the package import: it enables the functions, types and other elements that are embedded in a record to be made available as normal identifiers within the normal scope of the `let` action (or expression).

Of course, this code is still fairly clumsy; since we would like to use normal arithmetic notation over `fraction`s; which we can do by implementing the `arithmetic` contract:

```
let{
  open FR
  implementation arithmetic over fraction is {
    X+Y is frPlus(X,Y)
   ... -- more operators needed
  }
} do {
   def F0 is frParse("3/4")
   def F1 is fraction(1,2)
   def F2 is F0+F1
   show frString(F2)
}
```

We can improve this further by also implementing the `coercion` contract between `string`s and `fraction`s:

```
let{
  open FR
  implementation arithmetic over fraction is {
    X+Y is frPlus(X,Y)
   ... -- more operators needed
  }
  implementation coercion over (string,fraction) is {
    coerce(S) is frParse(S)
  }
  implementation coercion over (fraction,string) is {
    coerce(F) is frToString(F)
  }
}
```

This allows us to use a more natural notation for expressions involving our `fraction`s:

```
let{
  open FR
  ...
} do {
   def F0 is "3/4" as fraction
   def F1 is fraction(1,2)
   show F0+F1
}
```

While much better than our original, we still have too much code to write to use the `fraction` type: we have to get the type and then demonstrate the appropriate implementations. We want to be able to combine everything that is important about `fraction`s into a single structure.

There is a straightforward way we can do this. Our original signature for `fractionals` simply required the presence of the `fraction` type. What we can do is further require that the `arithmetic` and appropriate `coercion` contracts are also implemented; we do this by constraining the type definition for `fractionals`:

```
 type fractionals is fracts{
  fraction has kind type where arithmetic over fraction
       and coercion over (string,fraction)
       and coercion over (fraction,string)
  frPlus has type (fraction,fraction)=>fraction
  fraction has type (integer,integer)=>fraction
}
```

>Since we are using contracts we do not need the explicit `frParse` and `frToString` functions in the signature any more.

When we instantiate a `fracts` record we must provide — within the record itself — appropriate implementations of `arithmetic` and `coercion`:

```
def FX is fracts{
  type myFraction counts as fraction
  implementation arithmetic over myFraction is {
    fun X+Y is frPlus(X,Y)
   ... — more operators needed
  }
  implementation coercion over (string,myFraction) is {
    fun coerce(S) is frParse(S);
  }
  implementation coercion over (myFraction,string) is {
    fun coerce(F) is frToString(F)
  }
  ...
}
```

Notice that we implemented `arithmetic` for the internal `myFraction` type. We could have equally implemented the contract for `fraction` type too; the key requirement is to provide evidence that `arithmetic` is implemented for the type.

The `FX` record now has everything we want to expose about `fraction`al numbers.[^Assuming that we added the missing operators that we would actually need.} If we `open] the structure then indeed we can write programs like:

```
let{
  open FX
} do {
   def F0 is "3/4" as fraction
   def F1 is fraction(1,2)
   show F0+F1
}
```

This is virtually equivalent to the code we might have written if we were importing a package with the definition of the `fraction` type in it. The difference is that we have access to the full expressive power of the language in computing `FX`.

###Injection[injection]

Injection is a technique where we specialize a program with additional information; especially where that additional information is not part of the normal argument flow. Of course, it can be hard to be crisp about 'not part of the normal argument flow'; but injection is an architectural technique to apply if and when it makes a difference in the readability of your code.

Injection is often used to manage _configuration_ of code: the configuration is injected into the main program; for example, we might configure an application server with the path name of a particular application, or with the port on which the app server should be listening. Neither of these would normally be considered part of the normal information flow in an application server.

There is a standard functional programming style that can be used to represent injection — namely functions that return functions. To take an extremely simple example, suppose that we wanted to have a function that counted the percentage of a class that passes an exam. The function itself is pretty simple:

```
fun passes(L) is fraction(
   size(list of { all X where X in L and X.score>Pass}),
   size(L))
```

The configuration parameter here is obviously the `Pass` value; this is an important parameter to the function but is not part of the normal argument flow (think about computing the pass count for an entire school).

We can use the function returning approach to inject an appropriate value of `Pass`:

```
fun passes(Pass) is 
  (L)=>fraction(
     size(list of { all X where X in L and X.score>Pass}),
     size(L))
```

Using this `passes` is a two-step process; we first use a specific passing grade to construct the test function and then use this to measure performance on groups of students:

```
def HS is passes(60)

def allPass is list of { all C where C in Courses and HS(C)>0.80 }
```

>The two-step process is a key part of the injection technique.

###Extensible Types[extensible-types]

Sometimes, rather than configuring a program with a numeric value (or any other value for that matter), we need to configure it with a _type_. This does not happen that often, and **Cafe**'s type constraints can eliminate many cases where it might be needed; but the requirement still shows up occasionally. Where it can show up is in situations where you need to develop customizable applications — applications that can be extended further by your customers without you having to change a line of your own code.

For example, you might need to build a system that attempts to predict the behavior of equipment based on historical performance and current demand. This kind of software could be very useful in determining a proper maintenance schedule. Suppose that you determine that what is important in predicting potential breakdowns is the number of units processed and the number of days since the last scheduled maintenance. You might keep track of this in a record:

```
type maint is maint{
  date has type date
  units has type integer
}
```

And you will also probably have a description of each piece of equipment:

```
type equip is equip{
  id has type string
  eqpType has type string
  nextMaint has type date
}
```

Using this, and similar records, together with some clever algorithms, you design a function that determines the next most likely piece of equipment to fail — perhaps together with an expected failure date:

```
nextToFail has type
  (list of maint,list of equip)=>(equip,date)
...
```

>The details of this algorithm, while critical to an actual application, are of no concern to us here.

Now, you deliver your software to your customer and the first thing that they ask for is an ability to tweak it. You see, you designed it for generic pieces of equipment and they have particular pieces of equipment, with particular foibles affecting the computations needed to determine when equipment needs maintenance. And they need to keep some information in the description of equipment and maybe also in the maintenance records that is not in your types.

>Your challenge is to permit this kind of extension without requiring your code to be modified or even recompiled for each customer.

The standard OO approach to addressing would be to permit the customer to _sub-class_ some of the critical types (such as `maint` and `equip`). However, there are problems with using sub-types: in particular, if your algorithm requires computing _new_ instances of data structures then sub-classing cannot work: when your algorithm creates a new `equip` record, it will not know how to create a customer variant of that record:

```
fun updateEquip(E,W) is equip{
  id = E.id
  eqpType = E.eqpType
  nextMaint = W
}
```

with the result that the customer data is lost. An alternative approach is to allow some extensibility in the record by having a special `extra` field:

```
type equip of t is equip{
  id has type string
  eqpType has type string
  nextMaint has type date
  extra has type t
}
```

Since we do not want to constrain the kind of information our customizer can store we make its type quantified. The `extra` field is there to support extensions; and, because we know about its existence, we can carry the data with us:

```
fun updateEquip(E,W) is equip{
  id = E.id
  eqpType = E.eqpType
  nextMaint = W
  extra = E.extra
}
```

The problem with adding such an `extra` field is its type: this version changes the unquantified `equip` type into a quantified one. This will have potentially devastating impact on your code — especially if you want to allow multiple extensions for multiple data structures. The reason is that potentially a large number of functions will be required to carry the type parameters in their type signatures. This is doubly galling as these extra type parameters do not have any significance in the main code: they are there only to support potential customizations.

Instead of universal quantification, we can use an existential type for the `extra` field:

```
type equip is equip{
  id has type string
  eqpType has type string
  nextMaint has type date
  t has kind type
  extra has type t
}
```

This has the effect of permitting a local extension to a record type while also effectively hiding the type from the main code.

Of course, in order for `extra` to have any effect on our code, we have to be able to make use of it within our algorithm. This is another customization point in the code: not only do we need to allow additional data but we need to be able to reference it appropriately. For example, we might decide that the `extra` field should have a say in determining the next maintenance date; so our `updateEquip` function should take it into account — but how?

A simple way is to add to the `equip` record a set of extensibility functions that the customer must supply, in addition to the data itself:

```
type equip is equip{
  id has type string
  eqpType has type string
  nextMaint has type date
  t has kind type
  extra has type t
  extraDate has type (t,date)=>date
}
```

Then, our `updateEquip` function calls this `extraDate` function when computing the new maintenance schedule:

```
fun updateEquip(E,W) is equip{
  id = E.id
  eqpType = E.eqpType
  nextMaint = E.extraDate(E.extra,W)
  type t = E.t     — note evidence for type
  extra = E.extra
  extraDate = E.extraDate
}
```

A more succinct way of expressing this would be to use **Cafe**'s `substitute` operator:

```
fun updateEquip(E,W) is E substitute {
  nextMaint = E.extraDate(E.extra,W)
}
```

Of course, the customer has to provide functions that create the initial data structures, and the initial values of `extra` and the updating function `extraDate`. You, as the provider of the software, will offer a default implementation:

```
fun equip(Id,Tp,Maint) is equip{
  id = Id
  eqpType = Tp
  nextMaint = Maint
  type t = ()  — () is Cafe's void type
  extra = ()
  extraDate = (_,W) => W
}
```

This approach meets our goals: we can allow customers of our software access to key data structures in a safe way that does not require use to modify our code for each customer or even to recompile it.

##Phew

This Chapter covers some difficult material! We start with a requirement to scale — to be able to scale code from a single module through to applications built by assembling libraries. Along the way we take in existential quantification and abstract data types.

What we have not yet addressed are the needs of distributed applications. Managing distributed applications is one of the most tedious and difficult challenges of modern software development. However, before we can demonstrate **Cafe**'s approach to this, we must look at _agent oriented systems_ and **actors** — the subject of [Chattering Agents][chattering-agents].
#Concurrency[concurrency]

Concurrency is about doing more than one thing at once. Concurrency in programming languages is important for two basic reasons: modern processors are easily capable of executing many tasks in parallel (and by implication, if your program does not then you may not be making good use of the machine) and many applications have to be able to respond to events without having to process everything in a fixed order.

Building concurrent applications is hard for programmers primarily because many of the assumptions built into modern programming languages are not valid when performing activities concurrently. The most important of these assumptions relates to state — for example, the values of mutable variables.

In particular, the most basic assumption

>the value of a variable stays the same until it is modified

is _not_ valid when the variable in question is shared by multiple concurrent activities. It is difficult to overstate the importance of this.

>An interesting parallel shows up in dealing with weightlessness in space: astronauts regularly complain of items drifting about in the space station — 'nothing stays where you left it'. As a result, astronauts must get used to taping everything down while they are working.

>Programmers building concurrent systems have a similar experience: they are constantly having to 'tape down' pieces of state to make sure that they 'stay put'.

**Cafe** enables programmers to write concurrent and parallel programs with _tasks_ and _rendezvous_. Tasks allow us to manage when activities are performed; rendezvous are used to coordinate tasks when they must interact with each other.

##Tasks[tasks]

A task expression denotes a _suspended computation_ — which itself is a sequence of actions — and which may have a value when the computation is performed. In simple terms, tasks provide an extra level of indirection between expressions and values.

>Like tasks, functions can also be viewed as representing suspended computations. However, unlike functions, a task expression represents a single suspended computation. Furthermore, functions also incorporate the concept of abstraction. Essentially, it does not make sense to apply a `task` to arguments.

A `task` expression is written as a sequence of actions, enclosed in a `task` block; for example, in:

```
def T is task{
  def X is 1
  def Y is 2
  valis X+Y
}
```

the variable `T` is bound to a `task` expression whose returned value will be `3` — the value returned by the action:

```
valis X+Y
```

Task expressions have a type: `task of t` where `t` is the type of the value returned by the `task`. So, for example, the type of `T` above is given by

```
T has type task of integer
```

as it produces an `integer` value.

###The Value of a Task[the-value-of-a-task]

As we saw above, a task computation produces a value by performing the `valis` action:

```
valis X+Y
```

The _value_ of a `task` is obtained using the `valof` operator:

```
valof T
```

denotes the value returned by the task computation `T`.

It is important to remember, in thinking about tasks, that there is the extra level of indirection. The type of `T` is `task of integer`; the type of `T`'s value is `integer`.

Note that the `valof` operator also has the effect of forcing the completion of the task `T`; in particular, the `valof` operator _is blocked_ until the task has completed and its value can be returned.

>Although the `valof` operator denotes the end of the computation, it does not denote the start of the computation.

If a `task` does not return a value then its type will be

```
task of ()
```

Obviously a `task` that does not have a value does not require it's body to contain a `valis` action; however, if it does, then that action should be:

```
valis ()
```

In effect, tasks that do not purport to return values _do_ return a value — the void tuple value. To avoid the clumsiness of handling empty tuples, especially in the context of actions, it is better to `perform` such a task instead. Like the `valof` expression form, the `perform` action forces completion of its task argument — thus achieving some synchronization with the performed task.

For example, the action:

```
perform task {
  nothing
}
```

performs the `task` (which does `nothing`) and waits for nothing to finish (sic).

Void tasks are occasionally useful — for example, multi-tasked applications often contain a _server_ component: a component that responds to requests made by other tasks. Such a component typically is not expected to return and so does not have a useful return value.

####Getting the value more than once[getting-the-value-more-than-once]

Note that it is possible to ask a `task` for its value multiple times. However, the actions in the task body will only ever be performed once: subsequent attempts to get the value of the task simply return the value previously computed.

###Background Tasks[background-tasks]

A `task` can be started in the background — i.e., concurrently with other `task`s — by using the `background` operator, which is a function from `task`s to `task`s. It's type is given by

```
background has type for all t such that
  (task of t)=>task of t
```

Applying `background` to a task not only yields a task of the same type, but also one yielding the same value.

The `background` operator has a simple effect — it starts its argument `task` so that it will operate in parallel with other computations.

>If it happened to be the case that a backgrounded task already completed, then the `background` operator has no effect.

###Computing with tasks[computing-with-tasks]

Like other values, `task`s are first class: they can be assigned to variables, kept in data structures, passed to and from functions and so forth. This flexibility leads to great expressive power — many patterns of computations can be readily encoded as `task`-valued
functions.

For example, consider the `mp` function — which is a facsimile of the standard `map` function specialized for `list`
values:

```
fun mp(F,list of []) is list of []
 |  mp(F,list of [X,..Y]) is list of [F(X),..mp(Y)]
```

In this `mp` function, the functional variable `F` denotes the computation to be applied to each element of the input list. Suppose that each computation of `F` were non-trivial, and we wanted to spread the load across multiple cores — i.e., to perform the map operations in parallel. The `task` notation, in particular `background` tasks will help us to achieve that.

>Recall that `map` is a standard **Cafe** function that is defined in the `mapping` contract and is implemented for many different collection types.

We will first of all transform `mp` to use tasks rather than simply calling the function:

```
fun taskmap(F,list of []) is list of []
 |  taskmap(F,list of [X,..Y]) is list of
      [valof task{ valis F(X) } ,.. taskmap(F,Y)]
```

As it stands, this function has very similar performance characteristics to `mp`; except that we are using the `task` expression notation. In order to run the different elements in parallel we need to also use the `background` operator:

```
fun parmap1(F,list of []) is list of []
 |  parmap1(F,list of [X,..Y]) is
      list of [valof background task{valis F(X)} ,.. parmap1(F,Y)]
```

This program computes each element of the result in a separate background task. However, it is not a true parallel map because we wait for each element before continuing to the next element.

A better approach is to first of all construct a list of `task`s and then to separately collect their values in a second phase:

```
fun parmap2(F,L) is let{
  fun spread(list of []) is list of []
   |  spread(list of [X,..Y]) is list of [
        background task{ valis F(X)} ,.. spread(Y)]
            
  fun collect(list of []) is list of []
   |  collect(list of [T,..Ts]) is list of [valof T,..collect(Ts)]
} in collect(spread(L))
```

This function does not wait for any task to complete until they have all been spun out into background activities. This gives the maximum opportunity for the independent tasks to complete before we actually need their values.

A good rule of thumb is:

>when you are programming with tasks, everything that is not a completely task-less subcomputation should be enclosed in `task` brackets.

So, a more idiomatic way of writing the `parmap` function is to make it a `task` valued function; we also generalize away from the specific `list` collection type and use the standard `map` function to allow for any collection type:

```
fun parmap(L,T) is let{
  fun spread() is map((X)=>background task{valis F(X)},L)
  fun collect(Lt) is map((T)=>valof T,Lt)
} in task{
  valis collect(spread())
}
```

This `parmap` function is a `task`-valued function; its type is given by:

```
parmap has type for all t,e,f such that
  ((e)=>f,t of e)=>task of t of f where mappable over t 
```

The type of `parmap`, in particular the role of the type variables `t`, bears further explanation.

####Variable Type Constructor[variable-type-constructor]

So far, we have seen type variables that represent a type, normally a universally quantified type. In the type for `parmap` we see a different kind of type variable: here `t` denotes a variable type constructor, as in `t of e` and `t of f`. This represents a generalization of type variables; similar to that where a function is denoted by a variable.

###Computation Expressions and the **_M_** word [computation-expression]

Task expressions are instances of a more general **Cafe** feature: the _computation expression_. Task expressions are oriented towards the concurrent execution of computations; other forms of computation expression have different purposes. In particular, the simple `valof` expression:

```
valof{
  def x is 2
  def y is f(x)
  valis x+y
}
```

is actually a degenerate case of an `action` computation expression:

```
valof action computation {
  def x is 2
  def y is valof f(x)
  valis x+y
}
```

There are several forms of computation expression in the **Cafe** library; they are used for expressing tests for example. In fact, computation expressions are _syntactic sugar_ for _monadic_ expressions — and play the same role in **Cafe** as the `do` notation plays in Haskell.

Monads represent a generalization of the kind of deferred computation we see in `task` expressions. In effect, monads represent a standard way of _composing computations_.

The computation expression is syntactic support for using the monad contract — actually called `computation`. We can see the extent of this support by looking at the 'raw' version of the `action` function:

```
fun AA(f) is action computation {
  def x is 2
  def y is valof f(x)
  valis x+y
}
```

The `AA` function becomes the somewhat more complex form:[^This is actually a somewhat sanitized version of the raw code. This is because we have hidden a required transformation from action sequences to functional expressions that encode the computation's ordering as function calls.]

```
fun AA(f) is _delay(() => valof{
  def x is 2
  def y is _perform(f(x), raiser_fun)
  valis _encapsulate(x+y)
})
```

Note how the inner `valof` expression is transformed to a call to `_perform`; whereas the result of the `action computation` is encapsulated in a call to the function `_delay`. These are all functions in the standard `computation` contract; and so will actually be further specialized to be specific for the `action` expression.

It is beyond the scope of this book to explore more deeply the handling of monadic computations. However, if you wish to explore further, a more complete description of computation expressions is given in the **Cafe** language definition.

##Rendezvous[rendezvous]

Tasks support a very simple communication pattern between parts of a computation: a task computes for a while, and then (if appropriate) returns a result to anyone asking for its value — typically a parent task. While a task can spawn sub-tasks, there is no immediate way for tasks to communicate with each other _while_ they are running. Many applications, however, require more fine-grained coordination between tasks.

To permit this better coordination, and to permit data to flow between tasks, we have the _rendezvous_. A rendezvous is a meeting between two or more tasks or other computations. Data can flow between tasks at a rendezvous; with the guarantee that the data is consistent for the parties at the rendezvous.

The rendezvous mechanism is inspired by Reppy's Concurrent ML _events_ [][#Reppy1999], and by Hoare's Communicating Sequential Processes[][#hoare78] [][#hoare85] (which also involves the rendezvous concept). As we shall see, the rendezvous is a simple mechanism that allows implementing almost arbitrarily complex choreographies between concurrent computations. However, we shall also see that the rendezvous mechanism is relatively low-level and there are good higher-level abstractions that make writing concurrent applications significantly easier.

###A `rendezvous` is a Future Event[the-rendezvous-type]

A rendezvous is an object describing an event that may or may not happen in the future. The basic operation for a rendezvous is _waiting for_ the event, and when the event happens, it may yield or consume a value.

This is reflected in the `rendezvous` type itself:

```
for all t such that rendezvous of t
```

One of the simplest forms of rendezvous denotes a _timeout_:

```
timeoutRv has type (integer) => rendezvous of ()
```

The rendezvous returned by `timeoutRv(10)` describes an ''alarm clock'' event that happens 10 milliseconds in the future. Note that the type of the rendezvous returned specifies `()` as the type of the value yielded by the event - the timeout event provides no information other than that it happened.

It may seem a little odd that a rendezvous — which is after all a meeting — is denoted by an actual value. The primary reason for this is that is allows specific synchronizations between tasks to be _computed_, not just _programmed_. This is a simple but major departure from the approach taken to concurrency by languages that support locking features; but is actually highly reminiscent of Unix's `select` function.

Timeouts can be useful in their own right, but they are commonly used in conjunction with other rendezvous. The pattern being that if one of the other 'things going on' does _not_ happen, then the timeout will tell us that. However, _recovering_ from a timeout can be very problematic — since you now have to decide what to do about the other activities that were pending.

###Waiting for Events[waiting-for-events]

The `wait for` operator takes a rendezvous and yields a `task` that waits for the event described by the rendezvous to happen. So, for example:

```
wait for timeoutRv(2000)
```

is a task that, when `perform`ed, does nothing for 2 seconds, and yields the `()` value.

Note that `wait for` returns a task - it does not execute it. To actually wait for an event, the task must be performed. Typically, within a task block, it is used like so:

```
task {
  ...
  perform wait for timeoutRv(2000)
  ...
}
```

There are quite a few pieces here: the `timeoutRv` event itself, the `wait for` task function and the `perform`ance of the task itself.

This separation into distinct phases helps in flexibility of the concurrency features but, of course, can make straightforward scenarios complex to construct. We shall see below that there is a library of concurrency features that are quite high-level and may be more appropriate than 'rolling your own' setup.

###Channels of Communication[channels-of-communication]

In many cases the requirement for synchronization between tasks is based on needing reliable communication between them. Instead of building communication on top of a model of 'shared resources', we use direct 'message passing': one task sends a message to another. The mechanism we use is called the `channel`.

**Channel**
: A `channel` is a means by which two (or more) tasks can pass data in a type-safe way between themselves.

Channels are _synchronous_: in order for data to be passed between tasks they must both be paused. There are rendezvous associated with receiving messages from a channel and for placing messages on a channel — actual communication only occurs when both the sender and the receiver have waited for the event to occur.

>One might ask 'why is communication synchronous?' Why not asynchronous? The basic answer is that synchronous communication seems to be more basic than asynchronous communication.

It is straightforward to implement a more asynchronous communication pattern based on synchronous primitives. It is harder to do the converse: to implement synchronous communication using asynchronous primitives.

However, it is also true that synchronous communication is essentially impossible when that communication involves multiple computers. The concurrency features in **Cafe** support multiple threads of activity within a single application but do not directly support networked applications. We believe that, for other reasons, such networked applications are better served with different features — features that directly address issues involving agenthood. We shall see some of these in our treatment of [Concurrent Actors][concurrent-actors].

The `channel` function creates such a synchronous channel:

```
channel has type for all t such that () => channel of t
```

There are two rendezvous operators for sending and for receiving data on a channel:

```
sendRv has type for all t such that
  (channel of t, t) => rendezvous of ()
recvRv has type for all t such that
  (channel of t) => rendezvous of t
```

Notice that the `sendRV` operator returns a void-valued rendezvous; whereas the `recvRv` operator returns a rendezvous with a value.

_Type safety_ is ensured by the type carried by the `channel` value: the sender and receiver of a channel must also agree on the type of the data being communicated. Channels have no fixed concept of direction. A task can send a message over a channel in one step, and receive a message over the same channel in the next.

Here is a simple scenario involving a transmission over a channel:

```
def ch is channel()
ignore background task {
  def msg is valof (wait for recRv(ch))
}
ignore background task {
  perform wait for sendRv(ch,15)
}
```

The communication occurs when both `background`ed tasks reach their appropriate rendezvous: one is waiting for the message to arrive and the other waiting to be able to send it.

In our message passing sequence, we actually do _not_ want the task arguments to complete in a sequential order — so `ignore`ing (sic) the task is the appropriate command.

###Computing Primes With Tasks[computing-primes-with-tasks]

Before, we looked at implementing the sieve of Eratosthenes using operations over collections. The Sieve, as it is affectionately known, is also a great demonstrator of concurrent programming.

Recall that The Sieve has two parts: a collection of activities that filter a sequence of numbers looking for multiples of previously found primes and a master that grows the set of filters as new primes are found.

####Filtering for Primes[filtering-for-primes]

Let us start with modeling each filter as a concurrent task; which can be implemented using:

```
fun filter(P,inChannel) is let{
  def outChannel is channel()
  fun loop() is task{
    while true do {
      def I is valof (wait for recvRv(inChannel))
      if I%P!=0 then -- not a multiple, pass it on
        perform wait for sendRv(outChannel,I)
    }
  }

  { ignore background loop() }
} in outChannel
```

The heart of the `filter` function is a concurrently executing task that listens for input, checks to see if the input number is a multiple of 'its' prime, and if it is not passes it on.

Notice that the `filter` function is written in such a way that it does not directly know what the recipient of the output messages will be. This is quite common in programs of this type and is the concurrent analog of returning a value from a function. In this case the returned 'value' is presented as a sequence of messages on a channel.

The sharp-eyed reader will spot something else that is novel here: the `let` environment has a local action as well as two definitions. The form:

```
{ ignore Action }
```

within a `let` definition environment denotes an action that is performed as part of 'constructing' the `let` environment.

One might ask, why do we `ignore` the background task and not `perform` it? Well, the simple answer is that performing a task implies waiting for the task to finish. In fact, `perform`ing a task is equivalent to ignoring the result of taking its value:

```
ignore valof background loop()
```

will clearly not finish until the loop finishes; which it will not.

Our `filter` task will never terminate, indeed the entire Sieve consists of a network of tasks that do not terminate of their own accord.

####A Stream of Naturals[a-stream-of-naturals]

Before we look at the main sieve, let us see how we construct a generator stream. In the case of the sieve, we need a stream of positive integers (naturals) that will act as the source of potential prime numbers (to be filtered of course).

The `naturals` function is set up to return a `channel` on which will be placed the odd integers greater than `2`:

```
def naturals is let {
  def natChannel is channel()
  { ignore background task {
      var counter := 3
      while true do{
        perform wait for sendRv(natChannel,counter)
        counter := counter+2
      }
    }
  }
} in natChannel
```

This has a similar structure to the `filter` function except that the loop does not wait for any input — it keeps on generating odd numbers. This is where the synchronous nature of communication is important: the action:

```
perform wait for sendRv(natChannel,counter)
```

will only complete if there is a task that is waiting for the message. So this streamer will _not_ gush out numbers unless there is someone listening.

####Sieving for Primes[sieving-for-primes]

The final piece of The Sieve is the `sieve` function which constructs a network of filters to sieve out the primes.

At any one time there is an existing network of filters removing multiples of prime numbers found so far. For example, after processing `13`, the next number to look at will be 15 and the network of filters will look like:

![The Sieve of Eratosthenes at 13][eras13]

[eras13]:images/eras13.png width=350px

The `3` filter removes `15` because 15 is a multiple of 3. However, the following number — 17 — survives all the filters and the sieve responds by spinning off a new filter for `17` at the end of the chain:

![The Sieve of Eratosthenes at 17][eras17]

[eras17]:images/eras17.png width=350px

I.e., The Sieve also receives input from a source, but its response is to spin off a new filter. We can program this using the `sieve` function:

```
fun sieve0(inChannel) is valof{
  def nxPrime is valof (wait for recvRv(inChannel))
  valis sieve0(filter(nxPrime,inChannel))
}
```

The crux of the `sieve` function is that the same channel that it is listening to for the next prime number is then passed to the newly created `filter` task and the result of _that_ filter will be the input for the next cycle of the `sieve`.

This variant of The Sieve does not report the primes it finds; in fact, it will not terminate until the stack overflows. A more appropriate variant will find a list of primes up until some number. We first of all engineer another task that listens to a channel for a sequence of terms and collects the result into a list:

```
fun collectTerms(ch,Count) is background task{
  var data := list of []
  for cx in range(0,Count,1) do {
    def nxt is valof ( wait for recvRv(ch) )
    data := list of [data..,nxt]
  }
  valis data
}
```

Given this function, we augment our sieve with the additional channel for sending the results to:

```
fun sieve(inChannel,results) is valof{
  def nxPrime is valof (wait for recvRv(inChannel))
  perform wait for sendRv(results,nxPrime)
  valis sieve(filter(nxPrime,inChannel),results)
}
```

The complete system can be put together into a `primes` function that uses multiple tasks to sieve for primes:

```
fun primes(Max) is let{
  def resltCh is channel()
  { ignore background task { valis sieve(naturals, resltCh)}}
} in valof collectTerms(resltCh,Max)
```

###Philosophers Have to Eat
Too[philosophers-have-to-eat-too]

The analog of the 'hello world' example in concurrent programming is probably the in-famous dining philosophers problem. Made famous in this form by C.A.R. Hoare (also of QuickSort fame), the set up for this puzzle is quite straightforward: there are four philosophers sitting at a table wanting to eat. The catch being that there are only four forks on the table and a philosopher needs to use two forks to eat something from the table.

![Four Dining Philosophers][phil]

[phil]:images/phil.png width=350px

Of course, it is only a little eccentric that the philosophers eat with two forks rather than a knife and fork!

Being philosophers, they don't eat constantly: they spend some time talking too; which means that it is possible for them to _share_ the forks. Each philosopher has access to the two forks nearest him; each fork is shared by two philosophers; but a given fork can not be in use by more than one philosopher simultaneously.

This problem is one of _resource contention_ and the underlying issue is to avoid the different kinds of contention issues — such as _deadlock_ (where everyone is stuck and no progress can be made), _livelock_ (where no effective progress is made) and _starvation_ where one or more of the philosophers receives an unfair allocation of the forks — preventing others from using the forks.

There are two well-known solutions to the dining philosophers problem; one is due to Hoare and the other is due to Mani Chandy. We will show how to realize Hoare's solution as it introduces some additional features of our concurrency platform.

In essence, the Hoare solution depends on a combination of _semaphores_ to manage the status of the various forks and an additional _arbitrator_ to ensure deadlock-free execution.

####Using Channels and Rendezvous to Implement
Semaphores[using-channels-and-rendezvous-to-implement-semaphores]

A semaphore is a general structure for implementing mutual exclusive access to some resource.

**Semaphore**
: A **semaphore** is a variable, with associated **grab** and **release** operations, that can be used to
manage access to a shared resource by multiple concurrent activities.

It is traditional in designing semaphores to allow an arbitrary — but controlled — number of clients access to the resource. A non-recursive _lock_ is equivalent to a semaphore with a count of 1.

Given a semaphore, it is used in a sequence that looks like:

```
S.grab()
... -- access the shared resource
S.release()
```

The idea is, of course, that if the resource is not currently available then activities that attempt to **grab** the semaphore will be blocked. When the resource becomes available (by someone else performing the semaphore's **release** action) then the **grab** may be re-attempted.

Semaphores are useful, but they are prone to mis-programming. Use of a semaphore relies on the programmer ensuring that the **release** is invoked after a **grab** and, furthermore, any connection between the semaphore and the governed resource is purely implicit. Both of these are common sources of errors in concurrent programs.

To build a semaphore we have to arrange for a unique resource[^Not to be confused with the resource that is governed by the semaphore.] together with coordinated access to it; preferably without exposing the resource itself (to prevent it from being _contaminated_ by external code).

One way of implementing such a unique resource is to use an internal background task and to map calls to `grab` and `release` to communication to the task. This internal task only needs a simple structure: it is simply listening for messages either to `grab` or to `release`. The crux is that, at any one time, either a grab or a release message might arrive and we cannot control this ahead of time. In effect, we have to be able to listen to more than one potential message, and the choice rendezvous allows us to do this.

The main loop of the semaphore's internal task is modeled using a mutual recursion between the `grabR` function, `releaseR` function and the `semLoop` functions. The forms of `grabR` and `releaseR` are actually very similar:

```
fun grabR(X) is
  wrapRv(recvRv(grabCh), (_) => semLoop(X-1)
fun releaseR(X) is 
  wrapRv(recvRv(releaseCh), (_) => semLoop(X+1))
```

The `wrapRv` operator is a function that takes a rendezvous and a function as argument; it returns a new 'wrapped' rendezvous as its result. If the argument rendezvous fires then the result of that rendezvous is passed to the function argument — which in turn becomes the value returned by the `wrapRv` rendezvous.

In the case of `grabR`, the effect is that a message on the `grabCh` channel is passed to the lambda; which ignores the actual message but recursively invokes the semaphore loop function — with an decremented 'resource counter'. The `releaseR` function is similar except that `semLoop` will be invoked with an incremented counter and that it is listening to the `releaseCh` channel rather than the `grabR` channel.

The `wrapRv` function is a useful way to 'do something' during a rendezvous. It is also the best way of converting a rendezvous of one form into that of another — to allow, for example, a combination rendezvous. 

There is an analogous operator — `guardRv` that has the effect of enabling a rendezvous only if some condition is met.

In both cases, the result is a new invocation of `semLoop` which — depending on whether the counter is zero or not — will either listen for a `release` or both a `releaseR` or `grabR`:

```
fun semLoop(0) is wait for releaseR(0)
 |  semLoop(X) default is wait for grabR(X) or releaseR(X)
```

>In implementations of **Cafe** that do not support tail recursion, this code would need to be rewritten as a loop. However, we leave it as a recursion for expository purposes.

If we focus on the disjunction in:

```
wait for grabR(X) or releaseR(X)
```

This is a rendezvous that is composed of two rendezvous; and the `wait for` operator waits for either one to occur. Disjunctive rendezvous like this are critical for many concurrent applications.

It is important to emphasize that the disjunctive `wait for` waits for exactly one rendezvous to occur. In the situation that both happen simultaneously then only one will be picked for this choice. The unchosen rendezvous may be 'picked up' by another `wait for` operation.

Notice that the disjunctive rendezvous is only entered when the value passed to `semLoop` is non-zero (it will actually be positive). If the value was zero then the task will only wait for a `releaseR` rendezvous. This captures the essential requirement that a rendezvous will limit access to its associated resource to a fix number of clients.

We are now in a position to show the complete workings of the `semaphore` function in:

```
fun semaphore(Count) is let{
  def grabCh is channel()
  def releaseCh is channel()
 
  fun releaseR(X) is 
    wrapRv(recvRv(releaseCh), (_) => semLoop(X+1))

  fun grabR(X) is
    wrapRv(recvRv(grabCh), (_) => semLoop(X-1)

  fun semLoop(0) is wait for releaseR(0)
   |  semLoop(X) default is wait for grabR(X) or releaseR(X)

  { ignore background semLoop(Count) }
} in {
  fun grab() is wait for send(grabCh,())
  fun release() is wait for send(releaseCh,())
}
```

Notice that the value returned by the `semaphore` function is itself a record — with the two functions `grab` and `release` embedded in it. Thus an expression of the form:

```
S.grab()
```

is an invocation of the `grab` function from that record — and denotes an attempt to 'grab' the resource managed by the semaphore. Similarly, `S.release()` counts as releasing the resource.

Because the inner workings of the semaphore are protected by a `let` expression, they cannot be seen externally by other programs — only the `grab` and `release` functions are exposed; hence meeting one of the key requirements of the semaphore implementation.

####A Meeting of Philosophers[a-meeting-of-philosophers]

We can model a philosopher as a `task` function that iteratively acquires a left and right fork, 'eats' for a random period, and then stops eating after relinquishing its two forks (so another philosopher can eat).

The classic Hoare solution to the deadlock problem is to invoke an additional element: a central 'table'. The role of the table is to ensure that only one philosopher at a time is requesting forks; this, in turn, prevents a deadlock situation where two or more philosophers can start the process of acquiring their forks but are unable to complete because the another philosopher 'has' the other fork.

The table can be modeled straightforwardly as a semaphore with a limit of 1: only one philosopher can be starting to eat at any given time. Note that this does not mean that only one philosopher is eating at any one time: as soon as a philosopher has gotten two forks, another philosopher can start the process. Of course, given that there are four forks, only two philosophers can actually be eating simultaneously.

The main cycle for the philosopher task then looks like:

```
sleep(random(10L))  -- chat some
perform T.grab()    -- get permission first
perform L.grab()    -- get left fork
perform R.grab()    -- get right fork
perform T.release() -- release the table

sleep(random(15L))  -- eat some
  
perform L.release() -- let go of left fork
perform R.release() -- let go of right fork
```

The `sleep` calls in this sequence stand for some arbitrary activity — such as chatting or eating — that does not involve forks and/or the central table.

The complete `phil` function returns a `task` that performs this cycle a fixed number of times. The program below shows the `phil` function that takes the semaphores for the left and right forks as argument, together with an identifying philosopher number for debugging purposes.

```
fun phil(n,L,R) is task{
  for Ix in range(0,Count,1) do{
    sleep(random(10L))  -- chat some
    perform T.grab()    -- get permission first
    perform L.grab()    -- get left fork
    perform R.grab()    -- get right fork
    perform T.release() -- release the table
    sleep(random(15L))  -- eat some
    perform L.release() -- let go of left fork
    perform R.release() -- let go of right fork
  }
} 
```

>The eagled-eyed reader will notice that while `L` and `R` are parameters of the `phil` function, the `T` variable is not. Indeed, it is a free variable that must be bound in an outer scope.

The complete setup for the dining philosophers involves creating separate semaphores for each of the forks and for the table, spinning off background `task`s for the philosophers, and then forcing them to run with a sequence of `perform` actions is given in:

```
prc dining(Count) do let{
  def T is semaphore(1)  -- The table
  
  fun phil(n,L,R) is task{ ... }
} in {
  def fork1 is semaphore(1)
  def fork2 is semaphore(1)
  def fork3 is semaphore(1)
  def fork4 is semaphore(1)
  
  def phil1 is background phil(1,fork1,fork2)
  def phil2 is background phil(2,fork2,fork3)
  def phil3 is background phil(3,fork3,fork4)
  def phil4 is background phil(4,fork4,fork1)

  perform phil1
  perform phil2
  perform phil3
  perform phil4
}
```

This program will run the simulation for four philosophers for a `Count` number of times. We have not added any trace information, so there will not actually be any output; however, that is relatively straightforward to do.

The dining philosophers is a toy example; nevertheless it highlights many of the issues found in regular parallel applications where there is some shared resource.

##Asynchronous Communication[asynchronous-communication]

We stated at the beginning that 'synchronous communication was more basic' than asynchronous communication. Nevertheless, there are times when asynchronous communication is called for.

>In general, asynchrony increases the potential for parallelism; it also increases the complexity of coordination.

One pattern that makes inherent use of asynchronous communication is the worker-queue pattern. The worker-queue pattern consists of a source of 'work', a queue to hold unfinished work items and one or more 'workers' that perform a typically compute intensive task on each item. For example, an image rendering farm might consist of an image generator, a queue and a set of image renderers. (Image rendering, such as resizing an image encoded in JPEG or converting from one form to another, often takes considerable compute resources.)

>Our focus here will be on the implementation of the queue, rather than the implementation of any image rendering process.

Other patterns that make use of asynchronous communications include the _publish-subscribe_ and the _actor_ patterns. In applications where there is essentially a one-way flow of information from publishers to subscribers there is less need to tightly synchronize communications. Similarly, actors denote semi-autonomous activities that collaborate with each other to solve shared _goals_. In fact, almost _every_ multi-user application can be modeled in terms of agents.

###A Message Queue[a-message-queue]

We start by defining a type that denotes the interface to the queue. For convenience, we use a _type alias_ to a record with a `post` function and a `poll` function within it:

```
type messageQ of a is alias of {
  post has type (a)=>task of ()
  poll has type ()=>task of a
}
```

In effect, the record type is a specification of an API for accessing a message queue.

>One might ask why do we use `task` oriented types like `task of ()` and `task of a` types? I.e., why wrap the type of the message in a `task` type?

The `messageQ` is likely to be used in the context of `task` expressions; and certainly its implementation requires tasks. By exposing this task-nature we actually improve the potential responsiveness of the message queue and of functions that use it.

Like the semaphore that we saw earlier, we will build the message queue using an internal background task that manages the actual queue of messages.

```
msgQ has type for all a such that
  ()=>messageQ of a
fun msgQ() is let{
  def postMsgChnl is channel()
  def grabMsgChnl is channel()

  fun qLoop(Q) where isEmpty(Q) is wait for postM(Q)
   |  qLoop(Q) default is wait for postM(Q) or pollM(Q)
 
  fun postM(Q) is 
    wrapRv(recvRv(postMsgChnl), (A) => qLoop([Q..,A]))

  fun pollM([A,..Q]) is let{
    fun reply(R) is valof{
      perform send(R,A)    -- reply on the one-time channel
      valis qLoop(Q)
    }
  } in wrapRv(recvRv(grabMsgChnl), reply)
    
  { ignore background qLoop(queue of []) }
} in {
  fun post(A) is wait for sendRv(postMsgChnl,A)
  fun poll() is task{
    var ReplyChnl is channel()
    perform wait for sendRv(grabMsgChnl,ReplyChnl)
    valis valof (wait for recvRv(ReplyChnl))
  }
}
```

This code is quite complex in places. However, if we break it down and examine it piece by piece its secrets will be exposed. Let us start with the `qLoop`/`postM`/`pollM` triumvirate. These three functions form the heart of the message queue's mechanism.

The `qLoop` function has a similar structure to the `semLoop` function we saw in the semaphore. This time, instead of deciding what branch to take based on a resource counter, we decide based on the size of the actual queue of data.

>The `queue` data type is a builtin type to **Cafe**. It implements a queue functionality allowing elements to be posted at one end and removed from the other. Like other collection types, the `sequence` contract is implemented for it; which means that we can use sequence notation when writing queue expressions and patterns.

The `postM` function — like its cousin the `releaseM` function in the semaphore — is a rendezvous-valued function that the `qLoop` function will `wait for`. It's inner core is a recursive call to `qLoop` with a `queue` that is enhanced with the new work item.

The `pollM` function has a similar structure; but it is more complex because — in addition to setting up the next `qLoop` recursion — we also want to export the entry from the queue itself.

Polling from the message queue is accomplished by sending a special one-time channel to the message Q task. Somewhat paradoxically, the `poll` function asks for the next element of the message queue by sending the one-time channel to the message queue's `grabMsgChnl`:

```
fun poll() is task{
  def ReplyChnl is channel()
  perform wait for sendRv(grabMsgChnl,ReplyChnl)
  valis valof (wait for recvRv(ReplyChnl))
}
```

The first two actions in the `task` expression of the `poll` function are fairly normal: we create a channel, and send that on the `grabMsgChnl` channel. The third line bears some more examination: what exactly is that `valis valof` doing?

The simple answer here is that this is a `task`-valued function, and the value of the `wait for` expression is also a `task` — it happens to be the task that we want to return. But, normally, a task expression that performs a `valis E` action would have type

`task of` E~t~

so, apparently, we take the value of the `task` returned by the `wait for` and rewrap it back as a `task`!

In fact, this particular combination is a part of the overall task notation: instead of performing this unwrap and rewrap, the value returned by the `wait for` will be returned directly _without necessarily waiting for the task to finish_. We can do this safely because the overall value of the sequence is also a task.

>There is a further complication here. Recall that the task notation is based on monads. There are potentially many kinds of monads and the `valis valof` combination allows us to _change monads_. This, however, is a topic beyond the scope of this book.

We can use our message queue to knock up a quick simulation of a system of workers and a work queue:

```
worksheet{
  ...
  fun sender(Q) is task{
    for i in range(0,1000,1) do 
      perform Q.post(i)
    sleep(3000L)
    logMsg(info,"all done")
  }
  
  fun worker(W,Q) is task{
    while true do{
      def Nx is valof Q.poll()
      logMsg(info,"$W doing $Nx")
      sleep(random(100L))
    }
  }
  
  def MQ is msgQ()
  ignore background worker("alpha",MQ)
  ignore background worker("beta",MQ)
  
  ignore valof sender(MQ)
}
```

This example spins up 1000 work items and expects the `"alpha"` and `"beta"` workers to process them. Typical bosses attitude!

##Comment[comment]

The concurrency features of task expressions and rendezvous avoid the synchronization problems that plague concurrent and parallel programming in traditional languages. Moreover, tasks and rendezvous scale easily to hundreds of thousands of threads.

However, it should also be clear that they are also a little low-level: in order to program something like a semaphore or a message queue requires fairly careful attention to detail.

The true merit of **Cafe**'s concurrency features is that they do allow these higher level constructions to be programmed in a way that does not require extreme effort.

Indeed, as we shall see in [agent programming][chattering-agents] it is possible to implement a system of parallel agents with extremely minor tweaks to the regular sequential versions of the program.
#Application = Policy + Mechanism[application-policy-mechanism]

It should be clear at this point that **Cafe** is quite a rich language. There are many features and _sub-languages_ that form the whole. We have seen several of these, including quite complex features like queries. In Chapter [chattering-agents][] we will also see a sophisticated sub-language based on actors and speech actions. It may surprise you to learn that the core of **Cafe** is quite small, with many features implemented in **Cafe** itself. Our task now is to introduce you to some of the techniques available to build your own extensions.

One of the most fundamental and surprisingly difficult issues in language design is whether to allow 'regular' programmers to extend it. Different language communities have taken radically different stances to this: languages like C/C++ and the LISP family were designed to be extensible by the programmer.[^With LISP arguably being more successful in that.] On the other hand, the Java designers explicitly chose not to include any form of macro feature — their rationale being that macros make it easy to confuse programmers with strange syntax features.

However, no language that is in active use is fixed. Simply using a programming language is often enough to point to ways in which it might be improved; and language stewards are understandably interested in ensuring the continued relevance of the language in their care by augmenting it with new features.

In the end, it reduces to a question of democracy on the one hand and a curated environment on the other. Language designers can choose to allow evolution to occur 'in the wild' or can choose to try to control it with a disciplined process.

>Neither strategy is inherently better than the other. Both carry risks. A bottom-up approach risks splintering and difficulties with effective tooling. A top-down approach is often slower and more deliberate; which can mean being made irrelevant in a fast paced world.

For better or worse,[^We believe better of course.] **Cafe** was designed to allow programmers to extend it without requiring changes to the basic compiler — i.e., the language has facilities to support its own evolution and, particularly, the development of _Domain Specific Languages_.

>Although **Cafe** permits regular programmers to define extensions, you should be prepared for some difficult and complex topics. Designing and extending programming languages is a subtle art that calls for some obscure concepts at times.

On the other hand, designing for extensibility does not necessarily mean adopting macro language features like those in either C/C++ or LISP. Designing the language extensibility features needs to be done with the same care as the 'main' language.

**Cafe**'s has a well developed set of language elements that are designed to make it easier to develop reliable extensions to the core language. The primary features involved are:

* an extensible grammar — allowing new operators to be defined;

* a rule-based system for defining _well-formed notations_; and

*  a powerful _macro language system_ that is used to implement extensions by mapping them to core language elements.

Together, there are sufficient facilities for making a huge range of potential extensions and sub-languages.

##How to Design a Domain Specific Language[how-to-design-a-domain-specific-language]

One of the primary reasons for wanting to have domain specific languages is to be able to succinctly express _policy_. Pretty much every significant application and module tends to be more general than the individual problem it was designed for; which means that actually applying the application (sic) amounts to using a subset of the capabilities of a general mechanism to solve the specific problem.

Policy has a number of definitions, one of which is

**Policy**
:An expression of a constraint that governs the behaviors of a system.

The term _system_ here is intended to be used broadly, in particular to include users and humans agents participating in the system.

It is possible to see — without squinting too hard — that a policy is a statement that describes a subset of the potential behaviors of a system. Furthermore, one can often express a particular use of a capability or application in terms of constraints — i.e., in terms of policies.

An example might help. Suppose that you want to apply statistical algorithms to measure how well your stocks are doing. You might want to know if your portfolio is doing better than the average for example.

If you did not have access to any statistical functions, then you would have a choice when it comes to computing the average performance: you can construct a function that only computes the average of your stock portfolio, or you can be sensible and split the problem in two: write (or get) the necessary statistical code and _apply it_ to the problem of computing averages in your portfolio. In fact, you are most likely to construct a range of 'portfolio functions' in the expectation that you may have other calculations to make: like computing overall profit and loss of your stock; you will then select which function to apply when you use the application.

In this case, the core algorithms for computing statistical averages (and other statistical functions) form a _mechanisn_ that can be applied to solving the problem at hand. Using the portfolio analysis application becomes, in effect, a matter of choosing which policy to use to _constrain_ all the available uses of statistics into the one that is important.

It might seem a stretch to refer to selecting a function as a constraint, but consider a slightly different scenario: suppose that there are several forms of _regression analysis_ functions[^A regression function is a line fitting function.] in your library. You want to pick the best one to enable you to forecast your stock. Picking the best regression function is often a case of selecting which features are most important — i.e., it is a matter of policy.

The pressure to factor the problem into mechanism and policy is so strong that it is easy to believe that this split _is the only way of solving the problem_. It isn't, of course, and there are many less obvious examples than figuring out portfolios.

DSLs and policy languages often involve quite different _semantics_ from the host language; they are often _declarative_ in nature — specifying _what_ needs to be done and leaving the _how_ to the mechanism. One of the simplest purely declarative language systems is the _triple logic_ as seen in RDF. RDF makes a great basis for representing graph structures generally and _configurations_ specifically.

##Resource Definition Framework[rdf]
RDF semantics is extremely simple: a RDF graph is a set of _triples_ consisting of a _subject_, _predicate_ and _object_; each of which is a _concept_. A graph is constructed by having triples linking to each other: the object of one triple being the subject of another. RDF is particularly flexible here as even predicates may be the subjects and objects of other triples.

RDF is good for representing the simple 'facts' that one often sees in applications that have to model aspects of the real world. A classic example of this is in modeling things like giraffes: if we have a Giraffe called Joe, then we typically want to be able to say things like:

```
Joe isa giraffe
```

which means

>Joe is a Giraffe

RDF is not especially powerful — which is actually one of the key design points in the language. Its simplicity means that it is easily manipulated and processed.

Another nice feature of RDF is its extensibility by means of special vocabularies. One of the basic standard vocabularies is RDFS which introduces a languages of classes, sub-classes and instances to basic RDF. In our giraffe example, the RDFS vocabulary allows us to talk about categories as well as individuals:

```
:Joe isa :giraffe
:giraffe isa class
:giraffe subclass :mammal
:giraffe has :long-neck
:mammal isa class
:mammal subclass :animal
:mammal has :four-legs
```

RDF stores that understand this vocabulary can automate many simple inferences. Another standard vocabulary is OWL which is the standard for representing Ontologies in the Semantic Web. For example, from these facts, we can also infer:

```
:Joe has :long-neck
:Joe has :four-legs
```

These inferences are part of the semantics of RDFS; they are added to the graph as a consequence of the other facts. A graph simply consists of a set of such 'triples'; often there are many millions of triples when modeling complex domains.
 
Just having a graph is much like having a flat data structure; its utility for describing policies comes from being able to process it effectively, specifically to be able to _query_ it. There are many possible ways of querying triple graphs; will look at a simple one which can be easily integrated with **Cafe**'s standard query notation.

>It is popular to use a meta-syntax such as XML and JSON to encode policy expressions. The fundamental problem with using these is that they do not easily support type consistency and programming languages typically do not give good integration between XML/JSON strings and program code. The net effect is one of clumsiness and needless complexity.

###There is Methodology in my Ontology[there-is-methodology-in-my-ontology]

The most important initial task in designing a DSL or policy framework has nothing to do with operators, macros or any of the facilities of **Cafe** — you must identify the appropriate structure and semantics for your language. More formally, this can be defined as identifying the _ontology_ of the language.

Ontology, as a discipline, dates back to Aristotle. He was one of the first people to attempt to systematically classify the known world. Nowadays, Ontology refers to the study of the relationship between concepts and the real world.

**Ontology**
:A description of a set of concepts and how they relate to each other and to the intended domain.[^The 'official' definition of an ontology is a systematic conceptualization of a domain.]

A systematic, or formal, ontology is often written in a language that allows automated verification of desirable aspects of the ontology — such as its consistency.

Developing an ontology often starts by brainstorming a set of words that seem to be important in the domain. For example, an ontology of pet animals would include the concepts of Dog, Cat, Giraffe and so on. It would also include the concepts of caring for a pet and of breeding them.

Our RDF triple language also has an ontology — about the concepts involved in knowledge graphs. The figure below shows such a collection, which, while it is not the complete set needed for conceptualizing triple graphs, makes a good start:

![A Collection of Concepts Important to Triple Graphs][rdfConcepts]

[rdfConcepts]: images/rdfConcepts.png width=250px

Once you have a set of concepts it helps to organize them. Common meta-concepts in ontologies include the notions of _class_, _instance_, and the _sub-class_ relationship — not to be confused with similar concepts in Object Oriented Programming. For example, your giraffe Joe denotes an instance of the Giraffe class, and Giraffes are a sub-class of the more general concept of Pet.

One simple (sometimes simplistic) tool for figuring out an ontology is UML. UML has the great advantage of being very visual and therefore can often be useful in bringing out the salient parts of an ontology. For example, a UML diagram of our triple graph conceptualization highlighting some of the important classes and their relationships can be seen in:

![UML Diagram of Triple Graph Ontology][rdfUML]

[rdfUML]: images/rdfUML.png width=320px

The role of the UML diagram is to make it clear what the key features and components of your language are — so you don't miss anything important.

>The careful reader will notice that we have actually extended the ontology to include the concepts of _class_, _cub-class_ and _instance_; which are actually part of RDFS more than basic RDF.

###Nouns and Verbs[nouns-and-verbs]

Natural language grammar often has many kinds of words; but the two most important kinds of words are nouns and verbs. The same is usually true of computer languages: there are _things_ that we want to represent and process in some way and there are _actions_ that we want to be be able to model.[^Even in a pure expression language, a similar distinction holds: there is data and there are operators over that data. To be fair, there is an extreme variant of functional programming that only has functions; but even there the distinction between data and operators is still meaningful.]
 
The kind of UML-based analysis that we have used so far often leads naturally to a classification of the things in the DSL. Knowing what you want to do with those things may be harder to clarify. However, we have not finished with our use of UML.

One way of defining action, as illustrated in the UML diagram is:

![Ontology of Action][action]

[action]: images/action.png width=300px

**Action**
:An _action_ is the intentional application of force to achieve an objective.

Here, 'force' is intended to be broadly interpreted — it can include, for example, calling a function as well as lifting a rock.

Under this definition of action, if a tree in a forest falls on a squirrel and squashes it that was not an action because the tree did not (cannot) intend to fall on the squirrel. On the other hand, the jump the squirrel makes when it sees the tree falling is an action: it intends the application of force on the rock it was sitting on — in order to escape the falling tree.

This analysis of action is important because it gives us all kinds of hooks for designing the verbs in our DSLs. For example, a lot of object-oriented programming can be understood by letting the _target_ of an action be a data value.

More generally, we have:

**Target**
: The target of an action is the thing that is being operated on. Not all actions have an obvious target; for example, the true target in an e-mail marketing campaign is not the e-mail system but the human readers of e-mail.

**Instrument**
: The instrument is the means by which the action is performed. In the case of the squirrel above, the instrument is the squirrel itself; in the case of calling an API it is the API method.

**Force**
: This can be difficult to get one's head around. Without force though, there is no action. In the case of a computer program, force amounts to execution: the action is performed when the corresponding program fragment is executed.

**Agent**
: Agency is what distinguishes action from random events.

  There are many possible interpretations of agent; we shall see some of them in [our chapter on communicating agents][chattering-agents]. The simplest form of agent in a programming language is simply the thread of control. However, if your DSL is about multiple threads or parallelism in some way, then you will likely need to focus on agent as a specific concept.

**Intention**
: A key aspect of action is that it is intentional — or to put it counterfactually: an action is not an accident.

  When designing a DSL, the verbs in the DSL that correspond to actions are normally some form of command. A command also pre-supposes an agent that issues the order.

  >What difference does this make? There may be other kinds of verbs that you will need in your DSL. For example, declaratives state something about what is (or should be) true. Declaratives are not actions; if for no other reason than declaratives are not associated with intention.

**Objective**
: It is the _objective_ that often gives the most concrete guidance in designing DSLs. There may be many objectives and any given objective may have many actions associated with it. However, if you list your actions' objectives, then you may have the beginnings of a list of verbs.

  Note that we make a distinction between objectives and goals. Agents have goals, actions have objectives. Goals can be viewed as desirable states of the world, an objective is a measurable state. Goals are often not directly observable; by their nature, objectives observable in principle — though not necessarily by everyone.

  Objectives also allow us to distinguish between intended results and unintended effects.

Not all DSLs make all the features of an action explicit. However, the exercise of iterating through the different aspects of action will often lead to a clear conceptualization of the nouns and the verbs that form the core of the DSL.

In the case of our RDF language, the principal actions relate to store and recall. We need to be able to manipulate triple graphs and we need to be able to query them. Here, we focus on querying because that is core to the role of any knowledge store. 

##Designing Syntax[designing-syntax]

Once we have at least an approximate conceptualization, our next step is to design the syntax. Syntax is important because it encodes the manner in which different features of the DSL can be expressed and combined.

>There is an inevitable requirement for _taste_ in designing the syntax of a language extension. One of the foundations for this taste is knowledge of the existing syntactic patterns in the host language. Another is an awareness of the combinatorial potential in the language — i.e., what can be combined with what.

Notice that we identify a _graph_ as being a sub-class of _expression_; this represents an important choice point in DSL design: whether we are extending _expressions_, _statements_, _actions_, or _types_. Of course, in complex projects you may find yourself implementing multiple kinds of extensions. We choose to model graphs as _expressions_ as that gives us the maximum flexibility in using triple graphs to represent policies.

###Triple Notation[triple-notation]

The focal structure in a triple graph is the _triple_. This is described as being a triple of a subject/predicate/object. Our triple graph notation is based on the [N3][#N3-W3C] notation — which is a more human readable version of the standard XML RDF syntax adopted by W3C:

_Subject_ `!` _Predicate_ `$` _Object_

The idea is to make it easy to represent facts like:

```
john ! likes $ mary
peter ! in_department $ accounting
```

We also allow raw `string`s in our triples, to enable us to represent information that is _external_ to the graph:

```
john ! address $ "2 smart place"
john ! name $ "John Smith"
```

####Design is Iteration[design-is-iteration]

As we noted, having a triple graph is only as useful as our ability to use it; especially to examine its contents. To simplify that task we _could_ create a query language specifically for triple graphs. However, a more subtle and integrated approach is to _extend_ the standard built-in query language with conditions that are tailored to work with triple graphs. This has the advantage that we can leverage standard query processing to help process triple graphs.

So, we need an extension to standard query conditions that allow us to:

```
list of { all X where OrgTree says X ! department $ accounting }
```

However, we immediately hit a road-block: what about variables? How can we distinguish the occurrence of `X` above to denote a variable whilst allowing `accounting` to be the fixed name of a concept: they are both identifiers.

We are pretty much guaranteed to encounter variables in query expressions; but we may also encounter them in `graph` expressions — especially if we construct graphs dynamically. To ensure that we can reliably distinguish variables from named concepts we have to modify the design of our triple graph language.

So, to resolve this, we modify the syntax for named concepts to have a leading colon operator; our simple graph above becomes:

```
:john ! :likes $ :mary
:peter ! :in_department $ :accounting
```

and the query becomes:

```
list of {all X where OrgTree says X ! :department $ :accounting }
```

Now the occurrence of `X` above is clearly a variable and not the name of a concept.

>There is an additional reason for choosing to lead a concept with a colon: when dealing with multiple graphs it may be useful for one graph to refer to a concept in another graph. However, we will leave this extension for another day.

With this change, we can represent our original giraffe example in our RDF DSL:

```
:Joe ! isa $ :giraffe
:giraffe ! isa $ class
:giraffe ! subclass $ :mammal
:giraffe ! has $ :long-neck
:mammal ! isa $ class
:mammal ! subclass $ :animal
:mammal ! has $ :four-legs
```

With the inferred triples:

```
:Joe ! has $ :long-neck
:Joe ! has $ :four-legs
```

Designing DSLs, like designing anything, is often an iterative process. It is important to remain flexible when designing language features. For example, one rather glaring limitation to our triple graph notation is that it is limited to symbolic concepts and literal strings. We leave as an exercise to the reader how to modify the notation to allow arbitrary **Cafe** values to be referenced in triple graphs.

###Operator Precedence Grammar[operator-precedence-grammar]

**Cafe** has a very flexible syntactic foundation based on a _Operator Precedence Grammar_. You are already quite familiar with operator precedence grammars — they are used in nearly every programming language to represent arithmetic expressions. For example

```
X+Y*3
```

is a very common way of representing the addition of `X` to the result of multiplying `Y` by 3. It also represents the application of two operators: `+` and `*` which happen to be _binary_ operators.

>The operator structure of an expression is completely independent of any type information or its run-time performance.

We should be careful to note that the term _operator_ here has at least two overlaid meanings: there is the sense in which operator is a _syntactic_ structure — with rules for legal sequences of tokens — and there is the sense in which operator is a _function_ to be applied to arguments — with rules for type safety and information flow. In this section, we are focused on the syntactic aspects of operators.

**Cafe** makes quite extensive use of operators in its own grammar, nearly every feature of the language relies on operators for its syntax.

There are many different kinds of operator: we can have _prefix_ operators like unary `-`; _infix_ operators like `*` and _postfix_ operators like `;`. Although prefix and infix forms tend to be much more commonly used than the postfix form. It is quite possible for the same operator to have multiple forms: for example the `-` is both infix and prefix — which allows us to use the same symbol for subtraction and to represent negative numbers.

To support parsing and operator combination, an operator is associated with a _priority_ number — which encodes the relationship between operators. In the case of **Cafe**'s operators, this is an integer in the range 0 to 2000; where the higher the priority the more dominant the operator is in the syntax. For example, the priorities for `+` and `*` are 720 and 700 respectively. It is this relative difference that determines that `X+Y*3` means the same as `X+(Y*3)` and not `(X+Y)*3`.

The final attribute of an operator is its _associativity_. Associativity determines what happens when you have multiple operators of the same priority in sequence. For example, arithmetic operators are traditionally _left associative_. This means that

```
X-Y-Z
```

is the same as

```
(X-Y)-Z
```

rather than

```
X-(Y-Z)
```

All this can be put together in a single **Cafe** statement: the operator declaration. In the case of our triple we are using two operators `!` and `$` to 'glue' together the parts of a triple. We also use the `:` operator to mark named concepts. The operator declarations we need to represent them are:

```
#right("!",500) 
#right("$",450)
#prefix(":",100)
#infix("says",908)
```

One may ask where these priority numbers come from? Operators in **Cafe** are stratified into different levels depending on their syntactic role: 0-899 represent expressions, 900-999 represent predicates and conditions, 1000-1199  are used for forms that can be either expressions or actions, 1200-1499 represent actions and 1500-200 represent statements. The choice of 908 for `says` is to make it the same as the built-in predicate operator: `in`. The main effect of choosing the 'wrong' value for the priority of an operator is that expressions don't parse the way you would like.

Notice that the word `says` is also an operator! **Cafe** allows us to use words as operators as well as using 'graphical' symbols like `+`.

We chose to use a word-style operator here — `says` — to introduce the triple condition; yet we chose graphical operators for the triples themselves. One of the ways that taste shows up in designing DSLs is in areas like this: what names do we use.

>As it happens, part of the texture of **Cafe** is to use keywords frequently. There are graphical operators of course, but one of the hallmarks of **Cafe** is its liberal use of keywords. While this undoubtably makes for more typing it also makes for a more readable language.

>Picking the right priority for operators is one of the most subtle aspects of designing syntactic extensions to **Cafe**. The language definition has a table of all the standard operators and their priorities.

####Mixing Operators[mixing-operators]

The standard N3 notation includes some simple extensions to make certain patterns of facts easier to write. For example, we can write:

```
:Joe ! isa $ :giraffe
:giraffe ! [subclass $ :mammal, has $ :long-neck]
:mammal ! [subclass $ :animal, has $ :four-legs]
[:giraffe, :mammal, :animal] ! isa $ class
```

instead of

```
:Joe ! isa $ :giraffe
:giraffe ! isa $ class
:giraffe ! subclass $ :mammal
:giraffe ! has $ :long-neck
:mammal ! isa $ class
:mammal ! subclass $ :animal
:mammal ! has $ :four-legs
:animal ! isa $ class
```

For the purposes of the operator grammar, literal values like strings, numbers and _bracketed values_ have a priority of 0. However, within a bracketed term — such as

```
[:giraffe, :mammal, :animal]
```

each element of the list has an expected priority; in case of lists the expected priority is 999. I.e., the maximum priority of any term in a list sequence is 999. If you want to embed an operator expression whose top-level operator is higher than 999 in a list then you need to surround it with parentheses.

###Embedding Graphs[embedding-graphs]

One of **Cafe**'s standard syntactic paradigms is the 'brace term'. We use it to represent records, as in:

```
someone{ name="fred" }
```

We also use it to represent more complex entities like packages and worksheets:

```
worksheet{
  show 1+2
}
```

Continuing in this tradition we will use a similar syntactic structure to represent complete triple graphs:

```
graph{
  [:peter, :john] ! :in_department $ :accounting
  :john ! :address $ "2 smart place"
  :john ! :name $ "John Smith"
}
```

Since this is a first class value, we can have variables bound to triple graphs, as in:

```
OrgTree is graph {
  [:peter, :john] ! :in_department $ :accounting
  ... 
}
```

and our queries

```
list of {all X where OrgTree says X ! :department $ :accounting }
```

So far, we have designed a new syntax for a triple graph extension to **Cafe** with very little effort: a few operator declarations. However, we are not done yet., we need to make sure that the **Cafe** compiler understands the new syntax.

##Validation[validation]

Simply defining operators is not enough to design a language extension. We must be able to make sure that the operators are used sensibly in the appropriate context, and that the **Cafe** compiler can make sense of the new syntax. To help with this, **Cafe** has a standard way of representing the valid syntactic forms of language extensions — using a set of _validation rules_.

The purpose of a validation rule is two fold: it defines the legal syntactic forms and it enables a language extension designer to report syntax errors in terms of the DSL rather than in terms of operators (which most users of the DSL should not have to understand).

A validation rule uses patterns to define legal instances of syntactic _categories_. The top-level categories are `expression`, `pattern`, `statement` and `action`; however, it is often useful to introduce additional categories. For example, one validation rule that we use for defining the legal forms of triple might be:

```
# ?S ! ?P $ ?O :: triple :- 
    S::concept :& P::concept :& O::concept
```

This validation rule states that terms such as

```
:john ! :name $ "John Smith"
```

are `triple`s providing that `:john`, `:name` and `"John Smith"` are concepts. The operator `::` is a standard operator that can be read to mean 'has syntactic category'. The `:-` is read as 'if' and `:&` is read as 'and'.

>The meta language features of **Cafe** have a completely different texture and style to regular programs. This is deliberate; it allows a clear separation between regular code and meta code.

The validation rules for `concept` allow for the two basic forms of concept:

```
# : identifier :: concept
# string :: concept
```

These rules state that the two legal forms of concept are a colon-prefixed identifier and a literal string. However, we might want to allow simple identifiers also — given that such 'concept variables' may also appear:

```
# : identifier :: concept
# string :: concept
# identifier :: concept
```

####Escaping our DSL
We are making a subtle choice with these validation rules for `concept`. We are affirming that the only way that we can embed 'normal' **Cafe** expressions in a concept graph is through variables.

In general, it is normally desirable to allow arbitrary expressions to be embedded in DSL extensions. However, one has to design the syntactic mechanism that permits this escape. In our case we take the extremely simple approach of only allowing variables.

####Variations on a Theme
Because we want to allow different variations on the legal forms of triple, the complete validation rules for `triple` are more complex and involve several different categories:

```
# ?S ! ?V :: triple :- S::nounPhrase :& V::verbPhrase
# ?T :: triple :- error("$T is not a valid triple")
 
# ?P $ ?O :: verbPhrase :- P::verb :& O::nounPhrase
# [ ?VP ] :: verbPhrase :- VP::verbPhrases
# ?VP :: verbPhrase :- 
    error("$VP must have at least one predicate and one object")
 
# ?A,?B :: verbPhrases :- A::verbPhrase :& B::verbPhrases
# ?A :: verbPhrases :- A::verbPhrase
 
# [?V] :: verb :- V::verbs
# ?V :: verb :- V::concept
 
# #(?V1,?Vr)# :: verbs :- V1::concept :& Vr::verbs
# ?V :: verbs :- V::concept

# [ ?NP ] :: nounPhrase :- NP :: nounPhrases
# string :: nounPhrase
# ?C :: nounPhrase :- C::concept
 
# #(?NP1,?NPr)# :: nounPhrases :- NP1::nounPhrase :&
     NPr::nounPhrases
# ?NP :: nounPhrases :- NP::nounPhrase
```

One thing to notice about these validation rules are the rules of the form:

```
# ?VP :: verbPhrase :- 
    error("$VP must have at least one predicate and one object")
```

What this rule is saying is that if the earlier rules for `verbPhrase` don't apply then the compiler should report an error message. The notable thing is that the error message is in the context of the triple graph notation: i.e., the compiler is able to report syntax errors in terms of a language extension that is itself not known to the compiler! This is important because it helps to prevent _abstraction leaks_ — where the user of a system must understand what triples might compile to before understanding what is wrong with their program.

One other thing to notice can be seen in the rule for `verbs`:

```
# #(?V1,?Vr)# :: verbs :- V1::concept :& Vr::verbs
```

The term `#(?V1,?Vr)#` is a special kind of parenthesized term that means the same as (in this case) `?V1,?Vr`. We use `#()#`'s parentheses because the regular parentheses `()`'s are not dropped during parsing by the operator grammar, whereas `#()#`'s are.

>This is because of the importance that regular parentheses play in the language: apart from operator precedence overriding (their most common use in programming languages), `()`'s are used for type expressions and for tuple terms. In general, terms of the form `(`_E_`)` are _not_ syntactically identical to _E_.

>The compiler applies parentheses reduction later in the compilation process; when it can be proved to be safe to eliminate regular parentheses.

The intention of the `verbs` rule is to allow multiple `verbs` in a predicate of a `triple` — its context is the rule that picks up on the `[]` form of predicate:

```
# [?V] :: verb :- V::verbs
```

This rule allows multiple verbs in a `triple`:

```
:john ! [:in_department, :works_at] $ :accounting
```

This form is equivalent to two triples with the same subject and object with with different predicates.

Having rules for `triple` is not sufficient to ensure validation of triple graphs. This is because the compiler has no a priori reason to understand or look for rules about `triple`. We need to establish the entry point into the validation.

>At the top-most level, the compiler attempts to validate the entire input as a sequence of `statement`s. It so happens that `package`s and `worksheet`s count as statements.

We stated earlier that a triple graph will be treated as an _expression_ in the larger context. So, the natural entry point for validating triple graphs is as expressions; which we can formalize by defining a new rule for validating expressions:

```
# graph{?G} :: expression :-
    G;*triple ## {
      # ?S ! ?V :: triple :- S::nounPhrase :& V::verbPhrase
      # ?T :: triple :- error("$T is not a valid triple")
    ...
}
```

The form `G;*triple` is a validation condition that states that `G` must be a sequence of terms (optionally separated by semi-colons) and each term must be a `triple`. In addition, we nested the set of validation rules for `triple` within the rule for `graph`. The `##` operator is analogous to the `let` operator for expressions; except that here we use it to denote that we should look for the rules for `triple` within the braces.

Since, the natural scope rules for validation rules is one of cascading and augmentation rather than replacement, the meaning of the `##` is a little different to the normal `let` expression. The validation rules that are located within the body of the `##` are applied before other validation rules located externally.

###Validating Triple Conditions[validating-triple-conditions]

Recall that we also introduced a new form of condition that is suited for querying triple graphs. We need to validate such conditions too.

In some ways our `says` condition is simpler than the triple itself because we choose not to allow our complex noun phrase terminology in the condition. This results in a very simple validation rule for a query `condition` involving triple graphs:

```
# ?G says ?S ! ?P $ ?O :: condition :-
  S :: concept :& P :: concept :& O :: concept :&
  G :: expression
```

Notice that this rule simply states that a `says` condition is valid if `G` is an `expression`. We do not require that `G` is a literal graph; although its type will need to be consistent.

>Unlike the rules for triples, this must must _not_ be embedded within a `##` condition. The reason is that we must be able to find `says` conditions anywhere in a program — not just inside a triple `graph`.

##Translating Graphs[translating-graphs]

Once we have rules for validating `graph` expressions in place the compiler is able to verify the form of triple graph expressions but is not able to type check or compile them. For this we need to be able to translate triple graphs to something the compiler can understand; for that we use _macros_.

###Representing Triple Graphs[representing-triple-graphs]

We start with representing triple graphs. Reflecting the ontology we constructed above, we can define three types to represent _concepts_, _triples_, and _graphs_.

There are two kinds of concept, and we want to keep them distinct in our representation:

```
type n3Concept is n3C(string) or n3S(string)
```

The two kinds of concept are the named concept — identified by the `n3C` constructor — and the literal string — identified by the `n3S` constructor.

The triple is similarly represented by a type definition:

```
type n3triple is n3(n3Concept,n3Concept,n3Concept)
```

where the three arguments to `n3` are the subject, predicate and object respectively of the triple.

Notice that here we are being explicit about the strong connection between subjects, predicates and objects: they are all _concepts_.

A triple graph may be represented in a variety of ways — it is effectively a collection of triples. However, for the purposes of exposition, we will assume that triple graphs are represented as lists of triples. We can capture this with a type alias statement:

```
type n3Graph is alias of list of n3Triple
```

>A more robust implementation of the triple graph store would provide better support for querying triple graphs by the individual subjects, predicates and objects of triples. This would normally involve being able to index the triples by their subjects, predicates and objects.

###Macro Rules[macro-rules]

Macros are programs that are executed by the compiler in order to transform terms into simpler forms — with the eventual goal that the terms produced by macro processing are directly understood by the main compiler.

There are two kinds of macros in **Cafe**: the _macro rule_ and the _code macro_. A macro rule is a substitution rule that is applied by the **Cafe** compiler during normal compilation. For example, the macro rule:

```
# - ?X ==> __uminus(X)
```

is used by the compiler to replace occurrences of unary minus with a call to the standard `__uminus` function that is part of the standard `arithmetic` contract.

This rule will 'fire' whenever an occurrence of unary minus occurs in your program. It will _not_ fire for regular subtraction — since the pattern on the left of the `==>` arrow only matches the unary case.

Like validation rules, macro rules are pattern based: that is, the left hand side of a macro rule is a pattern that is applied to input **Cafe** terms. In particular, there may be multiple macro rules that potentially match a given term. This makes macro rules quite a bit more expressive than either the macros in C/C++ or LISP. For example, the following macro rule can replace a multiplication by 2 with an addition:

```
# 2 * ?X ==> let { def x is X } in x+x
```

This rule will _only_ match multiplication expressions where the left hand side is the literal integer 2. It will not fire for any other form of multiplicative expression.

>The eagled-eyed reader may notice a small problem with this macro rule — the variable `x` may already be free within `X`. The macro rule notation has a way of dealing with this scenario; which we omit for the sake of clarity.[^For the technically inclined reader, **Cafe** macros are not _hygienic_. This is a deliberate choice, based on the anticipated role of macros in **Cafe**.]

Although we can use operators when we write macro rules, they are actually _insensitive_ to operators. For example, we could have written this macro rule using a normal function call pattern on the left hand side:

```
# (*)(2,?X) ==> let { def x is X } in x+x
```

>The `(*)` forces the compiler to suppress any operator interpretation of the the `*` character.

Macros written with macro rules can be quite complex, and it is possible to construct cascading sequences of macro rules to implement some impressive transformations. For example, the `actor` notation and the [speech action notation][speech-actions] is transformed into more regular **Cafe** by means of a package of macro rules.

In our case, we will use a macro rule to handle the translation of a `says` condition:

```
#  ?G says ?S ! ?P $ ?O ==>
    n3(trCon(S),trCon(P),trCon(O)) in G
```

By itself, rule would translate the term denoting the condition:

```
personnel says :john :works_in D
```

into the term:

```
n3(trCon(:john),trCon(:works_in),trCon(D)) in personnel
```

This translation is not complete because `trCon` is not a known function symbol. In fact, we also want to map `:john` etc. into concepts, i.e., into `n3Concept` terms; for which we need another set of macro rules:

```
#trCon(string?S) ==> n3S(S)
#trCon(: #(identifier?N)# ) ==> n3C(N)
```

The macro pattern `string?S` matches any literal string — and, if successful, binds the macro variable `S` to the matched string; similarly the macro pattern `identifier?N` matches any identifier and binds `N` to the found identifier.

We also need a third rule that allows for _graph variables_ by mapping identifiers directly to identifiers:

```
#trCon(identifier?N) ==> N
```

Notice that, even in this simple situation of matching concept, we had to construct a two-level macro cascade involving the strictly macro-time `trCon` symbol to ensure that they were translated appropriately. This requirement is one the weaknesses of the macro rule.

###Meta Language[meta-language]

**Cafe** has a _meta language_ and a standard type to go with it. This means that expressions in the language may also be values. For example, consider the expression:

```
X+2
```

If the value of `X` is 3, then the value of this expression is 5. However, we can also examine the language that this expression is made of — by using the quote notation. The expression:

```
<| X+2 |>
```

means the _name_ of the expression; for compiler-buffs this is effectively the _abstract syntax tree_ of the expression. **Cafe** has a standard type — `quoted` — whose abbreviated type definition is:

```
type quoted is nameAst(string)          -- identifier
            or integerAst(integer)      -- integer literal
            or applyAst(quoted,quoted)  -- application 
            or tupleAst(list of quoted) -- tuple term
            ...
```

and the quoted expression above is equivalent to:

```
applyAst(nameAst("+"),tupleAst(list of [nameAst("X"),integerAst(2)]))
```

Clearly, the quoted term is a lot less noisy than the 'real' version. The power of the meta language comes from the fact that any fragment of **Cafe** may be represented in the meta language — even fragments that are not part of the base language. It is also the basis of **Cafe**'s macro processing capabilities.

Any macro (whether its a macro rule or a code macro) is a function whose type signature is:

```
(quoted)=>quoted
```

The special feature of macro programs is that they are invoked by the compiler on the source program itself — hence the 'meta' in 'meta-language'.

####Meta Variables[meta-variables]

In addition to the basic quote notation for expressions, it also has support for _meta-variables_. Meta variables are variables embedded in quoted terms — they are variables of the quoted term, not variables in the _object language_. They are very useful in programs that process quoted expressions — both as expressions and as patterns.

For example, to denote a triple _pattern_ one may construct the quoted pattern:

```
<| ?S ! ?P $ ?O |>
```

where `S`, `P` and `O` are meta-variables that would be bound to the subject, predicate and object of the triple respectively. This quoted pattern is equivalent to the normally written term:

```
applyAst(nameAst("n3Triple"),tupleAst(list of [S,P,O]))
```

Note that the type of meta-variables is always `quoted`.

If we wanted to construct a quoted triple using the quoted notation we might have the following sequence:

```
var S is <| :john |>
var P is <| :name |>
var O is <| "John Smith" |>
var T is <| ?S ! ?P $ ?O |>
```

As a result of this, the variable `T` would be bound to the equivalent of the term:

```
var T is <| :john ! :name $ "John Smith" |>
```

The quoted notation is sufficiently powerful to almost never require explicit use of the constructors that actually make of the `quoted` type definition. This is reinforced by the implementation of the `coercion` contract between standard types and `quoted`.

###Code Macros[code-macros]

Code macros, as their name suggests, are normal **Cafe** functions that are invoked by the compiler on the text that the compiler is compiling. Code macros give the macro programmer almost the full power of **Cafe** to implement translation; in particular they can call other functions and use other types than `quoted` internally. However, code macros are a little more awkward to use than macro rules — there are some special restrictions on their form and what their scope is.

Code macros are embedded within a macro rule, in particular on the right hand side of a macro rule we can use a `##` form — which, like its counterpart for validation rules, is a way of introducing nested scoping. For example, the top-level macro rule for processing triple graphs looks like:

```
 # graph{?Graph} ==> list of [triples(Graph) ] ## {
  #fun triples(A) is wrapComma(mapSemi(triple,A))     
  ...
```

The environment part of the `##` block consists of regular **Cafe** code with one or more equations prefixed by a `#` to mark them as code macros. In this case the function `triples` is such a code macro. It is invoked by name in the 'bound' part of the `##` block:

```
list of [triples(Graph)]
```

Only those functions within the block that are marked by the prefix `#` may be mentioned directly within the enclosing macro rule (or within any macro rules also defined within the `##` block). Code macros must have the type

```
(quoted)=>quoted
```

Other functions within the `##` block may have any valid type — and this is where the great power of code macros comes from. In particular, whereas macro rules only ever 'use' quoted terms, code macros can use other types and can invoke arbitrary functions (of any type) in order to achieve their task.

In addition, where the evaluation of macro rules is somewhat complex — reflecting the potential for overlapping rules — the evaluation of code macros is identical to that of regular programs.

The `triples` code macro above calls two functions: `mapSemi` and `wrapComma` — neither of which are themselves code macros. The first applies the function `triple` to each term in the brace structure that is the source of the triple graph. The definition of `mapSemi` is:

```
fun mapSemi(F,A) is
  leftFold(F,list of [],unwrapSemi(A,list of []))
```

where `unwrapSemi` unfolds the sequence of terms from a `;`-based structure to a list. Notice that `mapSemi` is not itself a code macro: it is simply a regular **Cafe** function; a fact that is reflected in it's type signature:

```
mapSemi has type ((quoted)=>quoted,quoted)=>list of quoted
```

>There are some restrictions — you may not invoke any **Cafe** functions defined in the _same_ package as the code macro itself is; other than functions defined within the same `##` block. You can, however, refer to functions that have been imported from other packages. (This is more difficult to say than to follow.)

The `wrapComma` function is a kind of inverse to `unwrapSemi` except that it takes a collection of terms and forms it into the structure needed for list literals. It's definition is:

```
fun wrapComma(list of [El]) is <| ?El |>
 |  wrapComma(list of [El,..More]) is <| ?El , ?wrapComma(More) |>
```

The definition of `wrapComma` makes good use of **Cafe**'s meta-language: in constructing what will be a `list` literal we have to form it from the appropriate structures. This figure illustrates the AST structure of the contents of a `list` literal:

![Graphical View of a `list` Expression][listAST]

[listAST]:images/listAST.png width=300px

It is useful that, in addition to being used to separate arguments in a list of arguments, the comma is also one of **Cafe**'s standard operators.

The most complex macros in our RDF macro package are those devoted to the processing of triples. The complexity arises from the inherent flexibility of the notation; in particular, any or all of the subject, predicate or object may be individual strings, named concepts or even lists of them.

We use an _accumulator_ style of programming that is similar to what we saw earlier in [our Sieve of Erastosthenes][original-sieve]: the main functions all have a `SoFar` argument that represents the current accumulation.

For example, the function `triple` may result in any number of actual triples being generated; so it must generate a list of triples. It takes the form:

```
fun triple(SoFar,<| ?Sub ! ?VP |>) is 
    SoFar++
    tripleJoin(trNounPhrase(list of [],Sub),
        trVerbPhrase(list of [],VP))
```

Here, `SoFar` refers to the triples found so far. This is appended to the triples gotten by parsing the components of the triple.

>The `++` function is a standard function to concatenate sequences.

The function `trNounPhrase` may find a list of nouns — i.e., a list of concepts — in the subject part of the triple. The `trVerbPhrase` performs a similar function for the predicate and it also invokes `trNounPhrase` to scan for objects:

```
fun trVerbPhrase(SoFar,<| [ ?VPs ] |>) is 
      SoFar++mapComma(trVerbPhrase,VPs) 
 |  trVerbPhrase(SoFar,<| ?V $ ?O |>) is 
      SoFar++pairJoin(trVerb(list of [],V),
        trNounPhrase(list of [],O))
```

`trVerbPhrase` has to 'glue together' an arbitrary collection of objects with the predicate; this is achieved with the `pairJoin` function:

```
fun pairJoin(L1,L2) is list of { (E1,E2) where E1 in L1 and E2 in L2 }
```

This function uses **Cafe**'s query expression notation to make the task simpler.

The triples in a triple graph are put together from the found subjects and predicate/object combinations — also using **Cafe**'s query notation:

```
fun tripleJoin(L1,L2) is
  list of { <| n3Triple(?S,?V,?O) |> where
    S in L1 and (V,O) in L2 }
```

The complete program (which we have not shown here) that implements the transformation of triple graphs is some 50 lines long. It is, in fact, part of **Cafe**'s standard library.

##After the Translation[after-the-translation]

Translating a DSL into core **Cafe** is not always the end of the story. In addition it is important to consider what other ways it should be integrated with the language. For example, we designed a new `condition` so that triple graphs could participate in queries. In addition, there may be one or more standard contracts that should be implemented for elements of the DSL.

Finally, we recall that we fixed on the representation of a triple graph as a `list` of triples, specifically of `n3Triple` terms. However, this was, perhaps not the best ultimate representation — who is to say that a `list` is the best way of representing all graphs? In fact, fixing on `list`s to represent graphs represents a premature commitment: and no single choice is necessarily any better than `list`s.

A better approach is to insulate the architectural choice point by introducing a contract layer. The purpose of the contract is to delay the actual implementation choice to a point where the choice is easier to make. The contract should encapsulate the choice point; in this case we would specify the operations we might expect of a triple graph:

```
contract graphStructure over t is {
  emptyGraph has type t
  addToGraph has type (t,n3Triple)=>t
  findAllSubjects has type (t,n3Concept)=>list of n3Triple
  findAllPredicates has type (t,n3Concept)=>list of n3Triple
  findAllObjects has type (t,n3Concept)=>list of n3Triple
  removeFromGraph has type (t,n3Triple)=>t
}
```

Using such a contract as an underlying specification of requirements for implementation has the merit of allowing evolution in the representation of triple graphs; because, like all languages, we should expect our DSL to evolve too.

###Is There a Downside to DSLs?[is-there-a-downside-to-dsls]

Nothing comes without risks, for every yin there is a yang. What, we should ask, are the risk factors in having a programming language that encourages programmers to create DSLs?

There are three primary areas that we need to point out:

1. Programming DSLs is somewhat more difficult than regular programming. This is certainly true: programming with macros requires a certain facility with meta-language. There is much in common between developing compilers and developing macro packages for a DSL. You have to be able to comprehend how to synthesize the appropriate implementations for your chosen DSL. To be fair, developing a macro package is several orders of magnitude simpler than developing a complete compiler: much of the 'heavy lifting' has been done for you.

1.  Risk of programmer confusion. This is the primary reason that languages like Java and C# do not have macro capabilities: it is easy to develop macro packages that end up causing confusion in the mind of your target audience. Our response to this is that macros are not special in this regard; any time that you build a library intended to be shared by other programmers there is a similar risk of introducing a poorly designed API.

1.  Tooling is harder for languages that support macro extensions. This is a serious issue; especially given the relatively weak technology often given to enable editors to be customized. [^fn2]

[^fn2]:Editor customization is almost universally based on using _regular expressions_ which are often too weak to handle **Cafe**'s syntactic features.
  
	However, the author recommends Emacs and Sublime Text; in part for their excellent customization features.

Overall, as we noted above, it comes down to a choice: of the _curated garden_ or the _wild democracy_. On balance, we believe that the power of being able to craft your own DSL, your own policy framework; or even to be able to rapidly respond to a changing requirement, outweighs the risks associated with being able to define your own macros.
#Chattering Agents[chattering-agents]

Software agents represent one of those concepts that are very compelling -- perhaps precisely because it speaks to natural human intuitions. Agents are also a popular model for distributed systems. Distributed systems are often characterized by multiple loci of control -- typically one per machine in the system -- and assigning an agent to each locus is a very natural architecture.

In this chapter we look at how we can build agent-based systems and hence distributed systems.

>You will notice that the style of this chapter is a little different from previous chapters. This is because many of the concepts we discuss relatively high-level and are not commonly directly grounded in conventional programming paradigms. However, building distributed systems _is_ a programming task; even if many of the issues and concepts don't have much in the way of touch points in language features.

On the other hand, **Cafe** does have some important features that simplify building this style of application: notably the _actor_, _speech actions_ and support for _component architectures_.

##A Taxonomy of Agents[a-taxonomy-of-agents]

There is a natural hierarchy of types of agent -- which roughly aligns with their roles and capabilities in distributed applications:

**Agent**
: An _agent_ is an entity that can act; often on behalf of another entity. By extension, a software agent is one that acts primarily in the software domain.

The difference between an agent and a function is that the latter must be invoked before any actions in it's body can be performed; whereas an agent is 'already' ready to act.

There is a vast potential range in capabilities of agents; for example, at a lower limit, one can argue that a thermostat can be viewed as a very simple agent -- because it represents a localized responsibility for controlling the temperature in a room. Another common, though considerably more complex example, is the web browser: from the perspective of most web servers, browsers are agents that acts in the network for and on behalf of consumers.

>There is even a fair amount of autonomy in browsers: modern browsers have features that attempt to keep their owners safe from phishing attacks and also are able to warn users that certain websites are dangerous to visit.

We have already encountered a **Cafe** feature that can be interpreted as being agent-like -- the `task`. In fact, tasks have quite a few characteristics of autonomy: they can be performed in the background, they can collaborate via rendezvous and they can be used to implement systems involving multiple loci of activity.

However, a more accurate assessment would be that `task`s and the rendezvous are good candidate technologies to _realize_ an agent; just as bits of wire and bimetal can be bent together to make a thermostat. Moreover, **Cafe**'s concurrency concepts are more suited to constructing concurrent algorithms than whole distributed systems. This is because the primary issues that drive the complexity of distributed systems are different to those of concurrent algorithms: in particular, the necessity of handling communication between different computers.

Given the basic concept of agent, one might ask what are the characteristic attributes of an agent. Clearly, the most basic attribute is a capacity for action (this is built in to the definition of agenthood). A corollary of this is some kind of sensing capability -- to determine what action to take (or not).

Once one can sense and act, the next required capability is the capacity to decide to act. This leads us to a hierarchy of introspective capabilities which we call _cognition_:

**Cognitive Agent**
: A software agent that has an explicit representation of its own activities and capabilities.

Typically, this model takes the form of internal data structures that contain representations of some of the data the agent has (beliefs), some of the activities the agent is engaged in (actions) and some of the objectives the agent is pursuing (goals). Cognitive agents may also have explicit structures that describe the capabilities of the agent (plans).

Note that mechanical thermostats cannot be considered to be cognitive: there is no representation within a typical thermostat of its switching capabilities: it simply flips when the temperature moves; it cannot know that that is what it is doing -- much less know about about the heating system it controls. On the other hand, some modern advanced computer-controlled thermostats certainly do understand that they are controlling heating and cooling systems.

Cognitive agent architectures are useful in situations where the activities of the agent are likely to be dynamic; perhaps responding to multiple stimuli, perhaps being able to perform a range of different capabilities.

![Cognitive Agent][cogAgent]

[cogAgent]:images/headnbody.png width=300px

A cognitive system can be interacted with at a higher level too. One can ask such a system if it is busy, for example. 

One systematic way of endowing a software system with such a model is to model the beliefs, goals and ongoing actions as logical structures, perhaps as RDF graphs:

```
...
beliefs has type graph of fact
goals has type graph of goal
actions has type graph of action
capabilities has type graph of plan
...
```

Together, of course, with the semantics that links changes in the real world with changes in these variables and with rules that allow beliefs, goals and actions to be properly integrated.

>We should also emphasize that many software systems already do have some limited cognitive capabilities. For example, a system with a 'plug-in' architecture can be said to have some awareness of its capabilities; similarly, in multi-threaded systems, _task queues_ represent a simple form of self-awareness of current activities.

  However, having a _systematic_ representation of beliefs, desires, intentions and capabilities should simplify and accelerate the development of heterogenous agent-based applications.

While the difference between a 'dumb agent' and a cognitive agent may be one of degree, self-awareness is viewed as _the_ major distinguishing feature of cognitive agents. Self-awareness is especially useful in situations where agents have to deal with complex overlapping scenarios; such as, for example, with human-computer interaction; or when trying to make sense of noisy physical data with multiple potential interpretations.

A self aware agent can even refuse to perform some activity -- one of the more popular definitions of software agents is that they are _autonomous_ systems. This can arise when a cognitive agent can _measure_ the utility value for performing tasks: the basis for refusing to do something would be if the utility is below some
threshold.

>Even for agent systems that do have explicit representations of themselves there is always a large and deep 'sub-conscious' part of the agent -- i.e., that part of the agent that is not modeled within the agent. It is simply not possible for an agent to know everything about itself -- a situation familiar to most human agents.

**Collaborative Agent**
: A collaborative agent is one which participates in a network of agents; and is able to achieve goals by involving other agents in the network. A particularly interesting case of collaboration is where one agent _recruits_ one or more agents to help it achieve it's goals.

Collaborative agents abound in distributed systems; for example, the triumvirate of the browser, the web server and the database server can be viewed as three collaborating agents.

Collaboration is independent of cognition. It is perfectly feasible to design networks of agents that have limited or no self-awareness. Conversely, cognitive agents are not _required_ to participate in a network; although much of the motivation for self-awareness disappears if the agent has no-one to talk to.

The obvious hallmark of collaborative agents is _communication_: you cannot recruit others to do your bidding if you can't talk to them. There are many styles of communication possible between software entities; the one that we suggest for use in **Cafe** agents is based on _speech actions_.

##Speech Actions[speech-actions]

As the term suggests, speech actions are a communications model based on an anthropomorphic understanding of how agents communicate. Speech actions were first investigated by John Austin[][#austin:60] in the 1940's as a vehicle for understanding the role of speech in human society. Since that time the basic ideas have been progressively formalized by John Searle [][#searle:69] and standardized in KQML [][#kqml:93] and FIPA [][#FIPA].

Beyond the anthropomorphism, there are sound technical justifications basing computer communications on speech act theory.

**Speech Action**
: A speech action is a communication action intended to have an effect on the listener.

For example, the classic:

>I pronounce you man and wife

is a speech action. It has an effect; even though no boulders are moved directly as a result of it, it has a substantial impact on the social fabric and therefore can have unbounded consequences in the physical world.

Informally, a speech action can be considered to be a pair -- a _performative_ verb and a _content_ -- which in some formalizations takes the form of a _declarative proposition_. The performative, a communication verb like _inform_, _command_, _query_, or _declare_, indicates how the content is to be interpreted by a listener. This is highly stylized of course, but it represented a huge advance at the time regarding how communication between intelligent entities (people) should be understood.

Another aspect of speech actions is that context is very important. It matters that the agent speaking has the authority to make the pronouncement, and that it is done in the right way and in the appropriate situation. Imagine a real judge saying ''I pronounce you man and wife'' in a theatrical play.

Under normal circumstances, i.e., in the setting where the judge would typically make such a pronouncement, the result of the utterance is a new married couple. However, in this case it would not _count as_ an official action because of the fact it was a line in a play. The person making the declaration is the right person, and has the authority to perform the action but the context is wrong.

>An interesting implication here is that _within the play_ the characters should respond to the matrimonial as though it were real -- if the play is to be believable!

Speech actions are a great basis for expressing the content of communication between software agents -- they permit systematization of communication at a far higher level than purely syntactic structures (such as XML and JSON). That higher level enables us to build a common platform that addresses some key questions:

*  Is the communication valid, authenticated, and authorized?
*  Is any requested action _congruent_ with our own objectives?
*  Can I reliably route the communication to another agent -- in a way   that properly conveys the intention of the communication?

The latter issue relates to the question of _public semantics_. This is often misunderstood, but can be defined:

**Public Semantics**
: A communication has a public semantics if a third party listening to the communication would understand the communication in the same manner as the sender and receiver of the communication.

Note that public semantics does _not require_ a third party listening in to every communication; it only requires the adherence to shared standards when communicating speech actions.

Since we _are_ talking about communication in the context of software, other considerations are also important; in particular, _type safety_, _crossing machine boundaries_ and _efficiency_.

##Programming Speech[programming-speech]

A critical aspect of human communication is the vocabulary employed -- more formally the _ontology_ being referenced. The natural analog of this in software systems is the Application Programming Interface (API). An API specifies which functions you may invoke, what their types are and what the expected results of invoking the functions should be.

>The major semantic difference between an API and an Ontology is that the latter can often convey more semantics. For example, it is possible, in an ontology but not in a typical API, to express the fact that a function called `plus` adds its numbers together.[^There are definite technical limits to this specificity though.]

As it happens, **Cafe** has a natural way of expressing a complete API -- by using the _record type_. For example, the record type defined with the type alias:

```
type myAPI is alias of {
  products has type list of (string,string)
  quantity has type (string)=>integer
}
```

can be viewed as the specification of an API. We are not limited to exposing functions in APIs: we can expose literal values, variables and even types.

Having a type that can represent an entire API also allows us to be careful about distinguishing the API from _access_ to the API. We determine access to an API by computing a value whose type is the API -- for example by having a variable of the right type:

```
A has type myAPI
```

Here the API is determined by the `myAPI` type; access to it is mediated via the variable `A`. We access the API by accessing `A`, as in:

```
show list of { all y where 
    (y,"1in-washer") in A.products and
    A.quantity(y)>20 }
```

While perfectly serviceable, there are some substantial issues with this approach to accessing APIs. For example, we have this `A` variable showing up everywhere; and it is hard -- at first glance -- to see how this style of API can help us with building distributed systems.

###Queries

We have, in **Cafe**, a better, more systematic, approach to describing and implementing public APIs -- based on speech actions.[^And contracts of course.] For example, assuming that `Ag` had the appropriate type similar to that of `A` above, we could issue a speech action against `Ag` with a very similar query:

```
query Ag with list of { all y where 
    (y,"1in-washer") in products and
    quantity(y)>20 }
```

One of the most obvious differences here being where the target of the API is mentioned: it is only mentioned at the top of the query. The type of `Ag`, assuming nothing else is known about it, takes the form:[^This is a simplification of the actual speech contract form.]

```
Ag has type t where speech over t determines myAPI
```

This type annotation highlights two important aspects of speech actions as **Cafe** program fragments: the entity being queried must implement the `speech` contract and that the API of the queried entity is also baked into its type -- albeit via the `speech` contract. Actual concrete implementations of `speech` tend to show the relationship more directly -- as we shall see below when we look at [actors][actors].

As we already noted, in classical speech action theory a speech action is a combination of a performative and content. **Cafe** supports three performatives: `notify` which corresponds to a notification that something has happened, `query` which corresponds to a question, and `request` which corresponds to a request to perform an action. We have found that these three performatives are sufficient to cover the vast majority of communication requirements in practical software systems.

The general form of the `query` speech action is an _expression_ of the form:

`query` _Agent_ `with` _Expression_

where _Agent_ is an entity with a constrained type that implements the `speech` contract.

>The `query` is an expression -- how can it be a speech action? The straightforward response is that it is not the expression that is the action, the speech action as a whole consists of the `query` performative and the content of the action is an expression. Syntactically, because the `query` has a value, it makes the most sense for the `query` to be presented to the programmer as an expression.

####Free Variables in Speech[free-variables-in-speech]

Although the body of a `query` speech action might be any expression, there are some syntactic restrictions on the valid forms of `query` expression. The primary reason for these restrictions is to make it simpler to determine the _scopes_ of identifiers occurring in the query expression. Specifically, we need to be able to determine for any identifier occurring in the queried expression whether or not it refers to the agent being queried or the outer context.

For example, consider the simple `query`:

```
query Ag with quantity("W-S-23")>0
```

In this query the function `quantity` is part of the API of `Ag`; but the function `>` is not -- it is actually a standard function defined as part of the `comparable` contract.

The **Cafe** compiler has to be able to reliably determine the scope of any identifier, including identifiers in embedded query speech actions. We address this in one of two ways: the speech action processor is able to 'understand' sufficient of the standard query notation that it can determine which of the identifiers in the query should be part of the remote agent's API and which should be local. I.e., we can use `query` speech actions like:

```
query Ag with list of { all X where ("W-S-23",X) in products }
```

and the compiler will assume that `products` refers to something that belongs to `Ag`.

The second way in which we can inform the compiler which identifiers belong to the agent is explicitly. For example, we can write:

```
query Ag's quantity with quantity("W-S-23")>0
```

>This is definitely more clumsy than one would like. Sometimes it is not possible to satisfy all requirements in the design of a programming language.

Why, one should ask, cant the compiler simply rely on the type of `Ag`? The answer is two-fold: of course, if `Ag`'s type is known then it will rely on it. However, **Cafe**'s type system is based on type inference -- it is quite possible that the only signal that `Ag` is a speech action correspondent is the presence of this speech action. In that scenario there is not enough information to also determine the types of all the free variables.

###Notifications[notifications]

A `notify` speech action is intended to communicate the occurrence of an event of some form. In the context of software systems it corresponds to a message being sent on a named channel; however that seems low level in comparison.

The form of a `notify` is a little different to the `query`:

`notify` _Agent_ `with` _Expression_ `on` _Channel_

To notify `Ag` that there is a new stock item might take the
form:

```
notify Ag with 
  ("MS-345","3/4in Machine Screw")
  on stock
```

The content here is the tuple term

```
("MS-345","3/4in Machine Screw")
```

and `stock` is the 'channel' of communication. In order for this to be valid we have to assign a new element to `Ag`'s API interface -- one that defines `stock` and the type of event it may respond to:

```
type myAPI is alias of {
  products has type list of (string,string)
  quantity has type (string)=>integer
  stock has type occurrence of (string,string)
}
```

Here we mark the ability to `notify` on the `stock` channel by giving `stock` an `occurrence` type. Later, when we look at how agents can be implemented, we will see how notifications are handled. For now, we note that `occurrence of (string,string)` denotes a program that can consume events whose data consists of values of tuple type `(string,string)`.

####No Time[no-time]

The `notify` speech action does not explicitly refer to time, including the time of the event itself. This is because there may be multiple senses in which time must be conveyed: the time of the occurrence, the time of its being noticed, or the time of this speech action.

Furthermore, not all applications _need_ time to be explicitly identified. An extreme example of this would be the ticking of a clock. Any listener to a mechanical clock's ticking would confirm that neither tick nor tock is timestamped! However, each tick does announce the passing of another second. Instead, it is assumed that the listener has some other way of determining the time (by looking at the clock face). In general, it is expected that each application will incorporate an appropriate model of time for its `notify` events.

###Requests[requests]

Our final form of speech action is the `request`. A `request` is intended to denote a request that the recipient perform some action. This is subtly different to the `query` in that -- apart from answering the question -- a `query` should not cause any change of state in the recipient, whereas the `request` likely would.

>Even though the difference between `query` and `request` may seem subtle to the average programmer the key difference is in the intended use.

The form of a `request` reflects the fact that an action is involved:

`request` _Agent_ `to` _Action_

A simple request may be to invoke a procedure from the API; however complex scripts are also possible. For example, we can request that all stock items that are empty be deleted:

```
request Ag to delete (Id,_) from stocks where quantity(Id)=0
```

The action

```
delete (Id,_) from stocks where quantity(Id)=0
```

is part of the standard CRUD (Create-Read-Update-Delete) feature that allows collections to be updated.

###A Missing Performative

**Cafe** does not have a `declare` performative -- currently. It may be instructive to see why not, especially since we introduced speech action theory with a classic declaration. While it is not likely that a software agent will marry couples any time soon, there are legitimate reasons for wanting the ability to make declarations.

A declaration is a speech action whose effect is embedded within the speech action itself. Declarations establish new facts that are shared by the listener and potentially others in the context. Perhaps the best example comes from transaction processing: declaring that a transaction has been committed to is the same as committing to the transaction. Similarly, declaring that a couple is married is the same as marrying them.

The real reason why there is no `declare` relates to it's putative argument -- which must be a proposition. For example, in signing a contract, each party says to the other:

>I agree to be bound by this contract

where contract refers to the normal human interpretation, not **Cafe**'s contracts. How, we must ask, might we represent this in a way that is amenable to automated processing? To answer this, we must try to unpack what a contract is.

A legal contract has two elements: it is a statement of constraints on potential behaviors of the parties involved and it defines a _value exchange_ (I exchange my money for your house). Very few programming languages, and essentially no 'conventional languages' have any way of representing concepts like value exchange and constraints on behavior.

A more general approach is to use a logical language in which we can encode contracts and the like. At the time of this writing **Cafe** does not have ready access to a well developed logical language (but see [our treatment of RDF][rdf]). As a result, there is no appropriate partner for the `declare` speech action.

Developing a usable logical language is fairly substantial task; however, once it exists, we have exactly the right speech action for it!

##Actors[actors]

So far we have discussed what talkative agents say to each other but not on how they are built. The simplest structure that responds to this is the `actor`. Actors are lightweight entities that can respond to speech actions. For example, this `actor` models some aspects of a bank:

```
agentBank is actor{
  private var accts := dictionary of []
  fun balance(N) where accts[N] matches Ac is Ac.balance()
  prc transfer(F,T,Amt) do{
    def Fr is accts[F]
    def To is accts[T]
    Fr.debit(Amt)
    To.credit(Amt)
  }
}
```

The public elements of the `actor` determine the kinds of speech actions it can respond to. This `actor` can respond to a `balance` query:

```
query agentBank with balance("fred")
```

and can also respond to a `request` to transfer some money between two of its accounts:

```
request agentBank to transfer("fred","tom",100.0)
```

If we wanted our bank to be able to respond to events, such as check posting events then we need to add an occurrence handler for them. Occurrence handlers take the form of an `on...do` rule, such as:

```
on deposit(Nm,Amnt) on cashier do
  accts[Nm].debit(Amnt)
```

There are three parts to an `on...do` rule: the _pattern_ that denotes the kind of events this rule will respond to, a _channel_ identifier and an _action_ that is performed when a suitable event is received. The complete rule is effectively a program that has type: `occurrence of` _type_; or in the case of this rule:

```
cashier has type occurrence of moneyTransaction
```

assuming that `deposit` was a constructor in the type:

```
type moneyTransaction is 
  deposit(string,float) or
  withdraw(string,float)
```

Multiple rules for the same channel may be present, and if two or more rules fire for a given occurrence then they all will be executed -- although the relative order of performing the rules is _not_ defined. This multiple rule firing is useful at times; for example it makes it easier to implement over-arching processing as well as specific processing:

```
on Tx on cashier do
  logMsg(info,"Transaction $Tx")
on deposit(Nm,Amnt) on cashier do
  accts[Nm].debit(Amnt)
```

In this case two actions will take place when a `deposit` is received: it will be logged and the appropriate account will be debited.

>If two occurrence rules fire for a given `notify` the order of their firing is not defined: it may be either order. Therefore, you should make sure that occurrence rules that overlap should not overlap in their actions.

Actors have a type of the form `actor of` _recordType_; where _recordType_ is the actor's API. For example, if we include our occurrence processing rules in our `agentBank`; then its type will be:

```
agentBank has type actor of {
  balance has type (string)=>float
  transfer has type (string,string)=>()
  cashier has type occurrence of moneyTransaction
}
```

One common technique when programming with actors is to use functions that generate actors. One is likely to need only a single bank actor in a system, but one may well need multiple client actors.

###Performance Characteristics of Actors[performance-characteristics-of-actors]

Actors are comparatively efficient at processing speech actions; and they are trusting: that is, they do not perform any validation on the speech actions. One resulting limitation is that they are definitely not safe in a concurrent environment. Again, no interlocking checks are performed -- which means that if you use a regular actor in background tasks (say) then you will likely get inconsistent results.

Also, actors are somewhat _stateful_ in nature. They are intended to encapsulate the processing of speech actions; and that implies that they normally carry some form of state.

A corollary of `actor`s' execution profile is that they are _re-entrant_: multiple tasks can access the same actor concurrently. This can be advantageous in certain circumstances where the actor is actually stateless and performance is critical.

However, in most concurrent situations the normal `actor`'s execution model is too dangerous. To make speech action processing safer it is necessary to _serialize_ access to the actor -- something that is accomplished with concurrent actors.

###Concurrent Actors[concurrent-actors]

A _concurrent actor_ is similar to a regular light weight actor in that you can communicate with a concurrent actor using speech actions and you can define event rules for the concurrent actor.

However, a concurrent actor has an important performance guarantee: only one speech action may be processed concurrently by the actor. This makes it straightforward to ensure that the internal state of a concurrent actor is always consistent in the presence of concurrent access to the actor.

It should be noted that the internal structure of a concurrent actor is more complex than that of a regular light weight actor. This may translate into a run-time performance difference.

A `concurrent actor` is written using the `concurrent` prefix. For example, we can make our `agentBank` concurrent very straightforwardly:

```
agentBank is concurrent actor{
  private var accts := dictionary of []
  fun balance(N) is accts[N].balance()
  prc transfer(F,T,Amt) do{
    accts[F].debit(Amt)
    accts[T].credit(Amt)
  }
  on deposit(Nm,Amnt) on cashier do
    accts[Nm].debit(Amnt)
}
```

As might be clear if you have read up this point, a `concurrent actor` works by having an internal `background task` that is actually responsible for processing speech actions. This background task is responsible for actually responding to speech actions and it 'serializes' them -- ensuring that only one is performed at any one time.

Performing speech actions on concurrent actors is identical to performing them on regular actors. However, concurrent actors have a different type -- `concActor of` _t_ -- which means that one has to be careful when constructing functions that are to work with both kinds of actor. For example, the `balQuery` function:[^Like other references to the `speech` contract, the types in this function are slightly simplified.]

```
balQuery has type for for all t,a such that
    (t,string)=>float where
      speech over t determines a and
      a implements { balance has type (string)=>float }
balQuery(A,U) is query A with balance(U)
```

will work with either of `actor`, `concurrent actor` or any entity that implements `speech` and whose API includes the `balance` function -- because it's type is carefully circumscribed. However, functions that have been type-specialized to work with `actor`s will not type check when used with `concurrent actor`s.

##Boxes and Arrows[boxes-and-arrows]

It is a kind of truism that whenever engineers need to explain their systems to each other they tend to resort to drawing pictures with boxes and arrows between them. For example:

![A System for Splitting Orders][BoxNArrows]

[BoxNArrows]:images/boxNarrows.png width=300px

could be used to explain the system for managing the way parts are ordered in a car factory that supported a 'build-to-order' model for manufacturing customized vehicles.

The different boxes show the major sub-systems and the arrows between them show the major flows between them: we have an input **Orders** sub-system that accepts incoming orders for cars, we have a **Split**ter that breaks up the orders into orders for specific assemblies and parts. We also have a **Parts** database that 'knows' how different order requirements translate into specific assemblies. The outputs of the system are orders for parts that will go to individual suppliers **S**~0~ through **S**~n~.

There have been a number of attempts to take the boxes-and-arrows intuition and turn it into something that is more actionable. Here, we take an approach that focuses on boxes representing _components_ that denote areas of responsibility and where arrows represent interactions between the components.

>I.e., we tighten up on one rather important aspect of designing systems: all interactions between components are explicitly identified.

We also tighten up the model with type correct interfaces and we also find a way for components to talk to each other.

For example, if we zoom in on the **Split** component above, and draw it in isolation we might arrive at:

![The Split Component][SplitComponent]

[SplitComponent]:images/splitComponent.png width=300px

The main new feature in this diagram -- compared to the informal version -- are the different _ports_ that represent the entry and exit points to components. The `SplitComponent` above has three different ports on it: a _responding_ port that handles incoming orders, an _originating_ port that will be connected to a database and a _publishing port_ that will be connected to multiple supplier components. Given this diagramming notation, the order processing system would look something like:

![An Order Processing System][OrderProcessing]

[OrderProcessing]:images/orderProcessing.png width=350px

The main difference between this diagram and the original boxes-and-arrows picture is a slight formalization of the notation: we have crystalized the role of boxes (into processing components) and we have formalized the connections in terms of different kinds of ports.

What has been left out so far is why bother with the formalization? The answer is both surprising and powerful: with some support from **Cafe** we can turn diagrams like these into executable code that can be deployed over distributed systems. The result is a powerful platform that can be a major productivity booster.

###Ports and Speech[ports-and-speech]

First, let us take a slightly deeper look at the anatomy of a port. Ports are intended to represent the points of connectivity of a component: in effect, they form the gateways into and out of the component. By restricting ourselves to components that only interact via their ports we foster re-usability of components and re-purposability (sic) of code.

>This is an important point: unlike most programming languages, and unlike **Cafe** itself, our diagramming notation does not rely on _scoping_. Instead, all connections and references are explicit. This is a strong constraint that helps to enforce so-called _loose coupling_ between components. This greatly simplifies the kinds of task of assembling applications from components.

Ports denote types as well as data flow: each port is associated with an API _schema_ that determines the type of data that is going through the port. For example, the responding port of the `SplitComponent` has a schema associated with it that shows that the component expects `Order`s coming in:

![An Order Port][OrderPort]

[OrderPort]:images/orderPort.png width=350px

The originating port will be connected to a data source component. It's type schema declares what kind of data it is looking for:

![The Parts DB Port][dbPort]

[dbPort]:images/dbPort.png width=350px

It should come as no surprise at this point that we also declare that ports are intimately associated with speech actions: communication between components is mediated by speech actions and ports codify both the sender/receiver relationship and the type of communication.

Notice that we are speaking in terms of _originating_ and _responding_ here. This reflects the fact that in any given speech action we have an originating speaker and a responding listener. We have deliberately avoided using terms such as input or output here because connections between components can involve _both_ flows of information but only one side ever initiates the action. For example, the `SplitComponent` will raise a query to the `PartsDB` component: the query itself involves sending the query in one direction and receiving the data from the `PartsDB` component in the other.

Our final kind of port is a specialization of the originating port: the _publishing port_ as in:

![The Supplier Port][supplierPort]

[supplierPort]:images/supplierPort.png width=360px

The difference between a normal originating port and a publishing port is two-fold: it represents a one-to-many fan-out -- i.e., it may be connected to an arbitrary number of other components -- and it also has an additional feature: the _discrimination function_ that will allow the `SplitComponent` internally to 'select' the right supplier to send parts orders to.

The net effect of this is that we have a diagramming notation that supports a high-level modeling of applications that is intuitive to many software engineers. Furthermore, we can support type safety of communication between elements of the application.

>This notation is not quite a complete 'spanning set' if the end goal is to construct a platform for building a variety of distributed applications. However, it is the kernel of such a set and can easily form the basis of a complete distributed applications platform.

###A Component in Cafe[a-component-in-Cafe]

One of the intentions behind boxes-and-arrows diagrams is to call out the major functional pieces of an application.[^By _functional_ we mean _important to the solution_ not necessarily as in functional programming.] The intuition is that individual components have a specific role in the application; but that they are typically 'quite large'. We have already seen that a component may have multiple ports but we have not exposed what kind of computation may be going on inside.

One of the non-goals of boxes-and-arrows diagrams is to be a complete programming language. Instead, the idea is to capture the large scale granularity in a picture but to use text for the actual programming of components. Actually building components is best left to 'real' code; in our case, we denote the code of a component as a **Cafe** `component`; which is a special form of `package`.

>The one exception to the strategy of using written code to build components is with _composite components_ -- components built by assembling and wiring other components. However, this is outside the scope of this book.

The code for a component must implement the various ports that the component has on the diagram and must also implement the functionality of the component. For example, our `SplitComponent` may start it's implementation with:

```
import boxesNarrows
splitComponent is component{
  port incoming is respond{
    on O on order do
      processOrder(O)
  }
  port suppliers is publish{
    discriminator has type (string) => port of { 
      order has type occurrence of Order
    }
  }
  port parts is originate{
    assembly has type relation of (part,list of part)
    supplier has type relation of (part,list of supplier)
  }
  ...
}
```

The ports that are surfaced in the order processing diagram are also represented in the code for the `splitComponent`. Responding ports have a body that essentially the same as for an actor (or concurrent actor); typically most of the activity within a component is initiated by code that is present in or referred from responding ports.

Notice that, unlike regular actors, outgoing speech actions are also strongly indicated. Any outgoing speech actions must be 'applied' to one of the originating ports. For example, part of processing a parts order will be a query to the `partsDB` component; but the `partsDB` component is not explicitly identified in the `splitComponent`. Instead, the query is directed to the `parts` originating port:

```
fun assemblySuppliers(A) is 
  query parts with list of { unique S where 
      (A,Ps) in assembly and P in Ps and
      (P,Ss) in supplier and
      S in Ss
    }
```

This function can be used by the order processing code to query the parts database for all the suppliers involved in a given assembly. It works by formulating the appropriate `query` speech action to the originating `parts` port.

>The `unique` keyword here implies that the result will have duplicates eliminated from it.

Using a publishing port is slightly more complex than using a regular originating port. Since there may be any number of components attached to a publishing port we must first of all select which one we want to address our speech action to. We do this using the embedded _discriminator function_ that is part of the publishing port. For example, to place a parts order for the supplier "Alpha Wheels Inc."" we perform:

```
prc placeOrder(S,parts) do
  notify suppliers.discriminator(S.name) with 
    order{ content=parts } on order
```

Notice that this is a regular speech action; the primary difference is that instead of a fixed recipient we compute who the recipient will be -- based on the name of the supplier.

###Re-purposing Components[re-purposing-components]

One of the distinguishing features of the boxes-and-arrows diagram is that it highlights the major sources of input and output in the system. This is in marked contrast with most programming languages where the I/O functionality is buried deep within the code itself. For example, our `splitComponent` does not directly communicate with a database; instead it poses a `query` to the `partsDB` component. Similarly, the components for recovering incoming orders and sending out orders to suppliers are mostly about implementing the appropriate I/O operations.

The `partsDB` component is interesting for another reason: it can be modeled as an adapter to a normal database engine. In fact, the `partsDB` is quite an interesting component.

Consider the problem of the `partsDB` component: if we are to build this component in a robust fashion then it must respond to arbitrary queries about the two `relation`s that are exposed in its responding port. However, we assume that the data it uses to answer those queries is not 'in' the component itself but is stored in some actual database; perhaps an SQL database.

In order to see how we can achieve this we need to look a little deeper into how speech actions are actually represented. Recall that we stated that a speech action requires an entity that implements the `speech` contract. That contract, in simplified form, is:

```
contract speech over t determines a is {
  _query has type for all s such that 
    (t,(a)=>s,()=>quoted,()=>dictionary of (string,quoted))=>s
  _request has type 
    (t,(a)=>(),()=>quoted,()=>dictionary of (string,quoted))=>()
  _notify has type (t,(a)=>())=>()
}
```

>The actual contract is somewhat more involved and involves the use of the `execution` contract -- which is part of the support for **Cafe**'s concurrency features. However, this variant is sufficient for us to expose the required issues.

The salient element here is the entry for `_query`. A speech action like:

```
query parts with list of { unique S where 
      (A,Ps) in assembly and P in Ps and
      (P,Ss) in supplier and
      S in Ss
    }
```

is translated -- by a standard built-in macro processor -- into a call to `_query` of the form:

```
_query(parts,
  (Ax)=>list of { unique S where
    (A,Ps) in Ax.assembly and P in Ps and
    (P,Ss) in Ax.supplier and S in Ss },
  ()=><|list of { unique S where 
      (A,Ps) in assembly and P in Ps and
      (P,Ss) in supplier and
      S in Ss
    }|>,
  ()=>dictionary of ["A"->A as quoted]
)
```

This is kind of complicated to follow at first; but is quite straightforward if taken one step at a time.

* The first argument to `_query` is the entity being queried with the `query` speech action. The structure of the `speech` contract allows a responder to a speech action to 'pass on' the action to another responder.

* The second argument is the expression that must be evaluated by the actual responding entity. It is encapsulated in a single argument function -- the argument being a record that implements the speech action API. Evaluating this function in the appropriate context has the effect of computing the response to the `query` speech action -- and the result of the function is returned as the value of the `query` speech action itself.

* The third and final arguments are used when the responder cannot or does not wish to use the 'compiled query'. The third argument is a function that returns the original 'text' of the query -- as a `quoted` value. The fourth argument is also a function, one that returns the values of any _free_ variables in the `query` -- variables whose values are determined by the context of the speech action itself. In this case there is only one free variable -- `A` -- which was the part that our `splitComponent` needed to find suppliers for.

This arrangement allows for two kinds of speech action processing: if the respondent trusts the originator of the speech action then a very rapid response to the `query` is possible -- by evaluating the embedded function. However, in the case where the respondent does not want to simply execute the query function, or it cannot, then the respondent has access to the original text of the query -- together with the values of any free variables that appear in the `query`.

In the case of the `partsDB` component it cannot simply trust the `query`; even if it wanted to. This is because the query must be mapped into a form that the attached SQL database can understand. In effect, the query must be translated from **Cafe**'s query language to SQL.

![The PartsDB Component][partsDB]

[partsDB]:images/dbComponent.png width=330px

I.e., the query expression is translated by the `partsDB` component into SQL:

```
select S from table assembly as As, supplier as Sp, Ps, Ss where
  As.part="Alloy Wheel" and Ps.partno=As.partno and 
  Ps.supplier = Sp.id and
  Ps.supplier=Ss.id and Ss.supplier=S
```

The details of how this translation are achieved are beyond the scope of this book; but it involves similar techniques to those we saw in [our chapter on DSLs][application-policy-mechanism].

One important point to note here is that the machinery for translating queries into SQL is quite general; and not at all restricted to our parts database. In fact, if we provide the actual database component with a URL of the database we are interested in, the DSL processor is able to dynamically inspect the actual database, construct the appropriate responder port for the `partsDB` component as well as being able to answer queries by mapping them to SQL.

As a result, we can construct a general purpose adapter component that is able to be used for _any_ database, not just the one we are using in this application. The result of which is that the application is populated with a mixture of standard components that are configured and specifically written components that implement the specific functionality of the application.

###Wiring up Boxes and Arrows[wiring-up-boxes-and-arrows]

We started this section with a graphical depiction of an application as boxes and arrows between them. However, not many computers can execute boxes, and so if we want to run the application we have to construct a complete _written_ program that represents the _drawn_ diagram.

The written form of our car part sourcing application is not that hard to follow, given the material we have covered so far:

```
import boxesNarrows
partSource is application{
  def ordersIn is import ordersInComponent
  def split is import splitComponent
  def db is import dbComponent
  import supplier

  ordersIn.out connect to split.incoming
  split.parts connect to db.queryIn
  split.suppliers publish to {
    def megaWheel is supplier("MegaWheel")
    ...
    def discriminator("Mega") is megaWheel
    ...
  }
}
```

This sketch shows how we can construct a written version of the boxes-and-arrows diagram in a way that lends itself to executable code.

###There is more to a platform than this

We have actually just scratched the surface of the potential of this kind of programming platform. In truth, like many of the chapters in this book, a full treatment of a boxes-and-arrows platform would justify a book in its own right.

Other aspects that we have omitted include composite components, component templates, wiring diagrams as **Cafe** programs, the dynamic behavior or components and so on.
#Are We Done Yet?[are-we-done-yet]

This book takes you on a journey starting with very simple **Cafe** programs and ending with a platform that is capable of being deployed on large scale distributed networks. In the meantime we also explored some quite deep topics in functional programming, concurrency and developing domain specific languages.

The truth is, of course, that we are only able to scratch the surface of many of the topics we cover. So, the question is what next?

If you want to go deeper into **Cafe** itself then the natural source is the language definition -- which can be obtained at [Github](https://github.com/fmccabe/Cafe/releases/download/v101RC1/reference.pdf). This covers the language and its standard library.

If you want to go deeper in concurrency then there are several sources one may go to. However, we recommend Reppy's [][#Reppy1999] original book on Concurrent ML. Our only complaint about this book is that it is not based on **Cafe**!

There is much material on agent programming of various flavors. A good place to start is also [Wikipedia](http://en.wikipedia.org/wiki/Agent-oriented_programming). For a variety of reasons we also recommend investigating the different specifications produced by the Foundation of Physical Intelligent Agents [FIPA](http://www.fipa.org).

Finally, dear reader, I would like to thank you for staying the course. I would like to recommend that you check out **Cafe** with your own programs and please keep us informed of your adventures by writing: <fmccabe@gmail.com>.

[#austin:60]: John L. Austin, "_How to do things with words_", Oxford : Clarendon, 1962

[#Reppy1999]: John H. Reppy, "_Concurrent Programming in ML_", Cambridge University Press, 1999

[#Bagwell01idealhash]: Phil Bagwell, "_Ideal Hash Trees_". in Es Grands Champs, Vol. 1195, 2001

[#FIPA]: "_The Foundation of Intelligent Physical Agents_" <http://www.fipa.org/index.html>

[#hoare78]: C. A. R. Hoare, "_Communicating Sequential Processes_". in Communications of the ACM, Vol. 21, 8 666-677, 1978

[#hoare85]: C. A. R. Hoare, "_Communicating Sequential Processes_", Prentice Hall, 1985

[#kqml:93]: T. Finin.  et al. "_DRAFT specification of the KQML Agent Communication Language_". The DARPA knowledge sharing initiative External Interfaces Working Group. 1993

[#rfc2396]: Tim Berners Lee. "_RFC2396: Uniform Resource Identifiers (URI): Generic Syntax_" 1998. <https://www.ietf.org/rfc/rfc2396.txt>

[#N3-W3C]: Tim Berners Lee.  and Dan Connolly. "_Notation3 (N3): A readable RDF syntax_" 2011. <http://www.w3.org/TeamSubmission/n3/>

[#searle:69]: J. R. Searle, "_Speech acts: an essay in the philosophy of language_", Cambridge University Press, 1969

