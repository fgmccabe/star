# Functional programming

There is a perception of functional programming that it is *weird* and *difficult*. This is unfortunate for a number of reasons; the most important being that functional programming is *not* weirder than procedural programming and that all programmers can benefit by programming functionally.

As for being difficult, a more accurate description would be that there is a deeper *range of features* in functional programming than in most modern programming languages: so a perception of complexity can arise simply because there is more to say about functional programming languages. However, the simplest aspects of functional programming are very simple and the ramp need not be steep.

What may be surprising to the reader who is not familiar with functional programming is that it is *old*: predating the origins of modern computing itself, that there is a huge amount that can be expressed functionally, and that functional programming is often at least as efficient and sometimes more efficient than procedural programming.

In this chapter we will show how we can utilize **Star** as a vehicle for functional programming. As a side-goal, we also hope to demystify some of the language and ideas found in functional programming.

## What is functional programming?

The foundations of functional programming rest on two principles:

1. Programs are expressed in terms of functions, where a somewhat mathematical view of functions is taken; i.e., functions always produce the same output for the same input.

	This is what people mean when they say that functional programs are *declarative*.[^The term declarative has a technical definition. But this captures much of the essence of declarativeness (sic).]
2. Functions are values: they can be passed as arguments to functions, returned from functions, and they can be put into and retrieved from data structures.

This last principle is what people mean when they refer to functions as being *first class* values. It is also what is meant when we say that functional programming languages are *higher order*.

It is worth asking why these two principles are so important. Although most early programmers were mathematicians; the extreme constraints imposed by early machines meant that one might not have seen much evidence of mathematical thinking in those early programs. However, almost by accident, programming and mathematics share some attributes: the importance of *composition* and the power of *abstraction*. Functional programming is almost ideally placed to exploit both of these to the full.

**Star** is not actually a *pure* functional programming language; i.e., it is possible to write programs that violate the declarative principle. However, it is a *functional-first* programming language: a declarative style is strongly encouraged. In this book we shall mostly focus on writing pure functional programs in **Star**.

## Basics

It is often easier to introduce functional programming using numerical examples. Last chapter we saw, for example, the factorial program. This is mostly because most programmers are already familiar with numbers. Continuing that tradition, here is a function that returns the sign of a number:

```
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) where X>0 => 1.
```
Each of these equations applies to different situations: the first equation applies when the input argument is negative, the second when it is exactly zero and the third when it is strictly positive. These represent the three possible cases in the definition of the sign function.

A **Star** function may be built from any number of rewrite equations; however, they must all be contiguous within the same group of statements.

Although it is good practice to ensure that equations in a function definition do not overlap, **Star** will try the equations in a function definition in the order they are written in.[^The *Church Rosser Theorem* guarantees some independence on the order of the rewrite equations provided that the different rewrite equations that make up function definitions do not overlap. Usually, however, it is too fussy to *require* programmers to ensure that their equations do not overlap; hence the reliance on ordering of equations.] We could have relied on this and written `sign` using:

```
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) => 1.
```

Sometimes it is important to mark a particular equation as the *default* case: i.e., an equation that should be used if none of the other cases apply:

```
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) default => 1.
```
An explicitly marked `default` equation does not need to be the last equation; but, wherever it is written, default equations are only attempted after all other equations have failed to apply.

### Patterns
A function is defined as a sequence of *rewrite equations* each of which consist of a *pattern* and an *expression*. There are three general forms of rewrite equations:

*Pattern* `=>` *Expression*

or

*Pattern* `where` *Condition* `=>` *Expression*

or

*Pattern* `default =>` *Expression*

The left hand side of a rewrite equation consists of a pattern which determines the applicability of the equation; and the right hand side represents the value of the function if the pattern matches.

**Pattern**
: A pattern represents a test or guard on an (implicit) value. Patterns can be said to succeed or fail depending on whether the value being tested matches the pattern.

We also refer to a pattern being *satisfied* when matching a value.[^This terminology originates from Logic -- where a formula can be satisfied (made true) by observations from the world.]

The pattern in a rewrite equation is a guard on the arguments of the function call. For example, given a call
```
sign(34)
```
the patterns in the different equations of the `sign` function will be applied to the integer value `34`.

When the pattern on the left hand side of a rewrite equation succeeds then the equation *fires* and the value of the expression on the right hand side of the equation becomes the value of the function.

There are many kinds of pattern in **Star**; here, we look at three of the most common kinds of pattern, and in later sections, we look at additional forms of patterns.

**Variable Pattern**
: A variable pattern is denoted by an identifier; specifically by the first occurrence of an identifier.

A variable pattern always succeeds and has the additional effect of *binding* the variable to the value being matched.

For example, the `X` in the left hand side of
```
double(X) => X+X
```
is a variable pattern. Binding `X` means that it is available for use in the right hand side of the equation -- here to participate in the expression `X+X`.

The part of the program that a variable has value is called its *scope*.

* Variables in rewrite equations always have scope ranging from the initial occurrence of the variable through to the whole of the right hand side of the equation.
* Variable patterns are the *only* way that a variable can get a value in **Star**.

Subsequent occurrences of variables in a pattern are semantically equivalent to an equality test; specifically a call to the `==` predicate. For example, in the equation:

```
same(X,X) => true.
```
the second occurrence of `X` is regarded as an equality test; i.e., this equation is equivalent to:

```
same(X,X1 where X==X1) => true.
```

Sometimes, the earlier occurrence of a variable is not in the pattern itself but in an outer scope.

**Literal Pattern**
: A literal pattern -- such as a numeric literal or a string literal -- only matches the identical number or string.*

Clearly, a literal match amounts to a comparison of two values: the pattern match succeeds if they are identical and fails otherwise.

Equality is based on *semantic equality* rather than *reference equality*. What this means, for example, is that two strings are equal if they have the same sequence of characters in them, not just if they are the same object in memory.

There is no automatic coercion of values to see if they *might* match. In particular, an integer pattern will only match an integer value and will not match a float value -- even if the numerical values are the same. I.e., there will be no attempt made to coerce either the pattern or the value to fit.

This, too, is based on the desire to avoid hard-to-detect bugs from leaking into a program.

**Guard Pattern**
: Sometimes known as a *semantic guard*, a guard pattern consists of a pair of a pattern and a condition:

*Pattern* `where` *Condition*

Conditions are boolean-valued and the guard succeeds if both the pattern matches and if the condition is *satisfied*. **Star** has a normal complement of special conditional expressions which we shall explore as we encounter the need. In the case of the equation:
```
sign(X) where X>0
```
the guard pattern is equivalent to:
```
X where X>0
```
We can put guard pattern anywhere that a pattern is valid; and, for convenience, we can also put them immediately to the left of the rewrite equation’s is operator.

Notice that any variables that are bound by the pattern part of a guarded pattern are *in scope* in the condition part of the guard.

In the pattern above, the variable X will be bound in the variable pattern X and will then be tested by evaluating the condition X>0.

### Order of evaluation
**Star** is a so-called *strict* language. What that means is that arguments to functions are evaluated prior to calling the function. Most programming languages are strict; for two main reasons:

1. It is significantly easier for programmers to predict the evaluation characteristics of a strict language.
1. It is also easier to implement a strict language efficiently on modern hardware. Suffice it to say that modern hardware was designed for evaluating strict languages, so this argument is somewhat circular.

There many possible styles of evaluation order; one of the great merits of programming declaratively is that the order of evaluation does not affect the actual results of the computation.

It may, however, affect whether you get a result. Different strategies for evaluating expressions can easily lead to differences in which programs terminate and which do not.

One other kind of evaluation that is often considered is *lazy* evaluation. Lazy evaluation means simply that expressions are only evaluated *when needed*. Lazy evaluation has many potential benefits: it certainly enables some very elegant programming techniques.

Essentially for the reasons noted above, **Star** does not use lazy evaluation; however, as we shall see, there are features of **Star** that allow us to recover some of the power of lazy evaluation.[^Even predominantly lazy languages like Haskell have features which implement strict evaluation. It reduces to a question of which is the *default* evaluation style.]

The other dimension in evaluation order relates to the rewrite equations used to define functions. Here, **Star** uses an in-order evaluation strategy: the equations that make up the definition of a function are tried in the order that they are written -- with the one exception being any default equation which is always tried last.

## Another look at types
Organizing data is fundamental to any programming language. **Star**’s data types are organized around the algebraic data type. In addition, **Star**’s supports quantified types of several varieties.

### Quantifier types

A *generic* type is one which has one or more type variables in it. For example, the type expression:
```
(x,x)=>boolean
```
is such a generic type (assuming that `x` is a type variable -- see below).

All type variables must be bound by a quantifier in some enclosing scope. If a type variable is not bound, it is considered *free* in that type expression.

A *quantified type* is a type that introduces (i.e., binds) a type variable. There are two quantifiers in **Star**: a universal quantifier and an existential quantifier.

The most common quantifier is the *universal quantifier* and universally quantified types correspond closely to generic types in other languages.

Universally quantified types are often used to denote function types and collection types. For example, the type
```
all x ~~ (x,x)=>boolean
```
denotes the type of a generic binary function that returns a boolean value. The standard type for the equality predicate `==` is similar to this type.

A universally quantified type should be read as ‘for all possible values’ of the bound variable. For example, this function type should be read as denoting functions that:

>for any possible type -- `x` -- the function takes two such `x`’s and returns a `boolean`.

**Star** also supports *existentially* quantified types -- these are useful for denoting the types of modules and/or abstract data types. However, we will leave our exploration of existential types for later.

### Contract constrained types
We noted above that the type of the standard equality predicate was ‘almost’ the same as:
```
all x ~~ (x,x)=>boolean
```
This type denotes a universally quantified function type that can be applied to arguments of any given type. However, equality in a normal programming language is *not* universal: not all values admit to being reliably tested for equality. A great example of such a limitation are functions -- equality between functions is not well defined.[^Strictly, not decidable.]

To capture this, we need to be able to constrain the scope of the quantifier; specifically to those argument types that do admit equality.

We can do this by adding a *contract constraint* to the type -- the constraint states that equality must be defined for the type. We do this by prepending a constraint clause to the type:
```
all x ~~ equality[x] |: (x,x)=>boolean
```
The `equality[x] |:` clause states that the type variable x must satisfy the equality contract.

What is implied when we state this? This is captured in the definition of the contract itself, in this case:
```
contract all t ~~ equality[t] ::= {
  (==) : (t,t) => boolean.
}
```
In effect, the `equality` contract states that there must be a single function defined -- `==` .

If this seems a little circular, it is not. The equality contract is effectively saying that the `==` function must be defined for the type; and we constrain function (and other) types with the equality contract constraint when we need to ensure that `==` is defined!

We provide evidence for contracts through the implementation statement. This declares that a given type satisfies a contract by providing implementations for the functions in the contract.

For example, we can provide evidence that the `equality` contract applies to strings using a built-in primitive to actually implement equality for strings:
```
implementation equality[string] => {
  s1 == s2 => _string_eq(s1,s2).
}
```
We shall explore more fully the power of this form of type constraint in later sections and chapters. For now, the core concept is that quantified types can be constrained to allow very precise formulations of types.

### Algebraic data types
An algebraic data type definition achieves several things simultaneously: it introduces a new type into scope, it gives an enumeration of the legal values of the new type and it defines both constructors for the values and it defines patterns for decomposing values. This is a lot for a single statement to do!

For example, we can define a type that denotes a point in a two-dimensional space:
```
point ::= cart(float,float).
```
This kind of statement is called a *type definition statement* and is legal in the same places that a function definition is legal.

The new type that is named by this statement is point; so, a variable may have point type, we can pass point values in functions and so on.

The constructor cart allows us to have expression that allow ‘new’ point structures to be made:
```
cart(3.4,2.1)
```
`cart` is also the name of a *pattern* operator that we can use to take apart point values. For example, the euclid function computes the Euclidian distance associated with a point:
```
euclid:(point)=>float.
euclid(cart(X,Y)) => sqrt(X*X+Y*Y).
```
Of course, this particular `point` type is based on the assumption that point values are represented in a cartesian coordinate system. One of the more powerful aspects of algebraic data types is that it is easy to introduce multiple alternate forms of data. For example, we might want to support two forms of point: in cartesian coordinates and in polar coordinates. We can do this by introducing another case in the type definition statement:
```
point ::= cart(float,float)
        | polar(float,float).
```
Of course, our `euclid` function also needs updating with the new case:
```
euclid:(point)=>float.
euclid(cart(X,Y)) => sqrt(X*X+Y*Y).
euclid(polar(R,T)) => R.
```
cart and polar are called *constructor functions*. The term *constructor* refers to the common programming concept of constructing data structures. They are called functions because, logically, they *are* functions.

For example, we might give a type to `polar`:
```
polar : (float,float)=>point
```
In fact, constructor functions are one-to-one functions. Variously known as *free functions* (in logic), *bijections* (in Math), one-to-one functions are guaranteed to have an inverse. This is the logical property that makes constructor functions useful for representing data.

Of course, we are talking of a *logical* property of constructor functions. Internally, when implementing functional languages like **Star**, the values returned by constructor functions are represented using data laid out in memory -- just like in any other programming language.

**Star** actually employs a special type for constructor functions; so the correct type of polar is given by:
```
polar : (float,float)<=>point
```
The double arrow representing the fact that constructor functions are bijections.

In addition to constructor functions, an algebraic type definition can introduce two other forms of data: *enumerated symbols* and *record functions*. Enumerated symbols are quite useful in representing symbolic alternatives. The classic example of an enumerated type is daysOfWeek:
```
daysOfWeek ::= monday
             | tuesday
             | wednesday
             | thursday
             | friday
             | saturday
             | sunday.
```
Another example is the standard `boolean` type which is defined:
```
boolean ::= true | false.
```
Unlike enumerated symbols in some languages, there is no numeric value associated with an enumeration symbol: an enumerated symbol ‘stands for’ itself only. The reason for this will become clear in our next type definition which mixes enumerated symbols with constructor functions:
```
sTree ::= sEmpty | sNode(sTree,string,sTree)
```
In addition to mixing the enumerated symbol (`sEmpty`) with the `sNode` constructor, this type is *recursive*: in fact, this is a classic binary tree type where the labels of the non-empty nodes are strings. (We shall see shortly how to generalize this).

Whenever you have a recursive type, its definition must always include one or more cases that are not recursive and which can form the base case(s). In that sense, an enumerated symbol like `sEmpty` plays a similar role in **Star** as null does in other languages; except that `sEmpty` is only associated with the sTree type.

We can use `sTree` to construct binary trees of string value; for example:
```
sNode(sNode(sEmpty,"alpha",sEmpty),
      "beta",
      sNode(sEmpty,"gamma",sEmpty))
```
denotes the tree in:

![A Binary string Tree][images/sTree.jpg]


One of the hallmarks of languages like **Star** is that *every* value has a legal syntax: it is possible to construct an expression that denotes a literal value of any type.

Just as we can define sTree values, so we can also define functions over sTrees. For example, the check function returns true if a given tree contains a particular search term:
```
check(sEmpty,_) => false.
check(sNode(L,Lb,R),S) => Lb==S || check(L,S) || check(R,S)
```
Here we see several new aspects of **Star** syntax:

* An empty pattern -- marked by `_` -- matches anything. It is called the *anonymous pattern* and is used whenever we don’t care about the actual content of the data.
* The `||` disjunction is a *short-circuit* disjunction; much like `||` in languages like Java. Similarly, conjunction (`&&`) is also short-circuiting.
* Functions can be recursive. **Star** permits *mutual recursion* just as easily: there is no special requirement to order function definitions in a program.

We can use `sTest` to check for the occurrence of particular strings:
```
T = sNode(sNode(sEmpty,"alpha",sEmpty),
          "beta",
          sNode(sEmpty,"gamma",sEmpty)).

show check(T,"alpha").          -- results in true
show check(T,"delta").          -- results in false
```

## Functions as values
The second principle of functional programming is that functions are first class. What that means is that we can have functions that are bound to variables, passed into functions and returned as the values of functions. In effect, a function is a legal *expression* in the language. It also means that we can have *function types* in addition to having types about data.

We can see this best by looking at a few examples. One of the benefits of passing functions as arguments to other functions is that it makes certain kinds of parameterization easy. For example, suppose that you wanted to generalize check to apply an arbitrary test to each node -- rather than just looking for a particular string.

We will first of all define our `fTest` function itself:
```
fTest:(sTree,(string)=>boolean)=>boolean.
fTest(sEmpty,_) => false.
fTest(sNode(L,Lb,R),F) => F(Lb) || fTest(L,F) || fTest(R,F).
```
The substantial change here is that, rather than passing a string to look for, we pass `fTest` a boolean-valued function to apply; within `fTest` we replace the equality test `Lb==S` with a call `F(Lb)`.

Notice that the type annotation for `fTest` shows that the type of the second argument is a function type -- from `string` to `boolean`.

Given `fTest`, we can redefine our earlier check function with:
```
check(T,S) => fTest(T,(X)=>X==S)
```
We have a new form of expression here: the *anonymous function* or *lambda expression*. The expression
```
(X)=>X==S
```
denotes a function of one argument, which returns `true` if its argument is the same value as `S`.

Interestingly, it would be difficult to define a top-level function that is equivalent to this lambda because of the occurrence of the variable `S` in the body of the lambda. This is an example of a *free variable*: a variable that is mentioned in the body of a function but which is defined outside the function. Because `S` is free, because it is not mentioned in the arguments, one cannot simply have a function which is equivalent to the lambda as a top-level function.

Free variables are a powerful feature of functional programming languages because they have an *encapsulating* effect:  in this case the lambda encapsulates the free variable so that the `fTest` function does not need knowledge of `S`.

### Functions and closures
If a function is an expression, what is the value of the function expression? The conventional name for this value is *closure*:

**Closure**
: A closure is a structure that is the value of a function expression and which may be applied to arguments.

It is important to note that, as a programmer, you will never ‘see’ a closure in your program. It is an implementation artifact in the same way that the representation of floating point numbers is an implementation artifact that allow computers to represent fractional numbers but which programmers (almost) never see explicitly in programs.

Pragmatically, one of the important roles of closures is to capture any free variables that occur in the function. Most functional programming languages implement functions using closure structures. Most functional programming languages (including **Star**) do not permit direct manipulation of the closure structure: the only thing that you can *do* with a closure structure is to use it as a function.

In the world of programming languages, there is a lot of confusion about closures. Sometimes you will see a closure referring to a function that captures one or more free variables.

### Let binding

We noted that it is difficult to achieve the effect of the `(X)=>X==S` lambda expression with named functions. The reason is that the lambda is *not* defined in the same way that named functions are defined -- because it occurs as an expression not as a statement. If we wanted to define a named function which also captures S, we would have to be able to define functions inside expressions.

There is an expression that allows us to do this: the let expression. A let expression allows us to introduce local definitions anywhere within an expression. We can define our lambda as the named function isS using the let expression:
```
let{
  isS(X) => X==S
} in isS
```
The region between the braces is a *definition environment* and **Star** allows *any* definition statement to be in such an environment. We can define check using a let expression:
```
check(T,S) => fTest(T,
    let{
      isS(X) => X==S.
    } in isS)
```
This is a somewhat long-winded way of achieving what we did with the anonymous lambda function -- we would not normally recommend this way of writing the check function as it is significantly more complicated than our earlier version. However, there is a strong inter-relationship between anonymous lambdas, let expressions and variable definitions. In particular, these are equivalent:
```
let{
  isS = (X) => X==S
} in isS
```
and
```
(X)=>X==S
```
Apart from being long-winded, the `let` expression is significantly more flexible than a simple lambda. It is much easier within a let expression to define functions with more than one rewrite equation; or to define multiple functions. We can even define local types within the let binding environment.

Conversely, lambda functions are so compact because they have strong limitations: you cannot easily define a multi-rewrite equation function with a lambda and you cannot easily define a recursive function as a lambda.

In short, we would use a `let` expression when the function being defined is at all complex; and we would use a lambda when the function being defined is simple and small.

Assembling functions in this way, either by using anonymous lambdas or by using let expressions, is one of the hallmarks of functional programming.

## Generic programs
Given this definition of the tree type, we can construct a more general form of the tree test function; which is almost identical to fTest:[^This time, we **must** use an explicit type annotation.]
```
test:all t ~~ (tree[t],(t)=>boolean)=>boolean.
test(tEmpty,_) => false.
test(tNode(L,Lb,R),F) => F(Lb) || test(L,F) || test(R,F).
```
and our original string check function becomes:
```
check(T,S) => test(T,(X) => X==S)
```
The type of `check` is also more generic:
```
check:all t ~~ (tree[t],t)=>boolean
```
I.e., check can be used to find any type of element in a tree -- providing that the types align of course.

Actually, this is not the correct the type for check. This is because we do not, in general, know that the type can support equality. The precise type for check should take this into account:

check:all t ~~ equality[t] |: (tree[t],t) => boolean


### Generic types

What actually makes `fTest` more constrained than it could be is the type definition of `sTree` itself. It too is unnecessarily restrictive: why not allow trees of any type? We can, using the type definition for tree:
```
all t ~~ tree[t] ::= tEmpty | tNode(tree[t],t,tree[t]).
```
Like the original `sTree` type definition, this statement introduces a new type: `tree[t]` which can be read as ‘tree of something’. The name `tree` is not actually a type identifier -- although we often refer to the tree type -- but it is a *type constructor*.

In an analogous fashion to constructor functions, a type constructor constructs types from other types. Type constructors are even bijections -- one-to-one functions from types to types.

The identifier `t` in the type definition for tree denotes a *type variable*. Again, similarly to regular variables and parameters, a type variable denotes a single unspecified type. The role of the type variable t is like a parameter in a function: it identifies the unknown type and its role.

The tree type is a universally quantified type. What that means is that instead of defining a single type it defines a family of related types: for example:
```
tree[string]
tree[integer]
...
```

are `tree` types. We can even have trees of trees:
```
tree[tree[string]]
```
We capture this genericity of the tree type by using a *universal quantifier*:
```
all t ~~ tree[t]
```
What this type expression denotes is a set of possible types: for any type t, tree[t] is also a type. There are infinitely many such types of course.

The all quantifier is important: as in logic, there are two kinds of quantifiers in **Star**’s type system: the *universal* quantifier all and the *existential* quantifier exists. We will take a deeper look at the latter in a later chapter when we look deeper at modular programming.

**Star** uses context to determine whether an identifier is a type variable or a type name. Specifically, if an identifier is bound by a quantifier then it must refer to a type variable.

The types of the two constructors introduced in the `tree` type definition are similarly quantified:
```
tEmpty:all t ~~ tree[t].

tNode:all t ~~ (tree[t],t,tree[t)]<=>tree[t].
```
The type `tree[t]` on the right hand side of `tEmpty`’s type annotation raises a couple of interesting points:

1. This looks like a type annotation with no associated definition. The fact that the `tEmpty` symbol was originally introduced in a type definition is enough of a signal for the compiler to avoid looking for a definition for the name.
2. The type of a literal `tEmpty` expression -- assuming that no further information is available -- will be of the form `tree[t34]` where `t34` is a ‘new’ type variable not occurring anywhere else in the program. In effect, the type of `tEmpty` is tree of *some type* `t34` where we don’t know anything more about `t34`.

### Generic functions
Given this definition of the tree type, we can construct a more general form of the tree test function; which is almost identical to fTest:[^This time too, we must use an explicit type annotation.]
```
test:all t ~~ (tree[t],(t)=>boolean)=>boolean.
test(tEmpty,_) => false.
test(tNode(L,Lb,R),F) => F(Lb) || test(L,F) || test(R,F).
```
and our original string check function becomes:
```
check(T,S) => test(T,(X) => X==S)
```
The type of check is also more generic:
```
check:all t ~~ (tree[t],t)=>boolean
```
I.e., check can be used to find any type of element in a tree -- providing that the types align of course.

Actually, this is not the correct the type for check. This is because we do not, in general, know that the type can support equality. The precise type for check should take this into account:
```
check:all t ~~ equality[t] |: (tree[t],t) => boolean
```

## Going further
Although better than the original sTest program there is still one major sense in which the test program is not general enough. We can see this by looking at another example: a function that counts elements in the tree:
```
count:all t ~~ (tree[t]) => integer.
count(tEmpty) => 0.
count(tNode(L,_,R)) => count(L)+count(R)+1
```
This code is very similar, but not identical, to the `test` function.

The issue is that test is trying to do two things simultaneously: in order to apply its test predicate to a binary tree it has to implement a walk over the tree, and it also encodes the fact that the function we are computing over the tree is a boolean-value function.

We often need to do all kinds of things to our data structures and writing this kind of recursion over and over again is tedious and error prone. What we would like to do is to write a single *visitor* function and specialize it appropriately when we want to perform a specific function.

This principle of separating out the different aspects of a system is one of the core foundations of good software engineering. It usually goes under the label *separation of concerns*. One of the beautiful things about functional programming is that it directly supports such good architectural practices.

Since this visitor may be asked to perform any kind of computation on the labels in the tree we will need to slightly generalize the type of function that is passed to the visitor. Specifically, the type of function should look like:
```
F : (a,t)=>a
```
where the a input represents accumulated state, `t` represents an element of the tree and the result is another accumulation.

Using this, we can write a `tVisit` function that implements tree walking as:
```
tVisit:all a,t ~~ (tree[t],(a,t)=>a,a)=>a.
tVisit(tEmpty,_,A) => A.
tVisit(tNode(L,Lb,R),F,A) => tVisit(R,F,F(tVisit(L,F,A),Lb)).
```
Just as the accumulating function acquires a new ‘state’ parameter, so the `tVisit` function also does. The `A` parameter in the two equations represents this accumulated state.

The second rewrite equation for `tVisit` is a bit dense so let us open it out and look more closely. A more expanded way of writing the tVisit function would be:
```
tVisit(tEmpty,_,A) => A.
tVisit(tNode(L,Lb,R),F,A) => let{
  A1 = tVisit(L,F,A).
  A2 = F(A1,Lb).
  } in tVisit(R,F,A2)
```
where `A1` and `A2` are two local variables that represent the result of visiting the left sub-tree and applying the accumulator function respectively. We have used the let expression form to make the program more obvious, rather than to introduce new functions locally; but this is a legitimate role for let expressions.

The `tVisit` function knows almost nothing about the computation being performed, all it knows about is how to walk the tree and it knows to apply functions to labels in the tree.

Given tVisit, we can implement our original check and count functions as one-liners:
```
check(T,S) => tVisit(T,(A,X)=>(A || X==S),false).
count(T) => tVisit(T,(A,X)=>A+1,0).
```
The lambda that is embedded in the definition of check bears a little closer scrutiny:
```
(A,X)=>(A || X==S)
```
In this lambda, we return `A` -- if it is true -- or we return the result of the test `X==S`. This is a common pattern in such programs: the accumulator `A` acts as a kind of state parameter that keeps track of whether we have already found the value.

Functional programs are not actually *state-free*; often quite the opposite. However, the state in a functional program is never *hidden*. This is the true distinction between functional and regular procedural programs.

Notice that we have effectively hidden the recursion in the function definitions of check and count -- all the recursion is encapsulated within the `tVisit` function. One of the unofficial mantras of functional programming is *hide the recursion*.

The reason we want to hide recursions that this allows the designer of functions to focus on *what* is being computed rather than focusing on the structure of the data and, furthermore, this allows the implementation of the visitor to be *shared* by all users of the tree type.

Notice that, while a and t are type variables, we did not put an explicit quantifier on the type of F. This is because the quantifier is actually put on the type of tVisit instead:
```
tVisit:all a,t ~~ (tree[t],(a,t)=>a,a)=>a
```
Just like regular variables, type variables have scope and points of introduction. Also like regular variables, a type variable may be *free* in a given type expression; although it must ultimately be *bound* by a quantifier.

### Going even further
We have focused so far on generalizing the visitor from the perspective of the tree type. But there is another sense in which we are still *architecturally entangled*: from the perspective of the check and count functions themselves.

In short, they are both tied to our tree type. However, there are many possible collection data types; **Star** for instance has some 5 or 6 different standard collection types. We would prefer not to have to re-implement the check and count functions for each type.

The good news is that, using contracts, we can write a single definition of check and count that will work for a range of collection types.

Let us start by defining a contract that encapsulates what it means to visit a collection:
```
contract visitor[c->>t] ::= {
  visit:all a ~~ (c,(a,t)=>a,a)=>a
}
```
This `visitor` contract defines a single function that embodies what it means to *visit* a collection structure. There are quite a few pieces here, and it is worth examining them carefully.

A contract header has a template that defines a form of *contract constraint*. The clause
```
visitor[c ->> t]
```
is such a constraint. The sub-clause
```
c ->> t
```
refers to two types: `c` and `t`. The presence of the `->>` term identifies the fact that `t` _depends_ on `c`.

The `visitor` contract itself is about the collection type `c`. But, within the contract, we need to refer to both the collection type and to the type of elements in the collection: the visit function is over the collection, it applies a function to elements of the collection.

Furthermore, as we design the contract, we *do not know* the exact relationship between the collection type and the element type. For example, the collection type may be generic in one argument type -- in which case the element type is likely that argument type; conversely, if the type is *not* generic (like `string` say), then we have no direct handle on the element type.

We *do know* that within the contract the element type is *functionally determined* by the collection type: if you know the collection type then you should be able to figure out the element type.

We express this dependency relationship with the the `c ->> t` form: whatever type `c` is, `t` must be based on it.

The body of the contract contains a single type annotation:
```
visit:all a ~~(c,(a,t)=>a,a)=>a
```
This type annotation has three type variables: the types `c` and `t` come from the contract header and `a` is local to the signature. What the signature means is

>Given the visitor contract, the visit function is from the collection type c, a function argument and an initial state and returns a new accumulation state.

It is worth comparing the type of `visit` with the type of `tVisit`:
```
tVisit:all t,a ~~(tree[t],(a,t)=>a,a)=>a
```
The most significant difference here is that in tVisit the type of the first argument is fixed to `tree[t]` whereas in visit it is left simply as `c` (our collection type).

Given this contract, we can re-implement our two check and count functions even more succinctly:
```
check(T,S) => visit(T, (A,X)=>A || X==S,false)
count(T) => visit(T, (A,X)=>A+1,0)
```
These functions will apply to *any* type that satisfies -- or implements -- the `visitor` contract. This is made visible in the revised type signatures for count:
```
count:all c,t ~~ visitor[c->>t] |: (c)=>integer
```
This type is an example of a *constrained type*. It is generic in `c` and `t` but that generality is constrained by the requirement that the `visitor` contract is appropriately implemented. The eagled-eyed reader will notice that `count` does not actually depend on the type of the elements in the collection: this is what we should expect since `count` does not actually care about the elements themselves.

The type signature for `check`, however, does care about the types of the elements:
```
check:all c,t ~~
  visitor[c->>t], equality[c] |: (c,t)=>boolean
```
This type annotation now has two contract constraints associated with it: the collection must be something that is visitable and the elements of the collection must support equality.

Given the work we have done, we can implement the visitor contract for our `tree[t]` type quite straightforwardly:
```
implementation all t ~~ visitor[tree[t]->>t] => {
  visit = tVisit
}
```
Notice that header of the implementation statement provides the connection between the collection type (which is `tree[t]`) with the element type (`t`). The clause
```
visitor[tree[t]->>t]
```
is effectively a declaration of that connection.

Now that we have disconnected `visit` from `tree` types, we can extend our program by implementing it for other types. In particular, we could also implement the visitor for the `sTree` type:
```
implementation visitor[Tree ->> string] => {
  visit = sVisit
}
```
however, we leave the definition of `sVisit` as a simple exercise for the reader.

Our final versions of count and check are now quite general: they rely on a generic implementation of the visit function to hide the recursion and are effectively independent of the actual collection types involved.

If we take a second look at our visitor contract we can see something quite remarkable: it counts as a definition of the famous *visitor pattern*. This is remarkable because although visitor patterns are a common design pattern in OO languages, it is often hard in those languages to be crisp about them; in fact, they are called patterns because they represent patterns of use which may be encoded in Java (say) whilst not necessarily being definable in them.

The combination of contract and implementation represents a quite formal way of defining patterns like the visitor pattern.

There is something else here that is quite important too: we are able to define and implement the visitor contract *without* having to modify in any way the type definition of tree or sTree. From a software engineering point of view this is quite important: we are able to gain all the benefits of interfaces without needing to entangle them with our types. This becomes critical in situations where we are not able to modify types -- because they don’t belong to us and/or we don’t have access to the source.

### Polymorphic arithmetic
There are other ways in which programs can be polymorphic. In particular, let us focus for a while on arithmetic. One of the issues in arithmetic functions is that there are many different kinds of numbers. Pretty much every programming language distinguishes several kinds of numbers; for example, Java distinguishes byte, short, int, long, float, double, BigInteger and BigDecimal -- and this does not count the wrapped versions. Other languages have even more choice.

One question that might seem relevant is why? The basic answer is that different applications call for different properties of numbers and no one numeric type seems to fit all needs. However, that variety comes at a cost: when we use numbers we tend to have to make too early a choice for the numeric type.

For example, consider the double function we saw earlier:
```
double(X) => X+X
```
What type should double have? In particular, what should the type of `+` be? Most people would be reluctant to use different arithmetic operators for different types of numbers.[^Although some languages -- such as ML -- do require this.] This is resolved in **Star** by relying on contracts for the arithmetic operations.

The result is that the most appropriate type signature for double is exquisitely tuned:
```
double:all t ~~ arith[t] |: (t)=>t
```
This type is precisely the minimal type that double could have. Any further constraints result in making a potentially premature choice for the numeric type.

If we take another look at our original fact function:
```
fact(0) => 1
fact(N) => N*fact(N-1)
```
this is constrained to be a function from `integer` to `integer` because we introduced the literal integers `0` and `1`. However, the `arith` contract contains synonyms for these very common literals. Using `zero` and `one` allow us to be abstract in many arithmetic functions:
```
genFact:all a ~~ arith[a] |: (a)=>a.
genFact(zero) => one.
genFact(N) => N*genFact(N-one).
```
We call out `zero` and one for special treatment because they occur very frequently in numerical functions. We can introduce other numeric literals without compromising our type by using *coercion*; although it is more clumsy:
```
factorialC:all t ~~
  arith[t],coercion[integer,t] |: (t)=>t.
factorialC(N) where N==0::t => 1::t.
factorialC(N) => N*factorialC(N-1::t).
```
The expressions `0::t` and `1::t` are coercions from `integer` to `t`.

Of course, coercion is also governed by contract, a fact represented in the type signature by the coercion contract constraints on the type of `t`.

In any case, using these techniques, it is possible to write numeric functions without unnecessarily committing to specific number types. That in turn helps to make them more useful.

### Optional computing
There are many situations where it is not possible to guarantee that a computation will succeed. The simplest examples of this include scenarios such as accessing external files; but may also apply to getting the first element of a list or the label of a `tree` node. The great unknown of accessing elements of a collection is ‘is it there?’. Its not guaranteed of course, and we need to be able to handle failure.

Many languages employ the concept of a special `null` value to denote some of these cases -- like a `someOne` not having a `spouse`. However, the special `null` value brings its own problems: the type of `null` is problematic (it is a legal value for every type) and there are many situations where `null` is never possible.

We address this by handling those situations where failure is possible differently than where it is not. Specifically, we do this via the `option` type.

The type definition of `option` is quite straightforward:
```
all t ~~ option[t] ::= none | some(t).
```
where `none` is intended to denote the non-existence of a value and `some` denotes an actual value.

The `option` type is intended to be used in cases where functions are known to be partial.[^A partial function does not have a value across the whole range of its arguments.] An `option` return type signals that the function may not always have a value.

Normal pattern matching can be used to access a value wrapped in a `some`; for example, to access someone's `spouse` we can use the condition:
```
isMarriedTo(P,J) where some(JJ).=P.spouse => J==JJ.
isMarriedTo(_,_) default => false.
```
The important detail here is that all access to a `option` wrapped value is gated by some form of pattern matching and that, normally, this takes place in a condition.

#### Special syntax for `option`al values
Of course, the code above _is_ kind of clumsy! There is a range of operators in **Star** to make using `option` values more pleasant.

The most important of these is the `^=` operator which combines the `.=` with the `some` match. Using this, the `isMarriedTo` function becomes:
```
isMarriedTo(P,J) where JJ^=P.spouse => J==JJ.
isMarriedTo(_,_) default => false.
```
The meaning of `^=` is similar to the pattern match condition `.=`; except that the pattern is assumed to be for a `some` value.

While the `^=` operator[^Read as ‘has a value’.] is very useful in unpacking an optional value, the `^|` operator allows us to handle cases where we always need to be able to give some kind of value. For example, normally a `map` returns none if an entry is not present. However, a *cache* is structured differently: if a value is not present in a cache then we must go fetch it:

```
cacheValue(K) => cache[K] ^| fetch(K)
```

We can also apply a match _in line_ to an `option`al value. The `^` operator allows a pattern to be formed by applying a `option` valued function directly in place. For example, the equation:
```
head(first^(1)) => "alpha"
```
is equivalent to:
```
head(X) where some(1).=first(X) => "alpha"
```
We will see more examples of this when we look more closely at sequences and collections processing.

Overall, the `option` type is part of an elegant approach to nullability that is easily incorporated into **Star**’s (and similar) type system.

## A word about type inference
We have seen some powerful forms of types in this chapter: recursive types defined using algebraic type definitions, generic types and even function types. Recall also that **Star** only requires programmers to explicitly declare the types of quantified variables and functions. It is worth pausing a little to see how this might be done.

Recall our original fact function:
```
fact(0) => 1.
fact(N) => N*fact(N-1).
```
The compiler is able to compute the types of the various variables automatically through a process known as *type inference*. Type inference may seem magical, but is actually (mostly) quite simple. Let us take a look at the expression:
```
N-1
```
which is buried within the recursive call in fact. Although it looks like a special operator, **Star** does not treat arithmetic expressions in a special way; the - function is just a function from numbers to numbers; its type is given by:[^We put the (-) in parentheses to highlight the use of an operator as a normal symbol.]
```
(-) : all t ~~ arith[t] |: (t,t)=>t
```
However, we should simplify this type a little in order to make the explanation of type inference a little simpler. In what follows, we assume that the type of (`-`) is:
```
(-) : (integer,integer)=>integer
```
Type inference proceeds by using special *type inference rules* which relate expressions to types, in this case the applicable rule is that a function application is consistent if the function’s parameter types are consistent with the types of the actual arguments. If they are consistent, then the type of the function application is the return type of the function.

The type inference process initially gives every variable an unknown type -- represented by a new type variable not appearing anywhere else. For our tiny `N-1` example, we will give N the type t~N~.

The (`-`) function has two arguments whose types can be expressed as a tuple of types:
```
(integer,integer)
```
and the types of the actual arguments are also a tuple:
```
(t~N~,integer)
```
In order for the expression to be type correct, the actual types of the arguments must be consistent with the expected types of the function; which we can do by making them *the same*. There is a particular process used to do this -- called *unification*.

![Inferring the Type of N-1][images/minustype]

**Unification**
: An algorithm that replaces variables with values in such a way as to make two terms identical.

Unification matches equals with equals and handles (type) variables by substitutions -- for example, we can make these two type expressions equal by *binding* the type variable tN to integer.

We initially picked the type of N to be an arbitrary type variable, but the process of checking consistency leads us to refine this and make the type concrete. I.e., the use of N in a context where an integer is expected is enough to allow the compiler to infer that the type of N is indeed integer and not t~N~.

Of course, if there are multiple occurrences of N then each of those occurrences must also be consistent with integer; and if an occurrence is not consistent then the compiler will report an error -- a given expression may only have one type!

The bottom line is that **Star**’s types are based on a combination of unification for comparing types and a series of type rules that have the effect of introducing *constraints* on types based on which language features the programmer uses. The type checker is really a constraint solver: if the constraints are not satisfiable (for example by trying to ‘call’ a variable and add a number to it) then there is a type error in the program.

The magic of type inference arises because it turns out that solving these constraints is sufficient for robustly type checking programs.

A sharp-eyed reader will notice that **Star**’s type system is different in nature to that found (say) in OO languages. In **Star**’s type system, types are considered to be consistent in **Star** if they are *equal*.[^This is a slight over-simplification.] This is quite different to the notion of consistency in OO languages where an argument to a function is consistent if its type is a *sub-type* of the expected type.

However, we would note that the apparent restriction to the type system imposed by type equality is much less severe in practice than in theory -- and that OO languages’ type systems also incorporate some of the same restrictions.

### Why is type inference restricted?
We have stated a few times that **Star**'s type system only infers types of variables that are _not_ quantified. In fact, it is fairly simple to build a type inference system that can infer such types. For example, such a complete type inference system would infer from these equations:

```
conc(nil,x)=>x.
conc(cons(h,tl),x) => cons(h,conc(tl,x))
```
the generalized type for `conc`:
```
conc:all t ~~ (cons[t],cons[t])=>cons[t].
```
However, several technical and non-technical considerations stay our hand at building such a type inference system:

* There are still types that cannot be correctly inferred; and would therefore require explicit type annotations to correctly type the program.
* Having explicit type annotations is 'good style' in general and definitely aids in debugging type errors.

On the other hand, requiring type annotations for _every_ variable is extremely tedious and verbose. An extreme version of this policy would require the `conc` program above to be written:

```
conc:all t ~~ (cons[t],cons[t])=>cons[t].
conc(nil,x:list[t])=>x.
conc(cons(h:t,tl:list[t]),x:list[t]) => cons(h,conc(t,x))
```
The designer of **Star** strikes a balance between useability and rigor: most variables do not require explicit type annotations. We require them only when something 'special' is being indicated; one of those special circumstances is when defining a generic function.

Even there, there are many situations where explicit type annotations are not needed: for example when defining a field in a record, or a function in the implementation of a contract, there already is a type that the type system can use to verify the program.

So, the precise rule for type inference is:

>If the type of a variable can be inferred from context, then use that type to verify the type of any value the variable may be bound to. Otherwise, use type inference on the value to infer a type for teh variable but do not attempt to generalize it by adding quantifiers to the inferred type.

We are only able to scratch the surface of the type system here. It is certainly true that -- like many modern functional languages -- **Star**’s type system is complex and subtle. The primary motivation for this complexity is to reduce the burden for the programmer: by being able to infer types automatically, and by being able to address many programming subtleties, the type system comes to be seen as the programmer’s friend rather than as an obstacle to be ‘gotten around’.

## Are we there yet?

The straightforward answer to this is no. There is a great deal more to functional programming than can be captured in a few pages. However, we have covered some of the key features of functional programming -- particularly as it applies to **Star**. In subsequent chapters we will take a closer look at collections, at modular programming, at concurrency and even take a pot shot at Monads.

If there is a single idea to take away from this chapter it should be that functional programming is natural. If there is a single piece of advice for the budding functional **Star** programmer, it should be to *hide the recursion*. If there is a single bit of comfort to offer programmers it should be that *Rome was not built in a day*.

In the next chapter we look at collections, one of the richest topics in programming.
