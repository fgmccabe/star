# Collections

Modern programming -- whether it is OO programming, functional programming or just plain C programming -- relies on a rich standard library. Given that nearly every program needs to be able to manage collections of *things*, the central pearl of any standard library is the *collections* library. Recalling our mantra of hiding recursion; a well designed collections library can make a huge difference to the programmer’s productivity, often by hiding many of the recursions and iterations required to process collections.

The collections architecture in **Star** has four main components:

1. a range of standard collection types -- including array-like lists, cons lists, red-black trees, first-in first-out queues, and dictionaries;
2. a range of standard functions -- mostly defined in contracts -- that define the core capabilities of functions over collection;
3. special notations that make programming with collections in a type independent way more straightforward; and
4. the final major component of the collections architecture is *queries*. **Star** has a simple yet powerful set of features aimed at simplifying querying collections.


## Sequence notation

A sequence is simply an ordered collection; a sequence expression is an expression involving a complete or partial enumeration of the values in the collection. Star has a simple notation for expressing sequences of any underlying type; for example, a cons sequence of integers from 1 through 5 can be written:
```
cons of [1, 2, 3, 4, 5]
```
In situations where we do not know or do not wish to specify the collection type, we can write instead:
```
[1, 2, 3, 4, 5]
```
This term -- it could be either an expression or a pattern -- denotes the sequence *without* specifying the underlying collection type. The difference in the types of the two terms is telling:
```
cons[integer]
```
and
```
stream[c->>integer] |: c
```
respectively -- where `c` is a type variable. The first is a concrete type expression, the second is a constrained type --  in this case `c` must implement the `stream` contract.

Although the second type expression is longer, and a bit more complex to read, it is also actually less constraining. The type expression `cons[integer]` does not allow for variation of the underlying collection type; the second type expression allows the term to be used in contexts that require different concrete types.

### Partial sequence notation
The sequence notation also allows for the specification of partial sequences; this is particularly useful in writing functions that construct and traverse sequences. The sequence term:
```
[1,2,..X]
```
denotes the sequence whose first two elements are `1` and `2` and whose remainder is denoted by the variable `X` -- which must also be a sequence of the correct type. Similarly, the term:
```
[F..,23]
```
denotes the sequence obtained by gluing `23` to the back of the sequence `F`.

There is a strong relationship between the normal sequence notation and the partial sequence notation. In particular, the sequence expression
```
cons of [1,2]
```
is equivalent to:
```
cons of [1,..cons of [2,..cons of []]]
```
However, we are not permitted to use both of `,..` and `..,` in the same expression:
```
[F..,2,3,..B]
```
is not permitted (since it amounts to a concatenation of two sequences which implies a non-deterministic decomposition when used as a pattern).

The major benefit of general sequence notation is that it allows us to construct programs involving collections that are independent of type *and* to do so in a syntax which is concise: the only constraint is the sequence contract.

For example, we can use sequence notation to write functions over sequences; such as the concat function that concatenates two sequences:
```
concat:all c,e ~~ stream[c->>e] |: (c,c)=>c.
concat([],X) => X.
concat([E,..X],Y) => [E,..concat(X,Y)].
```
This function will work equally well with `cons` lists, lists, strings, even your own collection types. All that is required is that there is an implementation of the sequence contract for the actual type being concatenated.

### The stream contract
Underlying the sequence notation is the `stream` contract. This contract contains type signatures that can be used to construct and to match against stream values. The stream notation is realized by the compiler translating stream terms to a series of calls to those functions.

The actual `stream` contract is
```
contract stream[t->>e] ::= {
  _nil:t.     -- empty stream
  _cons:(e,t)=>t. -- add to front
  _apnd:(t,e)=>e. -- add to back
  _eof:()=>boolean.   -- is stream empty
  _hdtl:(t)=>option[(e,t)]. -- match front of stream
  _back:(t)=>option[(t,e)]. -- match back of stream
}
```

The first three entries in this contract should be fairly self-evident:

* `_nil` is the empty stream;
* `_cons` is a function that ‘glues’ a new element to the front of the collection; and
* `_apnd` appends elements to the *back* of the collection.

The compiler uses these three functions to transform stream expressions into function calls.

For example, the sequence expression:
```
[1,2,3]
```
is transformed into
```
_cons(1,_cons(2,_cons(3,_nil())))
```
If a stream expression has an explicit type marker on it, then its translation is slightly different -- to allow the type checker to make use of the type information. For example,
```
cons of [1,2]
```
is translated as:
```
_cons(1,_cons(2,_nil())):cons[_]
```
This annotation is all that is needed to force the compiler to treat the result as a concrete cons list. Type inference does the rest of the hard work.[^fn2]

[^fn2]: The type expression `_` is a special type that denotes an anonymous type: each occurrence of the type expression denotes a different unknown type. It is useful in situations, like this one, where only some of the type information is known.

### Sequence patterns
The complete stream contract has six signatures in it -- the latter three signatures play an analogous role to the first three but for sequence *patterns* rather than sequence *expressions*. For example, the signature for `_pair` -- which is used to decompose sequences into a head and tail -- is:
```
_hdtl:(t)=>option[(e,t)].
```
This function will be applied to a sequence in the attempt to split it into a head and remainder. The question is how can a function be used in a pattern?

The term `[1,2,..X]` _as a pattern_ is rewritten as:
```
_hdtl^(1,_hdtl^(2,X))
```
where the `^` is syntactic sugar for the more elaborate form:
```
S0 where (1,S1) ^= _hdtl(S0) && (2,X)^=_hdtl(S1)
```
which is, in turn, syntactic sugar for:
```
S0 where some((1,S1)) .= _hdtl(S0) && some((2,X)) .= _hdtl(S1)
```
I.e., the sequence pattern becomes a series of progressive decompositions of the stream; at each stage an `option`-valued function is applied to peel off elements from the stream.

We can now straightforwardly give the translation for sequence patterns. Syntactically, there is no distinction between sequence expressions and sequence patterns -- what distinguishes them is context: sequence patterns show up as patterns in functions and sequence expressions show up in the expression context.

A sequence pattern, as in the pattern `[E,..X]` for the non-empty case in `concat`:
```
concat([E,..X],Y) => [E,..concat(X,Y)]
```
is transformed into the pattern:
```
_hdtl^(E,X)
```
and the entire `concat` equation becomes:
```
concat(_hdtl^(E,X),Y) => _cons(E,concat(X,Y))
```
which, as we noted above, is actually equivalent to:
```
concat(S0,Y) where some((E,X)).=S0 =>
  _cons(E,concat(X,Y)).
```

The `stream` contract is one of the most important and commonly used contracts in the **Star** library. As we shall see further, many of the standard collections functions are built on top of it.

### Notation and contracts
One of the distinctive features of the sequence notation is that it is an example of *syntax* that is underwritten by a semantics expressed as a *contract*. This is part of a widespread pattern in **Star**.

This has a parallel in modern OO languages like Java and C# where important contracts are expressed as interfaces rather than concrete types. However, **Star** extends the concept by permitting special notation as well as abstract interfaces -- as many mathematicians understand, a good notation can make a hard problem easy. In **Star** we further separate interfaces from types by separating the type definition from any contracts that may be implemented by it.

This is part of a general pattern in **Star**: there are many *sub-languages* that are actually underwritten by contracts for their realization. For example, the *indexing* notation has the same pattern: of a special notation backed by contract.

The merit of this combination of special syntax and contracts is that we can have the special notation expressing a salient concept -- in this case the sequence -- and we can realize the notation without undue commitment in its lower-level details. In the case of sequence notation, we can have a notation of sequences without having to commit to the type of the sequence itself.

## Indexing
Accessing collections conveniently is arguably more important than a good notation for representing them. There is a long standing traditional notation for accessing arrays:
```
L[ix]
```
where L is some array or other collection and ix is an integer offset into the array. **Star** uses a notation based on this for accessing collections with random indices; suitably generalized to include dictionaries (collections accessed with non-numeric indices) and *slices* (contiguous sub-regions of collections).

Before we explore **Star**’s indexing notation it is worth looking at the contract that underlies it -- the indexed contract.

### The indexed contract
The indexed contract captures the essence of accessing a collection in a random-access fashion. There are functions in the contract to access a directly accessed element, to replace and to delete elements from the collection:

```
contract all s,k,v ~~ indexed[s->>k,v] ::= {
  _index:(s,k)=>option[v].
  _insert:(s,k,v)=>s.
  _replace:(s,k,v)=>s.
  _remove:(s,k)=>s.
}
```
There are several noteworthy points here:

* the form of the contract itself; the signature for _index which accesses elements; and
* the signatures for `_insert`, `_replace` and `_remove` which return new collections rather than modifying in-place.

Recall that the stream contract had the form:
```
contract all s, e ~~ stream[s->>e] ::= ...
```
the `s->>e` clause allows the implementation of the contract to functionally determine (sic) the type of the elements of the collection.

In the case of indexed, the contract form determines *two* types denoted by k and v. The type k denotes the type of the key used to access the collection and v denotes the type of the elements of the collection. Each individual implementation of indexed is free to specify these types; usually in a way that best reflects the natural structure of the collection.

For example, the implementation of `indexed` for strings starts:
```
implementation indexed[string ->> integer,integer] => ...
```
reflecting the fact that the natural index for strings is integer and the natural element type is integer (neither being explicitly part of the string type name).  [^Note that we use `integer` to denote the type of string characters because of the complexities of Unicode representation which makes a consistent character encoding very difficult.]

On the other hand, the implementation of `indexed` for the concrete type `map` starts:
```
implementation all k,v ~~
  indexed[map[k,v] ->> k,v] => { ... }
```
reflecting the fact that dictionaries are naturally generic over both the key and value types.

If we look at the signature for `_index` we can see that this function does not directly return a value from the collection, but instead returns an `option` value. This bears further explanation.

The great unknown of accessing elements of a collection is ‘is it there?’. Its not guaranteed of course, and we need to be able to handle failure. In the case of the `_index` function, its responsibility is to either return a value wrapped as a `some` value -- if the index lookup is successful -- or the signal `none` if the index lookup fails. Just to be clear: `_index` can act both as a lookup *and* as a test for membership in the collection.

### Adding and removing elements
The function `_insert` is used to add an element to a collection associating it with a particular index position; and the function `_remove` removes an identified element from the collection. The function `_replace` is similar to `_insert` except that it is expected that an existing element is replaced rather than a new value being inserted.

These functions have a property often seen in functional programming languages and not often seen elsewhere: they are defined to return a complete new collection rather than simply side-effecting the collection. This is inline with an emphasis on *persistent data structures*[^fn1] and on *declarative programming*.

One might believe that this is a bit wasteful and expensive -- returning new collections instead of side-effecting the collection. However, that is something of a misconception: modern functional data structures have excellent computational properties and approach the best side-effecting structures in efficiency. At the same time, persistent data structures have many advantages -- including substantially better correctness properties and behavior in parallel execution contexts.

It should also be stressed that the `indexed` contract allows and encourages persistence but does not *enforce* it. It is quite possible to implement indexing for data structures that are not persistent.

[^fn1]: A persistent structure is one which is never modified -- changes are represented by new structures rather than modifiying existing ones.

### The index notation
Given the indexed contract we can now show the specific notation that **Star** has for accessing elements of a collection. Accessing a collection by index follows conventional notation:
```
C[ix]
```
will access the collection `C` with element identified by ix. For example, given a `map` D of strings to strings, we can access the entry associated with “alpha” using:

D["alpha"]

Similarly, we can access the third character in a string S using:

S[2]

As might be expected, given the discussion above, the type of an index expression is optional.

The most natural way of making use of an index expression is to use it in combination with a `^=` condition or an `^|` expression -- which allows for smooth handling of the case where the index fails. For example, we might have:

```
nameOf(F) where N ^= names[F] => N.
nameOf(F) => ...
```

We will take a deeper look at exceptions and more elaborate management of tentative computation in the section on [Computation Expressions](computation-expression).

**Star** also has specific notation to represent modified collections. For example, the expression
```
D["beta"->"three"]
```
denotes the map `D` with the entry associated with `"beta"` replaced by the value `"three"`. Note that the value of this expression is the updated map.

For familiarity’s sake, we also suppose a form of assignment for the case where the collection is part of a read-write variable. The action:
```
D["beta"] := "three"
```
is entirely equivalent to:
```
D := D["beta"->"three"]
```
always assuming that the type of D permits assignment.

Similarly, the expression:
```
D[\+"gamma"]
```
which denotes the map `D` where the value associated with the key `"gamma"` has been removed.

Although, in these examples, we have assumed that `D` is a map value (which is a standard type in **Star**); in fact the index notation does not specify the type. As with the sequence notation, the only requirement is that the `indexed` contract is implemented for the collection being indexed.

In particular, index notation is supported for the built-in `list` types, and is even supported for the `string` type.

In addition to the indexed access notation described so far, **Star** also allows a variant of the sequence notation for constructing indexed literals (aka dictionaries). In particular, an expression of the form:
```
["alpha"->1, "beta"->2, "gamma"->3]
```
is equivalent to a sequence of tuples, or to:
```
_cons(("alpha",1),_cons(("beta",2),_cons(("gamma",3),_nil))
```
which is understood by indexed types as denoting the contruction of a literal.

Note that there are two levels of domain-specific notation here: the representation of indexed literals in terms of a sequence of two-tuples and the implicit rule governing indexed types: they should implement a specific form of sequence contract. Both are actually part of the semantics of representing indexed literals.

### Implementing indexing
Of course, this includes our own types. For example, before, when looking at generic types we saw the tree type:
```
all t ~~ tree[t] ::= tEmpty | tNode(tree[t],t,tree[t]).
```
We can define an implementation for the indexed contract for this type -- if we arrange for the tree to be a tree of key-value pairs:
```
implementation all k,v ~~
    order[k], equality[k] |: indexed[tree[(k,v)]->>k,v] => {
  _index(T,K) => findInTree(T,K).
  _insert(T,K,V) => setKinTree(T,K,V).
  _replace(T,K,V) => setKinTree(T,K,V).
  _remove(T,K) => removeKfromTree(T,K).
}
```
The form of the type expression `tree[(k,v)]` is required to avoid confusion -- `tree` takes a single type argument that, in this case, is a tuple type. The extra set of parentheses ensures that tree is not interpreted (incorrectly) as a type that takes two type arguments.

With this statement in scope, we can treat appropriate tree expressions as though they were regular arrays or dictionaries:
```
T = tNode(tEmpty,("alpha","one"),tEmpty)
assert "one" ^= T["alpha"].
U = T["beta"->"two"]. -- Add in "beta"
assert "one" ^= U["alpha"].
assert present U["beta"].
```
The implementation statement relies on another feature of **Star**’s type system -- we need to constrain the implementation of indexed to a certain subset of possible instances of tree types -- namely, where the element type of the tree is a *pair* -- a two-tuple -- and secondly we require that the first element of the pair is comparable -- i.e., it has the comparable contract defined for it.

This is captured in the contract clause of the implementation statement:
```
implementation all k,v ~~ order[k], equality[k] |:
      indexed[tree[(k,v)]->>k,v] => ...
```
This implementation contract qualifier is fairly long, and the type constraints are fairly complex; but it is exquisitely targeted at precisely the right kind of tree without us having to make any unnecessary assumptions.[^fn1]

Implementing the indexed contract requires us to implement three functions: `findInTree`, `setKinTree` and `removeKfromTree`. The `findInTree` function is quite straightforward:
```
findInTree:all k,v ~~ equality[k], order[k] |: (tree[(k,v)],k)=>option[v].
findInTree(tEmpty,_) => none.
findInTree(tNode(_,(K,V),_),K) => some(V).
findInTree(tNode(L,(K1,_),_),K) where K1>K => findInTree(L,K).
findInTree(tNode(_,(K1,_),R),K) where K1<K => findInTree(R,K).
```
Notice that each ‘label’ in the tree is a 2-tuple -- consisting of the key and the value. This function is also where we need the key type to be both comparable and supporting equality. The comparable constraint has an obvious source: we perform inequality tests on the key.

The `equality` constraint comes from a slightly less obvious source: the repeated occurrence of the `K` variable in the second equation. This equation is actually equivalent to:
```
findInTree(tNode(_,(K,V),_),K1) where K==K1 => some(V)
```
We leave the implementations of `setKinTree` and `removeKfromTree` as an exercise for the reader.

Along with the implementation of `indexed`, we should also implement `stream` for our trees:
```
implementation all k,v ~~ order[k], equality[k] |:
  stream[tree[(k,v)]->>(k,v)] => {
    _nil = tEmpty.
    _cons((K,V),T) => setKinTree(T,K,V).
  ...
}
```
where, again, we leave the implementation of the remaining part of the contract to the reader. The reason for implementing `stream` is that that will allow users of the tree to use the `map` variant of the sequence notation for writing tree literals; as in:
```
tree of [1->”alpha”, 2->”beta”]
```
The `stream` contract includes functions that can be used to match sequences. It is not completely obvious what one should do when matching the head of a tree. The standard library chooses to match against the left-most leaf of the tree and return a new tree containing all the other elements for the tail of the match. Since the tree is ordered by key value, the order in which elements of a tree are written in a tree literal does not affect the finally constructed tree value.[^fn2]

[^fn1]: It is also true that most programmers will not be constructing new implementations of the indexed contract very frequently.

[^fn2]: With the possible exception of tree balance.

### Index slices
Related to accessing and manipulating individual elements of collections are the *indexed slice* operators. An indexed slice of a collection refers to a bounded subset of the collection. The expression:
```
C[fx:tx]
```
denotes the subsequence of `C` starting with -- and including -- the element indexed at `fx` and ending -- but *not* including the element indexed at `tx`.

As might be expected, the index slice notation is also governed by a contract -- the `sliceable` contract. This contract defines the core functions for slicing collections and for updating subsequences of collections:
```
contract all s,k ~~ sliceable[s->>k] ::= {
  _slice:(s,k,k)=>s.
  _tail:(s,k)=>s.
  _splice:(s,k,k,s)=>s.
}
```
The `_slice` function is used extract a slice from the collection, `_tail` is a variant that returns the ‘rest’ of the collection, and `_splice` is used to replace a subset of the collection with another collection.

Like the indexing notation, there is notation for each of the three cases:
```
C[fx:]
```
denotes the tail of the collection -- all the elements in `C` that come after `fx` (including `fx` itself);[^The complement of the tail slice is simple: `C[0:tx]`.] and
```
C[fx:tx->D]
```
denotes the result of splicing D into C. This last form has an additional incarnation -- in the form of an assignment statement:
```
C[fx:tx] := D
```
This action is equivalent to the assignment:
```
C := _splice(C,fx,tx,D)
```
which, of course, assumes that `C` is defined as a read/write variable.

**Star** encourages declarative programming but we fully recognize that side-effecting behavioral code is often the most effective solution to the problem.

The slice notation is an interesting edge case in domain specific languages. It is arguably a little obscure, and, furthermore, the use case it represents is not all that common. On the other hand, without specific support, the functionality of slicing is hard to duplicate with the standard indexing functions.

## Doing stuff with collections
One of the most powerful features of collections is the ability to treat a collection as a whole. We have already seen a little of this in our analysis of the [visitor pattern](going-even-further). Of course, the point of collections is to be able to operate over them as entities in their own right. As should now be obvious, most of the features we discuss are governed by contracts and it is paradigmatic to focus on contract specifications rather than specific implementations.

The number of things that people want to do with collections is only limited by our imagination; however, we can summarize a class of operations in terms several patterns:

* Filtering
* Transforming into new collections
* Summarizing collections
* Querying collections

Each of these patterns has some support from **Star**’s standard repertoire of functions.

### Filtering with filter
The simplest operation on a collection is to subset it. The standard filter function allows us to do this with some elegance. Using filter is fairly straightforward; for example, to remove all odd numbers from a collection we can use the expression:
```
Nums^/((X)=>X%2==0)
```
For example, if `Nums` were the list:
```
list of [1,2,3,4,5,6,7,8,9]
```
then the value of the filter expression would be
```
list of [2,4,6,8]
```
The right hand argument to `^/` is a *predicate*: a function that returns a `boolean` value. The `^/` function (which is part of the standard `filter` contract) is required to apply the predicate to every element of its left hand argument and return a *new* collection of every element that satisfies the predicate.[^fn1]

Note that the `%` function is arithmetic remainder, and the expression `X%2==0` amounts to a test that `X` is even (its remainder modulo 2 is 0).

By using a function argument to represent the predicate it is possible to construct many filtering algorithms whilst not making any recursion explicit. However, not all filters are easily handled in this way; for example, a prime number filter *can* be written
```
N^/isPrime
```
but such an expression is likely to be very expensive (the isPrime test is difficult to do well).

[^fn1]: The original collection is unaffected by the filter.

#### The filter contract
As noted above, the `^/` function is governed by a contract, the `filter` contract:
```
contract all c,e ~~ filter[c->>e] ::= {
  (^/):(c,(e)=>boolean) => c.
}
```
### The sieve of Erastosthneses
One of the classic algorithms for finding primes can be expressed using filters -- the so-called sieve of Eratosthenes. This algorithm works by repeatedly removing multiples of primes from the list of natural numbers. We cannot (yet) show how to deal with infinite lists of numbers but we can capture the essence of this algorithm using a cascading sequence of filter operations.

The core of the sieve algorithm involves taking a list of numbers and removing multiples of a given number from the list. This is very similar to our even-number finding task, and we can easily define a function that achieves this:
```
filterMultiples(K,N) => N^/((X)=>X%K=!=0).
```
The overall Eratosthenes algorithm works by taking the first element of a candidate list of numbers as the first prime, removing multiples of that number from the rest, and recursing on the result:
```
sieve([N,..rest]) => [N,..sieve(filterMultiples(N,rest))].
```
There is a base case of course, when the list of numbers is exhausted then we have no more primes:
```
sieve([]) => [].
```
The complete prime finding program is hardly larger than the original filter specification:
```
primes(Max) => let{
  sieve([]) => [].
  sieve([N,..rest]) => [N,..sieve(filterMultiples(N,rest))].

  filterMultiples(K,N) => N^/((X)=>X%K=!=0).

  iota(Mx,St) where Mx>Max => [].
  iota(Cx,St) => [Cx,..iota(Cx+St,St)].
} in [2,..sieve(iota(3,2))]
```
The `iota` function is used to construct a list of numbers, in this case the integer range from `3` through to Max with an increment of `2`. We start the `sieve` with `2` and the list of integers with `3` since we are making use of our prior knowledge that `2` is prime.

It should be emphasized that the sieve of Eratosthenes hardly counts as an efficient algorithm for finding primes. For one thing, it requires that we start with a list of integers; most of which will be discarded. In fact, each ‘sweep’ of the list of numbers results in a new list of numbers; many of which too will eventually be discarded. Furthermore, the filterMultiples function examines every integer in the list; it does not make effective use of the fact that successive multiples occupy predictable slots in the list of integers. In fact, building a highly optimized version of the sieve of Eratosthenes is not actually the main point here -- it is to illustrate the power of **Star**’s collections processing functions.

We might ask whether the sieve function can also be expressed as a filter. The straightforward answer is that it cannot: the sieve *is* a kind of filter, but the predicate being applied depends on the entire collection; not on each element. The standard filter function does not expose the entire collection to the predicate. However, we will see at least one way of achieving the sieve without any explicit recursion below when we look at folding operations.

### Mapping to make new collections
One of the limitations of the filter function is that it does not create new elements: we can use it to subset collections but we cannot transform them into new ones. The fmap function -- part of the functor contract -- can be used to perform many transformations of collections.

For example, to compute the lengths of strings in a list we can use the expression:
```
fmap(size,list of ["alpha","beta","gamma"])
```
which results in the list:
```
list of [5,4,5]
```
The `fmap` function is defined via the `functor` contract -- thus allowing different implementations for different collection types:
```
contract all c/1 ~~ functor[c] ::= {
  fmap:all a,b ~~ ((a)=>b,c[a])=>c[b].
}
```
Notice how the contract specifies the collection type -- `c` -- without specifying the type of the collection’s element type. We are using a different technique here than we used for the `stream` and `filter` contracts. Instead of using a functional dependency to connect the type of the collection to the type of the element, we denote the type of the input and output collections using a *type constructor* variable as in `c[a]` and `c[b]`.[^fn1]

We are also using a variant of the quantifier. A quantified type variable of the form `c/1` denotes a type constructor variable rather than a regular type variable. In this case, `c/1` means that the variable `c` must be a type constructor that takes one argument.

The reason for this form of contract is that `functor` implies creating a new collection from an old collection; with a possibly different element type. This is only possible if the collection is generic and hence the type expressions `c[a]` for the second argument type of `fmap` and `c[b]` for its return type.

One might ask whether we could not have used functional dependencies in a similar way to stream and filterable; for example, a contract of the form:
```
contract all c,e,f ~~ mappable[c->>e,f] ::=  {
  mmap:((e)=>f,c)=>c.
}
```
However, *this* contract forces the types of the result of the `mmap` to be identical to its input type, it also allows the implementer of the `mappable` contract to fix the types of the collection elements -- not at all what we want from a `fmap`.

It is not all that common that we need to construct a list of sizes of strings. A much more realistic use of `fmap` is for *projection*. For example, if we wanted to compute the average age of a collection of people, which is characterized by the type definition:
```
person ::= someOne{
  name:string.
  age:()=>float.
}
```
Suppose that we already had a function average that could average a collection of numbers; but which (of course) does not understand people. We can use our average by first of all projecting out the ages and then applying the average function:
```
average(fmap((X)=>X.age(),People))
```
In this expression we project out from the `People` collection the ages of the people and then use that as input to the average function.

There is something a little magic about the lambda function in this expression: how does the type checker ‘know’ that X can have a field age in it? How much does the type checker know about types anyway?

In this particular situation the type checker could infer the type of the lambda via the linking between the type of the `fmap` function and the type of the `People` variable. However, the type checker is actually capable of giving a type to the lambda even without this context. Consider the function:
```
nameOf(R) => R.name
```
This function takes an arbitrary record as input and returns the value of the name field. The `nameOf` function *is* well typed, its type annotation just needs a slightly different form than that we have seen so far:[^Note that the **Star** type system will not _infer_ this generalized type.]
```
nameOf:all r,n ~~ r <~ {name:n} |: (r)=>n
```
This is another example of a *constrained type*: in this case, the constraint on `r` is that it has a field called `name` whose type is the same as that returned by `nameOf` itself.

The type constraint:
```
r <~ {name:n}
```
means that any type bound to `r` must have a `name` field whose type is denoted by the type variable `n` in this case.

With this type signature, we can use `nameOf` with any type that that a `name` field. This can be a record type; it can also be a type defined with an algebraic type definition that includes a record constructor.

[^fn1]: This also means that the collection type in `fmap` must be generic: it is not possible to implement `functor` for strings.

### Compressing collections
Another way of using collections is to summarize or aggregate over them. For example, the average function computes a single number from an entire collection of numbers -- as do many of the other statistical functions. We can define average using the standard `foldLeft` function, which is part of the standard `folding` contract:
```
average(C) is foldLeft((+),0,C)::float/size(C)::float
```
This definition of the `average` function is about as close to a specification of average as is possible in a programming language!

Notice the use of coercion here -- coercing both the result of the `foldLeft` and `size` to float. The reason for doing this is that functions like `average` are ‘naturally’ real functions.[^fn1] Without the explicit coercion, averaging a list of integers will also result in an integer value -- which is likely to be inaccurate.

Of course, in our definition of average we need to coerce *both* the numerator and denominator of the division because **Star** does not have implicit coercion.

The `foldLeft` function applies a left-associative binary operator to a collection: starting from the first element and successively ‘adding up’ each of the elements in the collection using the supplied operator.

As we noted above, `foldLeft` is part of the `folding` contract. Like the `functor` contract, this uses some more subtle type constraints:
```
contract all c,e ~~ folding[c->>e] ::= {
  foldRight:all x ~~ (((e,x)=>x),x,c) => x.
  foldLeft:all x ~~ (((x,e)=>x),x,c) => x.
}
```
The `folding` contract uses quantifiers in two places: once in the contract specification and once in the type signature for `foldLeft` (and `foldRight`). What we are trying to express here is that any implementation of `folding` must allow for a generic function to process the collection.

The `foldLeft` (and `foldRight`) functions have an ‘accumulator’ (of type `x`) which need not be the same as the type of the elements of the collection.[^We saw something similar with the visitor pattern.] This argument acts as a kind of linking thread during the entire computation -- and represents the returned value when the fold is complete.

![Left Folding a Collection][leftfold]

But we can do much more than computing averages with a fold. Recall that when we realized the sieve of Eratosthenes, we still had a recursive structure to the program. Furthermore, the way our original program was written each filter results in a new list of numbers being produced. Instead of doing this, we can construct a cascade of filter functions.

Consider the task of adding a filter to an existing filter. What is needed is a new function that combines the effect of the new filter with the old one. The cascade function takes a filter function and a prime as arguments and constructs a new function that checks both the prime *and* the existing filter:
```
cascade:((integer)=>boolean,integer)=>((integer)=>boolean).
cascade(F,K) => (X)=>F(X) && X%K=!=0.
```
This is a truly higher-order function: it takes a function as argument and returns a another function.

Given `cascade`, we can reformulate the sieve function itself as a `foldRight` -- at each new prime step we ‘accumulate’ a new cascaded filter function:
```
stp:(integer,(integer)=>boolean)=>((integer)=>boolean).
stp(X,F) where F(X) => cascade(F,X).
stp(X,F) => F.
```
At each step in the fold we want to know whether to continue to propagate the existing filter or whether to construct a new filter.

The `sieve` function itself is now very short: we simply invoke `foldRight` using step and an initial ‘state’ consisting of a function that checks for odd numbers:
```
sieve(C) => foldRight(stp,(K)=>K%2=!=0,C).
```
This version of sieve is not quite satisfactory as, while it does find prime numbers, it does not report them. A more complete version has to also accumulate a list of primes that are found. We can do this by expanding the accumulated state to include both the cascaded filter function and the list of found primes. The main alteration is to the `step` function:
```
step:((list[integer],(integer)=>boolean),integer) =>
(list[integer],(integer)=>boolean).
step(X,(P,F)) where F(X) => ([P..,X],cascade(F,X))
step(_,(P,F)) => (P,F)
```
and the initial state has an empty list:
```
sieve(C) is fst(foldRight(step,([],(K)=>K%2=!=0),C)).
```
where fst and snd pick the left and right hand sides of a tuple pair:
```
fst:all s,t ~~ ((s,t))=>s.
fst((L,R)) => L
snd:all s,t ~~ ((s,t))=>t.
snd((L,R)) => R
```
There is one final step we can make before leaving our sieve of Eratosthenes -- we can do something about the initial list of integers. As it stands, while the sieve program does not construct any intermediate lists of integers, it still requires an initial list of integers to filter. However, this particular sequence can be represented in a very compact form -- as a `range` term.

`range` terms are special forms of collections that denote ranges of numeric values. For example, the expression
```
range(0,100,2)
```
denotes the sequence of integers starting at zero, not including 100, each succesive integer being incremented by 2.

Using a similar `range` term, we can denote the list of primes less than 1000 with
```
primes(Max) => let{
  cascade:((integer)=>boolean,integer) => ((integer)=>boolean).
  cascade(F,K) => (X)=>(F(X) && X%K=!=0).

  step(X,(P,F)) where F(X) => ([P..,X],cascade(F,X)).
  step(_,(P,F)) => (P,F).

  sieve:(range[integer])=>list[integer].
  sieve(R) => fst(foldRight(step,([],(K)=>true),R)).
} in sieve(range(3,Max,2)).

show primes(1000)
```
This final program has an important property: there are no explicit recursions in it -- in addition, apart from the `foldRight` function, there are no recursive programs at all in the definition of primes.

>Of course, it still would not count as the _most efficient_ primes finding program; but that was not the goal of this discussion.

[folding]: folding.png width=332px height=204px

[leftfold]: leftfold.png width=376px height=173px

[^fn1]: Real as in the Real numbers.

## Different types of collection
Just as there are many uses of collections, so there are different performance requirements for collections themselves. The most challenging aspects of implementing collections revolves around the cost of adding to the collection, the cost of *accessing* elements of the collection and the cost of *modifying* elements in the collection.

There is a strong emphasis on *persistent* semantics for the types and functions that make up **Star**’s collections architecture. This is manifest in the fact, for example, that functions that add and remove elements from collections *do not* modify the original collection.

However, even without that constraint, different implementation techniques for collections tend to favor some operations at the cost of others. Hence, there are different types of collection that favor different patterns of use.

### The list type
The list type offers a different trade-off to the cons type: where the latter is optimal for ease of constructing and for traversing complete lists, the list type offers constant-time access to random elements within the array -- at the potential cost of more expensive construction of lists.

Unlike the cons type, the list type does not have a straightforward definition as an algebraic type. Internally, an list structure consists of an array of locations with a ‘control pointer’ giving the portion of the array block that represents a given list value.

The list type is optimized for random access and for shared storage -- recall that **Star** collection types are persistent: that means that different values can share some or all of their internal structure. The diagram below shows two list values that overlap in their elements and consequently share some of their structure.

![Two lists Sharing Structure][twoarrays]

[twoarrays]: twoarrays.pdf width=311px height=143px

### The cons type

This is the simplest collection type; and is perhaps the original collection type used in functional programming languages. It is defined by the type declaration:
```
all t ~~ cons[t] ::= nil | cons(t,cons[t]).
```
Cons lists have the property that adding an element to the front of a list is a constant-time operation; similarly, splitting a cons list into its head and tail is also a constant time operation. However, almost every other operation is significantly more expensive: putting an element on to the end of a cons list is linear in the length of the list.

The main merit of the cons list is the sheer simplicity of its definition. Also, for small collections, its simple implementation outweighs the advantages that more complex collections offer.

### The map type
Unlike the cons or list type, the `map` type is oriented for access by arbitrary keys. The `map` is also quite different to hash trees as found in Java (say), the `map` type is *persistent*: the functions that access dictionaries such as by adding or removing elements return new dictionaries rather than modifying a single shared structure. However, the efficiency of the map is quite comparable to Java’s HashMap.

The template for the `map` type is:
```
all k,v ~~ equality[k] |: map[k,v]
```
Notice that there is an implied constraint here: the `map` assumes that the keys in the map can be compared for equality.

A `map` value can be written using the sequence notation, using tuple pairs for the key-value pairs:
```
map of [(1,"alpha"),(2,"beta”)]
```
Dictionaries also have a special variant of the sequence notation; instead of writing the pairs as tuples we can use an arrow notation for `map` terms:
```
map of [1->"alpha", 2->"beta"]
```
Dictionaries also have their own special variant of a *query search condition*. A condition of the form
```
K->V in D
```
where D is a `map` will be satisfied if there is a key/value pair in D corresponding to K and V. For example, the condition:
```
K->V in map of [1->"alpha", 2->"beta"] && V=="alpha"
```
is satisfied for only one pair of `K` and `V`: namely `1` and `"alpha"` respectively.

For the curious, dictionaries are implemented using techniques similar to Ideal Hash Trees, as described by Bagwell [#Bagwell01idealhash]. This results in a structure with an effective O(1) cost for accessing elements *and* for modifying the `map` -- all the while offering an applicative data structure.

### The set type

There are many instances where a programmer needs a collection but does not wish specify any ordering or mapping relationship. The standard set type allows you to construct such entities.

Using a set type offers the programmer a signal that minimizes assumptions about the structures: the set type is not ordered, and offers no ordering guarantees. It does, however, offer a guarantee that operations such as element insertion, search and set operations like set union are implemented efficiently.

Like `map`, the set type is not publicly defined using an algebraic type definition: its implementation is private. It’s type is given by the template:
```
all t ~~ equality[t] |: set[t]
```
### The range type

The `range` type is a very particular form of collection type: a range denotes a range of numbers. Its type description says it all:
```
all t ~~ arithmetic[t],comp[t] |: range[t] ::= range(t,t,t).
```
This type description has some special features; in particular it is a constrained type: a type expression of the form:
```
range[T]
```
is only valid if `T` is an `arithmetic` type; specifically a type that supports arithmetic and is `comp`arable. Thus a type expression such as range[integer] is fine, but range[list[integer]] will result in a syntax error!

Range terms are used to compactly represent regular ranges of numbers; for example the term
```
range(0,100,1)
```
denotes the first 100 integers. But, of course, we have already seen the `range` collection in our exploration of the [sieve of Eratosthenes](original-sieve).

There is a specific property of the `range` type that is difficult to capture with this type definition -- specifically, we rely in many places on ranges *half closed* property: that is, the range of numbers in the range include the first number but does not include the second. This property makes combining ranges much smoother than either a fully closed range (includes both ends) or an open range (includes neither end).

For example the following assertion is expected to hold for range terms:
```
range(F,I,Ix)++range(I,T,Ix)=range(F,T,Ix)
```
where `++` is the standard function -- i.e., is part of the concatenate contract -- for expressing sequence concatenation.

## Collecting it together

Collections form an important part of any modern programming language. The suite of features that make up the collections architecture in **Star** consists of a number of data types, contracts and special syntax that combine to significantly reduce the burden of the programmer.

The collections facility amounts to a form of DSL -- Domain Specific Language -- that is, in this case, built-in to the language. We shall see later on that, like many DSLs, this results in a pattern where there is a syntactic extension to the language that is backed by a suite of contracts that define the semantics of the DSL.
