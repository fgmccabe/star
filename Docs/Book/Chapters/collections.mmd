# Collections

Modern programming -- whether it is OO programming, functional programming or just plain C programming -- relies on a rich standard library. Given that nearly every program needs to be able to manage collections of *things*, the central pearl of any standard library is the *collections* library. Recalling our mantra of hiding recursion; a well designed collections library can make a huge difference to the programmer’s productivity, often by hiding many of the recursions and iterations required to process collections.

The collections architecture in **Star** has four main components:

1. a range of standard collection types -- including array-like lists, cons lists, first-in first-out queues, and ideal hash trees;
2. a range of standard functions -- mostly defined in contracts -- that define the core capabilities of functions over collections;
3. special notations that make programming with collections in a type independent way more straightforward; and
4. the final major component of the collections architecture is *queries*. **Star** has a simple yet powerful set of features aimed at simplifying querying collections.

The query component is sufficiently involved to merit a chapter of its own.

## Sequence notation

A sequence is an ordered collection; a sequence expression is an expression involving a complete or partial enumeration of the values in the collection. Star has a straightforward notation for expressing sequences of any underlying type; for example, a `cons` sequence of integers from 1 through 5 can be written:
```
cons of [1, 2, 3, 4, 5]
```
In situations where we do not know or do not wish to specify the collection type, we can write instead:
```
[1, 2, 3, 4, 5]
```
This term -- it could be either an expression or a pattern -- denotes the sequence *without* specifying the underlying collection type. The difference in the types of the two terms is telling:
```
cons[integer]
```
and
```
sequence[c->>integer] |: c
```
respectively -- where `c` is a type variable. The first is a concrete type expression, the second is a constrained type -- in this case `c` must implement the `sequence` contract.

Although the second type expression is longer, and a bit more complex to read, it is also actually less constraining. The type expression `cons[integer]` does not allow for variation of the underlying collection type; the second type expression allows the term to be used in contexts that require different concrete types.

### Partial sequence notation
The sequence notation also allows for the specification of partial sequences; this is particularly useful in writing functions that construct and traverse sequences. The sequence term:
```
[1,2,..X]
```
denotes the sequence whose first two elements are `1` and `2` and whose remainder is denoted by the variable `X` -- which must also be a sequence of the appropriate type. Similarly, the term:
```
[F..,23]
```
denotes the sequence obtained by gluing `23` to the back of the sequence `F`.

There is a strong relationship between the normal sequence notation and the partial sequence notation. In particular, the sequence expression
```
cons of [1,2]
```
is equivalent to:
```
cons of [1,..cons of [2,..cons of []]]
```
However, we are not permitted to use both of `,..` and `..,` in the same expression:
```
[F..,2,3,..B]
```
is not permitted (since it amounts to a concatenation of two sequences which, in turn, implies a non-deterministic decomposition when used as a pattern).

The major benefit of general sequence notation is that it allows us to construct programs involving collections that are independent of type *and* to do so in a syntax which is concise: the only constraint is the `stream` contract.

For example, we can use sequence notation to write functions over sequences; such as the `concat` function that concatenates two sequences:
```
concat:all c,e ~~ stream[c->>e], sequence[c->>e] |: (c,c)=>c.
concat([],X) => X.
concat([E,..X],Y) => [E,..concat(X,Y)].
```
This function will work equally well with `cons` lists, lists, strings, even your own collection types. All that is required is that there is an implementation of the `stream` and the `sequence` contracts for the actual type being concatenated.

>There are two contracts here: the `stream` contract is used when decomposing sequences and the `sequence` contract is used when building them.

### The `stream` and `sequence` contracts
Underlying the sequence notation are two contracts: the `stream` contrac and the `sequence` contract. These contracts contains type signatures that can be used to construct and to match against sequence values. The sequence notation is realized by the compiler translating sequence terms to a series of calls to those functions.

The standard `stream` contract is
```
contract stream[t->>e] ::= {
  _eof:()=>boolean.   -- is stream empty
  _hdtl:(t)=>option[(e,t)]. -- match front of stream
  _back:(t)=>option[(t,e)]. -- match back of stream
}
```
and the standard `sequence` contract is:
```
contract stream[t->>e] ::= {
  _nil:t.     -- empty stream
  _cons:(e,t)=>t. -- add to front
  _apnd:(t,e)=>e. -- add to back
}
```

The entries in the `sequence` contract should be fairly self-evident:

* `_nil` is the empty sequence;
* `_cons` is a function that ‘glues’ a new element to the front of the sequence; and
* `_apnd` appends elements to the *back* of the sequence.

The compiler uses these three functions to transform sequence expressions into function calls.

For example, the sequence expression:
```
[1,2,3]
```
is transformed into
```
_cons(1,_cons(2,_cons(3,_nil())))
```
If a sequence expression has an explicit type marker on it, then its translation is slightly different -- to allow the type checker to make use of the type information. For example,
```
cons of [1,2]
```
is translated as:
```
_cons(1,_cons(2,_nil())):cons[_]
```
This annotation is all that is needed to force the compiler to treat the result as a concrete cons list. Type inference does the rest of the hard work.[^The type expression `_` is a special type that denotes an anonymous type: each occurrence of the type expression denotes a different unknown type. It is useful in situations, like this one, where only some of the type information is known.]


### Sequence patterns
The the `stream` contract is used in *patterns* to match and decompose sequences. For example, the signature for `hdtl` -- which is used to decompose sequences into a head and tail -- is:
```
_hdtl:(t)=>option[(e,t)].
```
This function will be applied to a sequence in the attempt to split it into a head and remainder. The question is how can a function be used in a pattern?

The term `[1,2,..X]` _as a pattern_ is rewritten as:
```
_hdtl^(1,_hdtl^(2,X))
```
where the `^` is syntactic sugar for the more elaborate form:
```
S0 where (1,S1) ^= _hdtl(S0) && (2,X)^=_hdtl(S1)
```
which is, in turn, syntactic sugar for:
```
S0 where some((1,S1)) .= _hdtl(S0) && some((2,X)) .= _hdtl(S1)
```
I.e., the sequence pattern becomes a series of progressive decompositions of the stream; at each stage an `option`-valued function is applied to peel off elements from the stream.

We can now straightforwardly give the translation for sequence patterns. Syntactically, there is no distinction between sequence expressions and sequence patterns -- what distinguishes them is context: sequence patterns show up as patterns in functions and sequence expressions show up in the expression context.

A sequence pattern, as in the pattern `[E,..X]` for the non-empty case in `concat`:
```
concat([E,..X],Y) => [E,..concat(X,Y)]
```
is transformed into the pattern:
```
_hdtl^(E,X)
```
and the entire `concat` equation becomes:
```
concat(_hdtl^(E,X),Y) => _cons(E,concat(X,Y))
```
which, as we noted above, is actually equivalent to:
```
concat(S0,Y) where some((E,X)).=S0 =>
  _cons(E,concat(X,Y)).
```

The `sequence` and `stream` contracts are some of the most important and commonly used contracts in the **Star** library. As we shall see further, many of the standard collections functions are built on top of it.

> We have two contracts -- one for composing and another for decomposing sequences -- because not all collections are equally amenable to decomposing and/or composing. For example, the `map` type we describe below does not have a natural notion of decomposing (because ordering within a `map` is not preserved); even though it does have a natural form of describing actual `map` collections.

### Notation and contracts
One of the distinctive features of the sequence notation is that it is an example of *syntax* that is underwritten by a semantics expressed as a *contract*. This is part of a widespread pattern in **Star**.

This has a parallel in modern OO languages like Java and C# where important contracts are expressed as interfaces rather than concrete types. However, **Star** extends the concept by permitting special notation as well as abstract interfaces -- as many mathematicians understand, a good notation can sometimes make a hard problem easy. In **Star** we further separate interfaces from types by separating the type definition from any contracts that may be implemented by it.

The merit of this combination of special syntax and contracts is that we can have the special notation expressing a salient concept -- in this case the sequence -- and we can realize the notation without undue commitment in its lower-level details. In the case of sequence notation, we can have a notation of sequences without having to commit to the type of the sequence itself.

## Indexing
Accessing collections conveniently is arguably more important than a good notation for representing them. There is a long standing traditional notation for accessing arrays:
```
L[ix]
```
where `L` is some array or other collection and `ix` is an integer offset into the array. **Star** uses a notation based on this for accessing collections with random indices; suitably generalized to include dictionaries (collections accessed with non-numeric indices) and *slices* (contiguous sub-regions of collections).

Before we explore **Star**’s indexing notation it is worth looking at the contract that underlies it -- the `indexed` contract.

### The indexed contract
The indexed contract captures the essence of accessing a collection in a random-access fashion. There are functions in the contract to access a directly accessed element, to replace and to delete elements from the collection:

```
contract all s,k,v ~~ indexed[s->>k,v] ::= {
  _index:(s,k)=>option[v].
  _insert:(s,k,v)=>s.
  _replace:(s,k,v)=>s.
  _remove:(s,k)=>s.
}
```
There are several noteworthy points here:

* the form of the contract itself; the signature for `_index` which accesses elements; and
* the signatures for `_insert`, `_replace` and `_remove` which return new collections rather than modifying them in-place.

Recall that the `stream` contract had the form:
```
contract all s, e ~~ stream[s->>e] ::= ...
```
the `s->>e` clause allows the implementation of the contract to functionally determine (sic) the type of the elements of the collection.

In the case of `indexed`, the contract form determines *two* types denoted by `k` and `v`. The type `k` denotes the type of the key used to access the collection and `v` denotes the type of the elements of the collection. Each individual implementation of indexed is free to specify these types; usually in a way that best reflects the natural structure of the collection.

For example, the implementation of `indexed` for strings starts:
```
implementation indexed[string ->> integer,integer] => ...
```
reflecting the fact that the natural index for strings is integer and the natural element type is integer (neither being explicitly part of the string type name).  [^Note that we use `integer` to denote the type of string characters because the complexities of Unicode representation make a consistent character encoding very difficult.]

On the other hand, the implementation of `indexed` for the concrete type `map` starts:
```
implementation all k,v ~~
  indexed[map[k,v] ->> k,v] => { ... }
```
reflecting the fact that dictionaries are naturally generic over both the key and value types.

If we look at the signature for `_index` we can see that this function does not directly return a value from the collection, but instead returns an `option` value. This bears further explanation.

The great unknown of accessing elements of a collection is ‘is it there?’. Its not guaranteed of course, and we need to be able to handle failure. In the case of the `_index` function, its responsibility is to either return a value wrapped as a `some` value -- if the index lookup is successful -- or the signal `none` if the index lookup fails. Just to be clear: `_index` can act both as a lookup *and* as a test for membership in the collection.

#### Adding and removing elements
The function `_insert` is used to add an element to a collection associating it with a particular index position; and the function `_remove` removes an identified element from the collection. The function `_replace` is similar to `_insert` except that it is expected that an existing element is replaced rather than a new value being inserted.

These functions have a property often seen in functional programming languages and not often seen elsewhere: they are defined to return a complete new collection rather than simply side-effecting the collection. This is inline with an emphasis on *persistent data structures*[^A persistent structure is one which is never modified -- changes are represented by new structures rather than modifiying existing ones.] and on *declarative programming*.

One might believe that this is a bit wasteful and expensive -- returning new collections instead of side-effecting the collection. However, that is something of a misconception: modern functional data structures have excellent computational properties and approach the best side-effecting structures in efficiency. At the same time, persistent data structures have many advantages -- including substantially better correctness properties and behavior in parallel execution contexts.

>It should also be stressed that the `indexed` contract allows and encourages persistence but does not *enforce* it. It is quite possible to implement indexing for data structures that are not persistent.

### The index notation
Given the indexed contract we can now show the specific notation that **Star** has for accessing elements of a collection. Accessing a collection by index follows conventional notation:
```
C[ix]
```
will access the collection `C` with element identified by `ix`. For example, given a `map` `D` of strings to strings, we can access the entry associated with “alpha” using:
```
D["alpha"]
```
Similarly, we can access the third character in a string `S` using:
```
S[2]
```
As might be expected, given the discussion above, the type of an index expression is optional.

The most natural way of making use of an index expression is to use it in combination with a `^=` condition or an `^|` expression -- which allows for smooth handling of the case where the index fails. For example, we might have:

```
nameOf(F) where N ^= names[F] => N.
nameOf(F) default => ...
```

We will take a deeper look at exceptions and more elaborate management of tentative computation in the section on [Computation Expressions](computation-expression).

**Star** also has specific notation to represent modified collections. For example, the expression
```
D["beta"->"three"]
```
denotes the map `D` with the entry associated with `"beta"` replaced by the value `"three"`. Note that the value of this expression is the updated map.

For familiarity’s sake, we also suppose a form of assignment for the case where the collection is part of a read-write variable. The action:
```
D["beta"] := "three"
```
is entirely equivalent to:
```
D := D["beta"->"three"]
```
always assuming that the type of `D` permits assignment.

Similarly, the expression:
```
D[\+"gamma"]
```
which denotes the map `D` where the value associated with the key `"gamma"` has been removed.

Although, in these examples, we have assumed that `D` is a map value (which is a standard type in **Star**); in fact the index notation does not specify the type. As with the sequence notation, the only requirement is that the `indexed` contract is implemented for the collection being indexed.

In particular, as well as the `map` type, index notation is supported for the built-in `list` type, and is even supported for the `string` type.

In addition to the indexed access notation described so far, **Star** also allows a variant of the sequence notation for constructing indexed literals (aka dictionaries). In particular, an expression of the form:
```
["alpha"->1, "beta"->2, "gamma"->3]
```
is equivalent to a sequence of tuples, or to:
```
_cons(("alpha",1),_cons(("beta",2),_cons(("gamma",3),_nil))
```
which is understood by indexed types as denoting the contruction of a literal.

Note that there are two levels of domain-specific notation here: the representation of indexed literals in terms of a sequence of two-tuples and the implicit rule governing indexed types: they should implement a specific form of `sequence` contract. Both are actually part of the semantics of representing indexed literals.

### Implementing indexing
Of course, this includes our own types. For example, before, when looking at generic types we saw the tree type:
```
all t ~~ tree[t] ::= tEmpty | tNode(tree[t],t,tree[t]).
```
We can define an implementation for the indexed contract for this type -- if we arrange for the tree to be a tree of key-value pairs:
```
implementation all k,v ~~
    order[k], equality[k] |: indexed[tree[(k,v)]->>k,v] => {
  _index(T,K) => findInTree(T,K).
  _insert(T,K,V) => setKinTree(T,K,V).
  _replace(T,K,V) => setKinTree(T,K,V).
  _remove(T,K) => removeKfromTree(T,K).
}
```
The form of the type expression `tree[(k,v)]` is required to avoid confusion -- `tree` takes a single type argument that, in this case, is a tuple type. The extra set of parentheses ensures that `tree` is not interpreted (incorrectly) as a type that takes two type arguments.

With this statement in scope, we can treat appropriate `tree` expressions as though they were regular arrays or dictionaries:
```
T = tNode(tEmpty,("alpha","one"),tEmpty)
assert "one" ^= T["alpha"].
U = T["beta"->"two"]. -- Add in "beta"
assert "one" ^= U["alpha"].
```
The implementation statement relies on another feature of **Star**’s type system -- we need to constrain the implementation of indexed to a certain subset of possible instances of tree types -- namely, where the element type of the tree is a *pair* -- a two-tuple -- and secondly we require that the first element of the pair is comparable -- i.e., it has the `order` contract defined for it.

This is captured in the contract clause of the implementation statement:
```
implementation all k,v ~~ order[k], equality[k] |:
      indexed[tree[(k,v)]->>k,v] => ...
```
This implementation contract qualifier is fairly long, and the type constraints are fairly complex; but it is exquisitely targeted at precisely the right kind of tree without us having to make any unnecessary assumptions.[^It is also true that most programmers will not be constructing new implementations of the indexed contract very frequently.]

Implementing the indexed contract requires us to implement three functions: `findInTree`, `setKinTree` and `removeKfromTree`. The `findInTree` function is quite straightforward:
```
findInTree:all k,v ~~ equality[k], order[k] |: (tree[(k,v)],k)=>option[v].
findInTree(tEmpty,_) => none.
findInTree(tNode(_,(K,V),_),K) => some(V).
findInTree(tNode(L,(K1,_),_),K) where K1>K => findInTree(L,K).
findInTree(tNode(_,(K1,_),R),K) where K1<K => findInTree(R,K).
```
Notice that each ‘label’ in the tree is a 2-tuple -- consisting of the key and the value. This function is also where we need the key type to be both comparable and supporting equality. The comparable constraint has an obvious source: we perform inequality tests on the key.

The `equality` constraint comes from a slightly less obvious source: the repeated occurrence of the `K` variable in the second equation. This repeated occurrence means that the equation is equivalent to:
```
findInTree(tNode(_,(K,V),_),K1) where K==K1 => some(V).
```
We leave the implementations of `setKinTree` and `removeKfromTree` as an exercise for the reader.

Along with the implementation of `indexed`, we should also implement `sequence` for our trees:
```
implementation all k,v ~~ order[k], equality[k] |:
  sequence[tree[(k,v)]->>(k,v)] => {
    _nil = tEmpty.
    _cons((K,V),T) => setKinTree(T,K,V).
    _apnd(T,(K,V)) => setKinTree(T,K,V).
}
```
>>Notice that adding a pair to the front of a `tree` is semantically the same as adding it to the back -- since the `tree` is ordered.

The reason for implementing `sequence` is that that will allow users of the tree to use the indexed variant of the sequence notation for writing tree literals; as in:
```
tree of [1->”alpha”, 2->”beta”]
```
We do not implement the companion `stream` contract for our `tree` because its semantics would be somewhat problematic. However, there is nothing preventing the reader from experimenting with an appropriate tree-ordering.

### Index slices
Related to accessing and manipulating individual elements of collections are the *indexed slice* operators. An indexed slice of a collection refers to a bounded subset of the collection. The expression:
```
C[fx:tx]
```
denotes the subsequence of `C` starting with -- and including -- the element indexed at `fx` and ending -- but *not* including the element indexed at `tx`.

As might be expected, the index slice notation is also governed by a contract -- the `sliceable` contract. This contract defines the core functions for slicing collections and for updating subsequences of collections:
```
contract all s,k ~~ sliceable[s->>k] ::= {
  _slice:(s,k,k)=>s.
  _tail:(s,k)=>s.
  _splice:(s,k,k,s)=>s.
}
```
The `_slice` function is used extract a slice from the collection, `_tail` is a variant that returns the ‘rest’ of the collection, and `_splice` is used to replace a subset of the collection with another collection.

Like the indexing notation, there is notation for each of the three cases:
```
C[fx:]
```
denotes the tail of the collection -- all the elements in `C` that come after `fx` (including `fx` itself);[^The complement of the tail slice is simple: `C[0:tx]`.] and
```
C[fx:tx->D]
```
denotes the result of splicing D into C. This last form has an additional incarnation -- in the form of an assignment statement:
```
C[fx:tx] := D
```
This action is equivalent to the assignment:
```
C := _splice(C,fx,tx,D)
```
which, of course, assumes that `C` is defined as a read/write variable.

The slice notation is an interesting edge case in domain specific languages. It is arguably a little obscure, and, furthermore, the use case it represents is not all that common. On the other hand, without specific support, the functionality of slicing is hard to duplicate with the standard indexing functions.

## Doing stuff with collections
One of the most powerful features of collections is the ability to treat a collection as a whole. We have already seen a little of this in our analysis of the [visitor pattern](going-even-further). Of course, the point of collections is to be able to operate over them as entities in their own right. As should now be obvious, most of the features we discuss are governed by contracts and it is paradigmatic to focus on contract specifications rather than specific implementations.

The number of things that people want to do with collections is only limited by our imagination; however, we can summarize a class of operations in terms several patterns:

* Filtering
* Transforming into new collections
* Summarizing collections
* Querying collections

Each of these patterns has some support from **Star**’s standard repertoire of functions.

### Filtering
The simplest operation on a collection is to subset it. The standard filter function -- `^/` -- allows us to do this with some elegance. Using filter is fairly straightforward; for example, to remove all odd numbers from a collection we can use the expression:
```
Nums^/((X)=>X%2==0)
```
For example, if `Nums` were the list:
```
list of [1,2,3,4,5,6,7,8,9]
```
then the value of the filter expression would be
```
list of [2,4,6,8]
```
The right hand argument to `^/` is a *predicate*: a function that returns a `boolean` value. The `^/` function (which is part of the standard `filter` contract) is required to apply the predicate to every element of its left hand argument and return a *new* collection of every element that satisfies the predicate.[^The original collection is unaffected by the filter.]

Note that the `%` function is arithmetic remainder, and the expression `X%2==0` amounts to a test that `X` is even (its remainder modulo 2 is 0).

The `^/` operator allows us to represent many filtering algorithms whilst not making any recursion explicit. However, not all filters are easily handled in this way; for example, a prime number filter *can* be written
```
N^/isPrime
```
but such an expression is likely to be very expensive (the `isPrime` test is difficult to do well).

#### The filter contract
As noted above, the `^/` function is governed by a contract, the `filter` contract:
```
contract all c,e ~~ filter[c->>e] ::= {
  (^/):(c,(e)=>boolean) => c.
}
```
### The sieve of Erastosthneses
One of the classic algorithms for finding primes that can be expressed using filters is the so-called _sieve of Eratosthenes_. This algorithm works by repeatedly removing multiples of primes from a list of natural numbers. We cannot (yet) show how to deal with infinite lists of numbers but we can capture the essence of this algorithm using a cascading sequence of filter operations.

The core of the sieve algorithm involves taking a list of numbers and removing multiples of a given number from the list. This is very similar to our even-number finding task, and we can easily define a function that achieves this:
```
filterMultiples(K,N) => N^/((X)=>X%K=!=0).
```
The overall Eratosthenes algorithm works by taking the first element of a candidate list of numbers as the first prime, removing multiples of that number from the rest, and recursing on the result:
```
sieve([N,..rest]) => [N,..sieve(filterMultiples(N,rest))].
```
There is a base case of course, when the list of numbers is exhausted then we have no more primes:
```
sieve([]) => [].
```
The complete prime finding program is hardly larger than the original filter specification:
```
primes(Max) => let{
  sieve([]) => [].
  sieve([N,..rest]) => [N,..sieve(filterMultiples(N,rest))].

  filterMultiples(K,N) => N^/((X)=>X%K=!=0).

  iota(Mx,St) where Mx>Max => [].
  iota(Cx,St) => [Cx,..iota(Cx+St,St)].
} in [2,..sieve(iota(3,2))]
```
The `iota` function is used to construct a list of numbers, in this case the integer range from `3` through to Max with an increment of `2`. We start the `sieve` with `2` and the list of integers with `3` since we are making use of our prior knowledge that `2` is prime.

It should be emphasized that the sieve of Eratosthenes hardly counts as an efficient algorithm for finding primes. For one thing, it requires that we start with a list of integers; most of which will be discarded. In fact, each ‘sweep’ of the list of numbers results in a new list of numbers; many of which too will eventually be discarded. Furthermore, the `filterMultiples` function examines every integer in the list; it does not make effective use of the fact that successive multiples occupy predictable slots in the list of integers.

>In fact, building a highly optimized version of the sieve of Eratosthenes is not actually the main point here -- our purpose is to illustrate the power of **Star**’s collections processing functions.

We might ask whether the `sieve` function can also be expressed as a filter. The straightforward answer is that it cannot: the sieve *is* a kind of filter, but the predicate being applied depends on the entire collection; not on each element. The standard filter function does not expose the entire collection to the predicate. However, we will see at least one way of achieving the sieve without any explicit recursion below when we look at folding operations.

### Mapping to make new collections
One of the limitations of the filter function is that it does not create new elements: we can use it to subset collections but we cannot transform them into new ones. The `fmap` function -- part of the `functor` contract -- can be used to perform many transformations of collections.

For example, to compute the lengths of strings in a list we can use the expression:
```
fmap(size,list of ["alpha","beta","gamma"])
```
which results in the list:
```
list of [5,4,5]
```
The `fmap` function is defined via the `functor` contract -- thus allowing different implementations for different collection types:
```
contract all c/1 ~~ functor[c] ::= {
  fmap:all a,b ~~ ((a)=>b,c[a])=>c[b].
}
```
Notice how the contract specifies the collection type -- `c` -- without specifying the type of the collection’s element type. We are using a different technique here than we used for the `stream` and `filter` contracts. Instead of using a functional dependency to connect the type of the collection to the type of the element, we denote the type of the input and output collections using a *type constructor* variable as in `c[a]` and `c[b]`.[^This also means that the collection type in `fmap` must be generic: it is not possible to implement `functor` for strings.]

We are also using a variant of the quantifier. A quantified type variable of the form `c/1` denotes a type constructor variable rather than a regular type variable. In this case, `c/1` means that the variable `c` must be a type constructor that takes one argument.

The reason for this form of contract is that `functor` implies creating a new collection from an old collection; with a possibly different element type. This is only possible if the collection is generic and hence the type expressions `c[a]` for the second argument type of `fmap` and `c[b]` for its return type.

One might ask whether we could not have used functional dependencies in a similar way to stream and filterable; for example, a contract of the form:
```
contract all c,e,f ~~ mappable[c->>e,f] ::=  {
  mmap:((e)=>f,c)=>c.
}
```
However, *this* contract forces the types of the result of the `mmap` to be identical to its input type, it also allows the implementer of the `mappable` contract to fix the types of the collection elements -- not at all what we want from a `fmap`.

It is not all that common that we need to construct a list of sizes of strings. A much more realistic use of `fmap` is for *projection*. For example, if we wanted to compute the average age of a collection of people, which is characterized by the type definition:
```
person ::= someOne{
  name:string.
  age:()=>float.
}
```
Suppose that we already had a function `average` that could average a collection of numbers; but which (of course) does not understand people. We can use our average by first of all projecting out the ages and then applying the average function:
```
average(fmap((X)=>X.age(),People))
```
In this expression we project out from the `People` collection the ages of the people and then use that as input to the average function.

#### More Type Inference Magic
There is something a little magic about the lambda function in this expression: how does the type checker ‘know’ that `X` can have a field `age` in it? How much does the type checker know about types anyway?

In this particular situation the type checker could infer the type of the lambda via the linking between the type of the `fmap` function and the type of the `People` variable. However, the type checker is actually capable of giving a type to the lambda even without this context. Consider the function:
```
nameOf(R) => R.name
```
This function takes an arbitrary record as input and returns the value of the `name` field. The `nameOf` function *is* well typed, its type annotation just needs a slightly different form than that we have seen so far:[^Note that the **Star** type system will not _infer_ this generalized type.]
```
nameOf:all r,n ~~ r <~ {name:n} |: (r)=>n
```
This is another example of a *constrained type*: in this case, the constraint on `r` is that it has a field called `name` whose type is the same as that returned by `nameOf` itself.

The type constraint:
```
r <~ {name:n}
```
means that any type bound to `r` must have a `name` field whose type is denoted by the type variable `n` in this case.

With this type signature, we can use `nameOf` with any type that that a `name` field. This can be a record type; it can also be a type defined with an algebraic type definition that includes a record constructor.

### Compressing collections
Another way of using collections is to summarize or aggregate over them. For example, the `average` function computes a single number from an entire collection of numbers -- as do many of the other statistical functions. We can define average using the standard `foldLeft` function, which is part of the standard `folding` contract:
```
average(C) is foldLeft((+),0,C)::float/size(C)::float
```
>This definition of the `average` function is about as close to a specification of average as is possible in a programming language!

Notice the use of coercion here -- coercing both the result of the `foldLeft` and `size` to float. The reason for doing this is that functions like `average` are ‘naturally’ real functions.[^Real as in the ℝeal numbers.] Without the explicit coercion, averaging a list of integers will also result in an integer value -- which is likely to be inaccurate.

Of course, in our definition of average we need to coerce *both* the numerator and denominator of the division because **Star** does not have implicit coercion.

The `foldLeft` function applies a binary function to a collection: starting from the first element and successively ‘adding up’ each of the elements in the collection using the supplied operator.

![Left Folding a Collection][leftfold]

As we noted above, `foldLeft` is part of the `folding` contract. Like the `functor` contract, this uses some more subtle type constraints:
```
contract all c,e ~~ folding[c->>e] ::= {
  foldRight:all x ~~ (((e,x)=>x),x,c) => x.
  foldLeft:all x ~~ (((x,e)=>x),x,c) => x.
}
```
The `folding` contract uses quantifiers in two places: once in the contract specification and once in the type signature for `foldLeft` (and `foldRight`). What we are trying to express here is that any implementation of `folding` must allow for a generic function to process the collection.

The `foldLeft` (and `foldRight`) functions have an ‘accumulator’ (of type `x`) which need not be the same as the type of the elements of the collection.[^We saw something similar with the visitor pattern.] This argument acts as a kind of linking thread during the entire computation -- and represents the returned value when the fold is complete.

But we can do much more than computing averages with a fold. Recall that when we realized the sieve of Eratosthenes, we still had a recursive structure to the program. Furthermore, the way our original program was written each filter results in a new list of numbers being produced. Instead of doing this, we can construct a cascade of filter functions - each level in the cascade is responsible for eliminating multiples of a specific prime.

The complete cascade filters by checking each level of the cascade: for example, after encountering 3, 5 and 7, there will be a cascade of three functions that check each incoming number: one to look for multiples of 3, one for multiples of 5 and one for multiples of 7. When we encounter the next prime (11) then we glue on to the cascade a function to eliminate multiples of 11.

Consider the task of adding a filter to an existing cascade of filters. What is needed is a new function that combines the effect of the new filter with the old one. The `cascade` function takes a filter function and a prime as arguments and constructs a new function that checks both the prime *and* the existing filter:
```
cascade:((integer)=>boolean,integer)=>((integer)=>boolean).
cascade(F,K) => (X)=>F(X) && X%K=!=0.
```
>This is a truly higher-order function: it takes a function as argument and returns another function.

Given `cascade`, we can reformulate the `sieve` function itself as a `foldRight` -- at each new prime step we ‘accumulate’ a new cascaded filter function:
```
stp:(integer,(integer)=>boolean)=>((integer)=>boolean).
stp(X,F) where F(X) => cascade(F,X).
stp(X,F) => F.
```
At each step in the fold we want to know whether to continue to propagate the existing filter or whether to construct a new filter.

The `sieve` function itself is now very short: we simply invoke `foldRight` using `stp` and an initial ‘state’ consisting of a function that checks for odd numbers:
```
sieve(C) => foldRight(stp,(K)=>K%2=!=0,C).
```
This version of `sieve` is not quite satisfactory as, while it does find prime numbers, it does not report them. A more complete version has to also accumulate a list of primes that are found. We can do this by expanding the accumulated state to include both the cascaded filter function and the list of found primes. The main alteration is to the `step` function:
```
step:((list[integer],(integer)=>boolean),integer) => (list[integer],(integer)=>boolean).
step(X,(P,F)) where F(X) => ([P..,X],cascade(F,X)).
step(_,(P,F)) => (P,F).
```
and the initial state has an empty list:
```
sieve(C) is fst(foldRight(step,([],(K)=>K%2=!=0),C)).
```
where `fst` and `snd` are standard functions that pick the left and right hand sides of a tuple pair:
```
fst:all s,t ~~ ((s,t))=>s.
fst((L,R)) => L
snd:all s,t ~~ ((s,t))=>t.
snd((L,R)) => R
```
There is one final step we can make before leaving our sieve of Eratosthenes -- we can do something about the initial list of integers. As it stands, while the sieve program does not construct any intermediate lists of integers, it still requires an initial list of integers to filter. However, this particular sequence can be represented in a very compact form -- as a `range` term.

`range` terms are special forms of collections that denote ranges of numeric values. For example, the expression
```
range(0,100,2)
```
denotes the sequence of integers starting at zero, not including 100, each succesive integer being incremented by 2.

Using a similar `range` term, we can denote the list of primes less than 1000 with
```
primes(Max) => let{
  cascade:((integer)=>boolean,integer) => ((integer)=>boolean).
  cascade(F,K) => (X)=>(F(X) && X%K=!=0).

  step(X,(P,F)) where F(X) => ([P..,X],cascade(F,X)).
  step(_,(P,F)) => (P,F).

  sieve:(range[integer])=>list[integer].
  sieve(R) => fst(foldRight(step,([],(K)=>true),R)).
} in sieve(range(3,Max,2)).

show primes(1000)
```
This final program has an important property: there are no explicit recursions in it -- in addition, apart from the `foldRight` function, there are no recursive programs at all in the definition of `primes`.

>Of course, it still would not count as the _most efficient_ primes finding program; but that was not the goal of this discussion.

[folding]: folding.png width=332px height=204px

[leftfold]: leftfold.png width=376px height=173px

## Different types of collection
Just as there are many uses of collections, so there are different performance requirements for collections themselves. The most challenging aspects of implementing collections revolves around the cost of _adding_ to the collection, the cost of *accessing* elements of the collection and the cost of *modifying* elements in the collection.

There is a strong emphasis on *persistent* semantics for the types and functions that make up **Star**’s collections architecture. This is manifest in the fact, for example, that functions that add and remove elements from collections *do not* modify the original collection.

However, even without that constraint, different implementation techniques for collections tend to favor some operations at the cost of others. Hence, there are different types of collection that favor different patterns of use.

### The cons type

This is the simplest collection type; and is perhaps the original collection type used in functional programming languages. It is defined by the type declaration:
```
all t ~~ cons[t] ::= nil | cons(t,cons[t]).
```
Cons lists have the property that adding an element to the front of a list is a constant-time operation; similarly, splitting a `cons` list into its head and tail is also a constant time operation. However, almost every other operation is significantly more expensive: putting an element on to the end of a `cons` list is linear in the length of the list.

The main merit of the `cons` list is the sheer simplicity of its definition. Also, for small collections, its simple implementation may outweigh the advantages that more complex collections offer.

### The list type
The list type offers a different trade-off to the `cons` type: where the latter is optimal for ease of constructing and for traversing complete lists, the list type offers constant-time access to random elements within the array -- at the potential cost of more expensive construction of lists.

Unlike the `cons` type, the `list` type does not have a straightforward definition as an algebraic type. Internally, a list structure consists of an array of locations with a ‘control pointer’ giving the portion of the array block that represents a given list value. This is sometimes called a Copy on Write (COW) structure.

The `list` type is optimized for random access and for shared storage -- recall that **Star** collection types are persistent: that means that different values can share some or all of their internal structure. The diagram below shows two list values that overlap in their elements and consequently share some of their structure.

![Two lists Sharing Structure][twoarrays]

[twoarrays]: twoarrays.pdf width=311px height=143px

### The map type
Unlike the `cons` or `list` type, the `map` type is oriented for access by arbitrary keys. The `map` is also quite different to hash maps as found in Java (say), the `map` type is *persistent*: the functions that access dictionaries such as by adding or removing elements return new dictionaries rather than modifying a single shared structure. However, the efficiency of `map` is quite comparable to Java’s HashMap.

The template for the `map` type is:
```
all k,v ~~ equality[k], hash[k] |: map[k,v]
```
Notice that there is an implied constraint here: the `map` assumes that the keys in the map can be compared for equality, and that they are hashable -- have a `hash` function.

A `map` value can be written using the sequence notation, using tuple pairs for the key-value pairs:
```
map of [(1,"alpha"),(2,"beta”)]
```
As we saw before, `map`s also have a special variant of the sequence notation; instead of writing the pairs as tuples we can use an arrow notation for `map` terms:
```
map of [1->"alpha", 2->"beta"]
```
Maps also have their own special variant of a *query search condition*. A condition of the form
```
K->V in D
```
where D is a `map` will be satisfied if there is a key/value pair in D corresponding to K and V. For example, the condition:
```
K->V in map of [1->"alpha", 2->"beta"] && V=="alpha"
```
is satisfied for only one pair of `K` and `V`: namely `1` and `"alpha"` respectively.

For the curious, dictionaries are implemented using techniques similar to Ideal Hash Trees, as described by Bagwell [#Bagwell01idealhash]. This results in a structure with an effective O(1) cost for accessing elements *and* for modifying the `map` -- all the while offering an applicative data structure.

### The set type

There are many instances where a programmer needs a collection but does not wish specify any ordering or mapping relationship. The standard `set` type allows you to construct such entities.

Using a `set` type offers the programmer a signal that minimizes assumptions about the structures: the set type is not ordered, and offers no ordering guarantees. It does, however, offer a guarantee that operations such as element insertion, search and set operations like set union are implemented efficiently.

Like `map`, the `set` type is not publicly defined using an algebraic type definition: its implementation is private. It’s type is given by the template:
```
all t ~~ equality[t] |: set[t]
```

## Collecting it together

Collections form an important part of any modern programming language. The suite of features that make up the collections architecture in **Star** consists of a number of data types, contracts and special syntax that combine to significantly reduce the burden of the programmer.

The collections facility amounts to a form of DSL -- Domain Specific Language -- that is, in this case, built-in to the language. We shall see later on that, like many DSLs, this results in a pattern where there is a syntactic extension to the language that is backed by a suite of contracts that define the semantics of the DSL.
