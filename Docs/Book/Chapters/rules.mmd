# Making choices
One of the common themes of systems that 'face the world' is that they often have to make decisions. More specifically, any outward facing system will receive requests that the system must decide whether they are legitimate or not. Before acting on a request, we must decide whether or not to trust it.

In general, a response to an external event goes through a series of phases:

1. Taking _measurements_ of the event;
1. applying some form of _inference_ to the measurements to derive _facts_; and
1. based on certain criteria, taking _action_ in response to the event.

![A pipeline for making choices][Pipeline]

[Pipeline]:../images/pipeline.png 


There are many approaches to providing for such inference but one of the most robust is _rules_. A rule language, especially a logic rule language, is very well suited to expressing the logic (sic) of this kind of scenario. In particular, we can map measurements to _values_, facts to _assertions_, and we can realize decision-making in terms of _rules_.[^There are other approaches to classification; such as those based on Machine Learning.]

However, when _embedding_ a different semantics there are some usability issues to consider. Classical approaches to logic are not well suited to _all_ of the above stages: it can be very effective for describing inference but is typically not as effective for expressing action.

But, before we go too far, it may be worth digressing a little to explore what it means to ask a question -- to have queries. This will help to understand why we can benefit from going beyond what we have already seen in **Star** to address classification (and other) problems.

## Queries

Consider, if you will, the problem of finding a set of grandparent-grandchild pairs -- given information about parent-child relationships. For example, suppose that we had a list of pairs - each pair indicating a parent and child:
```
parents:list[(string,string)].
parents = [("john","peter), ("peter","jane"), ... ].
```
and that we wanted to construct a result list -- also of pairs -- along the lines of:
```
GC:list[(string,string)].
GC = [("john","jane"),...].
```
Computing the list of grandparent/grandchildren pairs involves searching the `parents` for pairs  that satisfy the grandparent relationship. This, in turn, involves a double iteration: each pair in the `parent`s list might represent the upper or lower half of a grandparent/grandchild relationship (or both).

Based on the collection operators we have seen so far, we can build such a search using two `foldLeft` operations:
```
foldLeft(
 (SoFar,(X,Z)) => foldLeft(
   let {
     acc(gp1,(ZZ,Y)) where Z==ZZ => [gp1..,(X,Y)].
     acc(gp1,_) => gp1.
   } in acc,
   SoFar,parents),
   list of [],
  parents)
```
This, rather intimidating,[^There are, unfortunately, some functional programmers that revel in complex code expressions like this one. We are not one of them!] expression uses one `foldLeft` to look for the grandparent, and the second `foldLeft` finds all the grand-children. All without any explicit recursion.

The `acc` function defined above in the `let` expression implements the logic of deciding what to accumulate depending on whether we had found a grandparent or not.

The various filter, `fmap` and `foldLeft` functions *are* powerful ways of processing entire collections. However, as we can see, domination iterations can be difficult to construct and harder to follow; something that is not helped by the occasional need to construct complex functions in the middle, as in this case.

What is needed is a way of expressing such complex query conditions in a way that can be _implemented_ using `foldLeft` expressions but which is easier to read.

Consider the conjunction:
```
(X,Z) in parents && (Z,Y) in parents
```
This could be read as specifying what it means to be grandparents:

>For which `X`, `Y` and `Z` can we assert that `X` is the parent of `Z`, and `Z` is the parent of `Y`?.

This should be significantly easier to follow than the complex expression above; and it is easier to be sure whether it is a correct representation of what it means to be a grand parent.

### Satisfaction semantics
Conditions like the ones above are boolean valued -- but they are not always expressions. For example, the first condition there being a parent:
```
(X,Z) in parents
```
is not evaluated in the way that expressions are normally evaluated -- by *testing* to see if a given pair of `X` and `Z` are in some `parents` collection. Instead, the condition needs to be evaluated by trying to find `X` and `Z` that are in the `parents` collection. In effect, the condition becomes a _search_ for suitable candidate pairs.

Technically this is called *satisfying* the condition -- to distinguish what is going on in normal expression *evaluation*. Of course, satisfying and evaluating are close cousins of each other and amount to the same thing when there is no search involved.

In addition to individual *search conditions* like this, it is also possible to use logical operators -- called *connectives* -- to combine conditions. In the case of our grand-parent query, there is a conjunction; which uses the variable `Z` to acts as a kind of glue to the two search conditions.

>In database query terminology, conjunctions like this one amount to _inner joins_. Languages like SQL have many operators and features to make expressing queries easier; the fundamental semantics of **Star**'s queries are similar in power to those of SQL.

The available connectives include the usual favorites: `&&`, `||`, and `\+`. We may also see the need for some less familiar connectives: `implies` and `otherwise`.

>An `implies` connective is a way of testing complete compliance with a condition; for example, we can define a query capturing the situation that a manager earns more than his/her members by requiring that anyone who works for the manager earns less than they do:
```
(X,M) in worksFor implies X.salary=<M.salary.
```


### Abstracting queries
The grandparent condition shows how to define what grandparent-hood means; but we also need ways of abstracting and naming such queries. The most straightforward way of this is to use a _query abstraction_ expression. For example, we can embody the grandparent situation in:
```
{ (X,Y) | (X,Z) in parents && (Z,Y) in parents}
```
The value of a query abstraction expression is typically a `set` -- in this case a set of the pairs of strings in the grandparent-grandchild relation.

Since it's an expression, we can assign it to a variable:
```
gp = { (X,Y) | (X,Z) in parents and (Z,Y) in parents}
```
and we can feed this set into another query:
```
(X,"f") in gp
```
and we can define functions whose values are the results of queries:
```
gpOf(GC) => {X | (X,GC) in gp}
```
We shall explore query expressions a little more, but first an editorial:

### A Splash of Inference

Queries have an important role in **Star** for two reasons: queries often have a much more obvious and transparent semantics than other programs -- even functional programs! Secondly, queries have a deep connection to logic; and logic is at the heart of many classification systems.

There are many reasons why one might be interested in logic; from a theoretical modeling perspective (does the universe follow rational rules[^Surprisingly, yes! Of course, discovering the rationality may be hard; but the immense success of Western thought was only possible because the universe is very rationally constructed.]) to the deeply pragmatic reason that logical programs are often easier to understand and therefore easier to trust.

The reason we promote a logic-based system for classifying is that it is easier to trust a classifier if you can understand its reasoning.

Like programming languages, it turns out that there are many kinds of logic. Again, like programming languages, there is a trade-off between expressivity and complexity. The primary source of complexity in a rule language is the machinery needed to realize it -- together with understanding it sufficiently to be able to predict the meaning of a written rule.

A, somewhat simplified, enumeration of the different kinds of logic might be:

* Propositional calculus. This is characterized by single-letter conditions (sometimes confusingly called _predicate variables_) and a guaranteed finite evaluation mechanism.
* Datalog. This is characterized by relations with simple unstructured values (i.e., strings and numbers). Execution in Datalog has similar performance characteristics as querying databases.
* First Order Predicate Calculus. This is probably the most well known and well understood logic. From an expressiveness point of view its focus is on the logical relationships amongst individual entities -- which includes things like trees, lists and so on. Inference in First Order has many of the same characteristics as program evaluation: not decidable in general but many effective sub-cases.
* Higher Order Predicate Calculus. There are actually many higher-order logics. The main expressive enhancement over First Order is that one can directly talk about relationships between entities as well as entities themselves. The cost of this is that inference becomes problematic -- even equality is undecidable.

Each of these levels represents a step both in expressiveness and in complexity. In general, the right logic for your application is something only you can decide; however, in designing a language, we have to choose for you.

In our view, there is a sweet spot between Datalog and First Order Logic. Datalog allows one to right rules (unlike pure SQL) but is not capable of handling arbitrary data structures. On the other hand, it may be that _recursion_ is something that we can do without -- as we have seen earlier, many well structured functional programs have no explicit recursion!

However, we must also be able to _embed_ our logic into our more regular programs. The key goal here is to maximize the benefit of providing a logical formalism whilst minimizing the burden on both the programmer and on the language implementation. This also recognizes that, while important, logical reasoning is typically only a small part of an overall system. It also recognizes the fact that gaps in the reasoning capability of a system can be patched more easily if the logic is simpler.

And so, in **Star**, we highlight the _query_ aspect of logical reasoning and bless queries as first class entities in the language. 

>Critically, queries have a _declarative_ semantics as well as a _programmatic_ one; this dual reading is essential if one is to be able to understand the reasoning.

>Historical note: in earlier iterations of the design for embedding logic into Star, a more-or-less complete rule system was envisaged. Such inference rules would have a similar status to functions and equations do. However, a combination of complexities and edge cases (such as how to handle a combination of inputs and outputs in rules) lead the designers to radically simplify the proposal and simply focus on queries. This gave us 90% of the potential benefit of inference rules at 10% of the cost.

Sometimes, a splash of logic is all we need. In terms of styles of logic, our approach is most reminiscent of _answer set programming_.

### Anatomy of a Query

A query can be seen as combining two elements: a _condition_ and an _answer template_. A query condition may be _satisfied_ in one or more ways -- each time potentially binding variables in the condition to values -- and the answer template encodes how we want to use the result of a successful satisfaction. Notice that the variables that are bound by the condition _are in scope_ within the answer template.

The syntax and style of **Star**’s query notation has strong echoes with SQL’s syntax -- deliberately so. Specifically, we take SQL’s *relational calculus* subset -- the language of wheres and of boolean combinations. **Star**’s query expressions do not have the equivalent of explicit relational join operators.

#### Query Conditions

The condition takes the form of a boolean combination of _predications_; a predication is either a normal boolean-valued expression, a _match_ condition, or a _search_ condition. Various types of boolean combinations are supported; the most common being conjunction (`&&`), disjunction (`||`) and negation (`\+`).

We have already seen match conditions; for example:
```
some(X) .= opValue
```
is a match condition that is satisfied if the value of `opValue` matches the pattern `some(X)`. A successful match has the additional effect of binding `X` to the value embedded in the `some` value.[^The form `X^=opValue` is actually a short form of the same condition.]

Where a match condition has at most one way of being satisfied, a search condition can potentially have many solutions. Search conditions look like:

_Pattern_ `in` _Expression_

We saw an example of this earlier in our grandparent query:
```
(X,Z) in parents
```
As should be anticipated at this point, search is realized via a contract. This allows us to search any type — so long as the `iterable` contract is implemented for that type. 

The final form of query condition is simply the boolean-valued expression. Note that, unlike the other forms of query condition, boolean expressions are test-only: they cannot result in bindings for query variables.

#### The `iterable` contract
The `iterable` contract is similar in intention to the `folding` contract we saw before. However, it is more tuned to supporting different kinds of combination of search.

The `iterable` contract looks like:
```
contract all s,e ~~ iterable[s->>e] ::= {
  _iterate:all r ~~ (s,(e,iterState[r])=>iterState[r],iterState[r]) => iterState[r].
}
```
This contract states that to search a collection, you have to be able to iterate over it. This function (and this contract) uses the `iterState` type to help guide the search:
```
all t ~~ iterState[t] ::= noneFound
                      | noMore(t)
                      | continueWith(t)
                      | abortIter(string).
```
The different cases in the `iterState` type codify different things that can happen during a search. The most commonly used case is the `continueWith` case — which is used to encapsulate the results found so far.

There are two companion contracts to the `iterable` contract: the `indexed_iterable` contract supports search over key/value pairs and the `generator` contract is used to help construct answers.

#### Answer Templates
The result of a query is governed by the _answer template_ of the query abstraction. There are two main forms of answer template: the expression template and the fold template.

ExpressionTemplate
: A expression template is simply an expression. 

The expression template is evaluated for each successful way that the query condition can be satisfied; typically, there are free variables in the template expression that refer to variables bound in the query condition. That way. values found during the search can be extracted and made part of the overall answer. 

Fold Template
: A fold template is used when it is desired to aggregate over the found solutions. The form of a fold template is:

```
fold Fn with Exp
``` 

### Assertions, facts and sets
An assertion is a statement that something is true -- i.e., it is a fact.

>There is a subtle distinction between something being true, and stating that something is true.

For us, taking the same minimalist route we took for queries, we represent facts using `set`s; typically sets of tuples. We already saw this when we encoded the grandparent example as a set of pairs (aka binary tuples).

## Classifying
Recall that we asserted that classification is a key part of any outward facing system: the system has to decide how to act on an incoming request. It may be instructive to see just how the query formalism can be used to help.

Let us imagine a system that has some external face, for example, a server that allows users to fetch and store documents. We have a business requirement to allow anyone to store and fetch documents; however, we also reserve the right to not process bad documents.[^However that is defined!]

Since we anticipate enormous success for our service, and we also anticipate that some people will try to game our system in order to further their own nefarious goals. So, we have to put in place  a classification system that can decide how to process requests and which we anticipate will need continuous evolution.

The simplest online processing system can be viewed as having four components: a source of events, a processor, a sink where the output of the system is targeted and a knowledge base that is used to inform the processor.

![A Simple Event Processor][Events]

[Events]:../images/events.pdf

Classification always depends on a combination of information gleaned from the event itself and on global or contextual information. In our case, we can usefully breakdown the contextual knowledge into two kinds: _rules_ that embody our processing policy and _reputation_ data that the system has collected about external entities.


![Rules and Reputation][RuleRepu]

[RuleRepu]:../images/rules-repu.pdf

We distinguish these two sources of knowledge, partly because the way that we collect and use them are different; but mainly because they are about different things. However, we can use our query formalisms — together with functions — for both.


### Query quantifiers



### Querying for statistical purposes

We wrap up our exposition on collections with an example that highlights how we can combine many of the collection manipulation features; with a specific goal of statistical processing of data.

Statistics is, of course, one of the key application areas of computers in general. However, there is often a substantial gap between the theoretical aspects of statistical processing and the pragmatics of collecting and processing data. We aim to demonstrate **Star**’s power in both areas.

One fecund source of statistics is the web; we can even get statistics about the web. The on-line tool [Web Page Test](http://www.webpagetest.org/) can be used to generate a lot of data about how a browser responds to a website. Furthermore, we can down this data as a file of comma separated values (CSV). The first few lines of this file shows the detail of the data collected:

Date,...,URL,Response Code,Time to Load (ms),...,Bytes In,...

9/12/14,...,/,302,397,...1272,...

There are over 70 columns there. Suppose that we wanted to process this to find out how much time is spent loading Javascript data. Our first step is to introduce the CSV file to **Star** which we can do with a special macro:

import metaCSV.

worksheet{

generateCSV Wpt from "file:WPT_Sample.csv".

...

}

The generateCSV line is a macro that parses the sample CSV file, constructs a type (Wpt) that reflects the entries in the file and a parser that can parse similar CSV files into Wpt entries. The generated Wpt type looks like:

Wpt ::= Wpt{

Date:string.

...

URL:string.

'Response Code':integer.

'Time to Load (ms)':integer.

...

'Bytes In':integer.

...

};

One immediate feature to notice is that some of the field names are quoted. **Star** allows a variable (and by extension a field) identifier to have any characters in them -- so long as the identifier is quoted. This allows us to access ‘foreign’ data structures like this CSV file in a straightforward way.

Given the implicit generation of the type, and of a parser, we can use this to process real data files. For example, we can extract out from the CSV file records containing just the URLs, the file sizes and the load times using a query:

import metaCSV.

worksheet{

generateCSV Wpt from "file:WPT_Sample.csv".

wptData = WptParser("http:www.webpagetool.org/...csv").

extracted = list of { all

{ URL=D.URL

size=D.'Bytes In'

loadTime=D.'Time to Load (ms)'} where

D in wptData}

...

}

We used the generated parser to reach out to the website for the data and to parse the resulting content -- in a single high-powered statement.

A single run of the web page test may involve many separate browser actions -- the purpose of the tool is to test the performance of a website, which needs many trials to achieve statistical significance. So a better organization of the data is to categorize the raw data by URL. We can do this using a group by query:

import metaCSV.

worksheet{

generateCSV Wpt from "file:WPT_Sample.csv".

wptData = WptParser("http:www.webpagetool.org/...csv").

extracted = list of { all

{ URL=D.URL.

size=D.'Bytes In'.

loadTime=D.'Time to Load (ms)'} where

D in wptData} group by ((X)=>X.URL)

...

}

The group by operator takes a collection, and a categorization function, are produces a `map` of collections.

We want to produce average load times and standard deviations of the load times -- to smooth out the vagaries of the Internet. For now, we will assume that we have average and stddev functions that have type signatures:

average:all c,e ~~ stream[c->>e] |:(c,(e=>float))=>float.

stddev:all c,e ~~ stream[c->>e] |:(c,(e=>float))=>float.

I.e., we are assuming functions that take collections, and an ‘accessor’ function, and return the average and standard deviation respectively.

Given these functions, we can compute our statistics using:

import metaCSV.

import stats.

worksheet{

generateCSV Wpt from "file:WPT_Sample.csv".

wptData = WptParser("http:www.webpagetool.org/...csv").

extracted = list of { all

{ URL=D.URL.

size=D.'Bytes In'.

loadTime=D.'Time to Load (ms)'} where

D in wptData} group by ((X)=>X.URL)

ldTime:(Wpt)=>float.

ldTime(D) => D.loadTime::float.

show list of { all (K,average(V,ldTime),stddev(V,ldTime)) where

K->V in extracted }

}

The query condition:

K->V in extracted

is analogous to the regular search condition:

```
D in wptData
```
except that it is used to search within a `map`-based collection. The pattern K->V matches the successive key/value pairs in the `map`.

That is it! The final query should get output along the lines of:

```
...
("/o/oauth2/auth?...", 337.0, 0.0),
("/main-thumb-58380181-100-yqpttun...", 101.18518518519, 24.41468441456),
("/main-thumb-49759239-100-pcgldxn...",93.48148148148, 19.03285636867),
...
```

Notice that most of the remaining complexity in this example related to selecting the right parts of the data to pick up and process.[^fn1]

[^fn1]: A standard deviation of 0.0 is likely a signal that the indicated URL only occurred once in the sample data.
