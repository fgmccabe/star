@node Functional Programming
@chapter Functional Programming

There is a perception of functional programming that it is
@emph{weird} and @emph{difficult}. This is unfortunate for a number of
reasons; the most important being that functional programming is
@emph{not} weirder than procedural programming and that all
programmers can benefit by programming functionally.

As for being difficult, a more accurate description would be that
there is a deeper @emph{range of features} in functional programming
than in most modern programming languages: so a perception of
complexity can arise simply because there is a lot more to say about
functional programming languages. However, the simplest aspects of
functional programming are very simple and the ramp need not be steep.

What may be surprising to the reader who is not familiar with
functional programming is that it is @emph{old}: predating the origins
of modern computing itself, that there is a huge amount that can be
expressed functionally, and that functional programming is often at
least as efficient and sometimes more efficient than procedural
programming.

Functional programming is also @emph{new} in a way that may be
surprising. As a paradigm, functional programming encourages a
different approach to programming. In particular, for many functional
programmers, the focus is on @emph{data transformation} rather than
@emph{making recipies}.

In recent years some extremeley impressive advances have been made on
@emph{how to program}; in particular the use of monads and related
concepts represent quite advanced programming
techniques.@footnote{While we will explore monads, we will likely not
touch on other more advanced topics in this book.}

This leads to an emphasis on @emph{composition} and @emph{generality}
-- often over optimization of individual elements. We trust the
compiler's optimization strategies to pull the rabbit out of the hat
when it comes to performance.

In this chapter we will show how we can utilize @Star{} as a vehicle
for functional programming. As a side-goal, we also hope to demystify
some of the language and ideas found in functional programming.

@node What is functional programming?
@section What is functional programming?

The foundations of functional programming rest on two principles:

@enumerate
@item
Programs are expressed in terms of functions, where a somewhat
mathematical view of functions is taken; i.e., functions always
produce the same output for the same input.

This is what people mean when they say that functional programs are
@emph{declarative}.@footnote{The term `declarative' has a technical
definition; but this captures much of the essence of declarativeness
(sic).}

@item
Functions are values: they can be passed as arguments to functions,
returned from functions, and they can be put into and retrieved from
data structures.
@end enumerate

This last principle is what people mean when they refer to functions
as being @emph{first class} values. It is also what is meant when we
say that functional programming languages are @emph{higher order}.

It is worth asking why these two principles are so important. Although
most early programmers were mathematicians; the extreme constraints
imposed by early machines meant those early programs were far from
functional in style.

However, almost by accident, programming and mathematics share some
attributes: the importance of @emph{composition} and the power of
@emph{abstraction}. Functional programming is almost ideally placed to
exploit both of these to the full.

@Star{} is not actually a @emph{pure} functional programming language;
i.e., it is possible to write programs that violate the declarative
principle. However, it is a @emph{functional-first} programming language: a
declarative style is strongly encouraged. In this book we shall mostly
focus on writing pure functional programs.

@node Basics
@section Basics

It is often easier to introduce functional programming using numerical
examples. Last chapter we saw, for example, the factorial
program. This is mostly because most programmers are already familiar
with numbers. Continuing that tradition, here is a function that
returns the sign of a number:

@example
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) where X>0 => 1.
@end example

@noindent
Each of these equations applies to different situations: the first
equation applies when the input argument is negative, the second when
it is exactly zero and the third when it is strictly positive. These
represent the three possible cases in the definition of the sign
function.

A function may be built from any number of rewrite equations; however,
they must all be contiguous within the same group of statements.

Although it is good practice to ensure that equations in a function
definition do not overlap, the equations in a function definition are
tried in the order they are written in.@footnote{The @emph{Church
Rosser Theorem} guarantees some independence on the order of the
rewrite equations provided that the different rewrite equations that
make up function definitions do not overlap. Usually, however, it is
too fussy to @emph{require} programmers to ensure that their equations
do not overlap; hence the reliance on ordering of equations.} We could
have relied on this and written `sign` using:

@example
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) => 1.
@end example

There is something somewhat unsatisfying about that third equation --
it is not actually always true -- and it depends on the previous
equations not applying for its meaning. We address this by using the
concept of a @emph{default} case: i.e., an equation that should be
used if none of the other cases apply:

@example
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) default => 1.
@end example
An explicitly marked `default` equation does not need to be the last
equation; but, wherever it is written, default equations are only
attempted after all other equations have failed to apply.

@node Functions
@subsection Functions
A function is defined as a sequence of @emph{rewrite equations} each
of which consist of a @emph{pattern} and an @emph{expression}. There
are three general forms of rewrite equations:

@example
@var{Pattern} => @var{Expression}
@end example

or
@example
@var{Pattern} where @var{Condition} => @var{Expression}
@end example

or
@example
@var{Pattern} default => @var{Expression}
@end example

@noindent
The left hand side of a rewrite equation consists of a pattern which
determines the applicability of the equation; and the right hand side
represents the value of the function if the pattern matches.

@quotation Pattern
A pattern represents a test or guard on an (implicit) value. Patterns
can be said to succeed or fail depending on whether the value being
tested matches the pattern.
@end quotation

We also refer to a pattern being @emph{satisfied} when matching a
value.@footnote{This terminology originates from Logic -- where a
formula can be satisfied (made true) by observations from the world.}

The pattern in a rewrite equation is a guard on the arguments of the
function call. For example, given a call
@example
sign(34)
@end example
the patterns in the different equations of the @code{sign} function
will be applied to the integer value @code{34}.

When the pattern on the left hand side of a rewrite equation succeeds
then the equation @emph{fires} and the value of the expression on the
right hand side of the equation becomes the value of the function.

There are many kinds of pattern; here, we look at three of the most
common kinds of pattern, and in later sections, we look at additional
forms of patterns.

@table @emph
@item Variable Pattern
A variable pattern is denoted by an identifier; specifically by the
first occurrence of an identifier.
@end table

A variable pattern always succeeds and has the additional effect of
@emph{binding} the variable to the value being matched.

For example, the @code{X} in the left hand side of
@example
double(X) => X+X
@end example
is a variable pattern. Binding @code{X} means that it is available for
use in the right hand side of the equation -- here to participate in
the expression @code{X+X}.

The part of the program that a variable has value is called its @emph{scope}.

@itemize
@item
Variables in rewrite equations always have scope ranging from the
initial occurrence of the variable through to the whole of the right
hand side of the equation.
@item
Variable patterns are the @emph{only} way that a variable can get a
value.
@end itemize

Subsequent occurrences of variables in a pattern are semantically
equivalent to an equality test; specifically a call to the @code{==}
predicate. For example, in the equation:

@example
same(X,X) => true.
@end example
the second occurrence of @code{X} is regarded as an equality test;
i.e., this equation is equivalent to:

@example
same(X,X1 where X==X1) => true.
@end example

Sometimes, the earlier occurrence of a variable is not in the pattern
itself but in an outer scope.

@table @emph
@item Literal Pattern
A literal pattern -- such as a numeric literal or a string literal --
only matches the identical number or string.
@end table

Clearly, a literal match amounts to a comparison of two values: the
pattern match succeeds if they are identical and fails otherwise.

Equality is based on @emph{semantic equality} rather than
@emph{reference equality}. What this means, for example, is that two
strings are equal if they have the same sequence of characters in
them, not just if they are the same object in memory.

@quotation NOTE
not all types admit equality: for example, functions are not
comparable; similarly, implementing equality for circular structures
is problematic.@footnote{None of the standard data types are circular
in nature.}
@end quotation

There is no automatic coercion of values to see if they @emph{might}
match. In particular, an integer pattern will only match an integer
value and will not match a float value -- even if the numerical values
are the same. I.e., there will be no attempt made to coerce either the
pattern or the value to fit.

This, too, is based on the desire to avoid hard-to-detect bugs from
leaking into a program.

@table @emph
@item Guard Pattern
Sometimes known as a @emph{semantic guard}, a guard pattern consists
of a pair of a pattern and a condition:

@example
@var{Pattern} where @var{Condition}
@end example
@end table

A guard succeeds if both its pattern matches and if its condition is
@emph{satisfied}.

@quotation
Note that conditions @emph{may} bind variables. This is why we do not state
that conditions are boolean-valued expressions.
@end quotation

There is a normal complement of special conditional forms which we
shall explore as we encounter the need. In the case of the equation:
@example
sign(X) where X>0
@end example
the guard pattern is equivalent to:
@example
X where X>0
@end example
We can put guard pattern anywhere that a pattern is valid; and, for
convenience, we can also put them immediately to the left of the
rewrite equation's @code{=>} operator.

@quotation
Any variables that are bound by the pattern part of a guarded pattern
are @emph{in scope} in the condition part of the guard. Furthermore,
any variables that are bound by the condition part of the guarded
pattern have the same scope as variables introduced in the pattern.
@end quotation

In the pattern above, the variable @code{X} will be bound in the
variable pattern @code{X} and will then be tested by evaluating the
condition @code{X>0}.

@node Order of evaluation
@subsection Order of evaluation
@Star{} is a so-called @emph{strict} language. What that means is that
arguments to functions are evaluated prior to calling the function.

@quotation
We do not specify the order of evaluation of the arguments of the
function; except that:
@itemize
@item
Arguments are evaluated before entry to the function.
@item
The function is evaluated before entry to the function.
@end itemize
@end quotation

Most programming languages are strict; for two main reasons:

@enumerate
@item
It is significantly easier for programmers to predict the evaluation
characteristics of a strict language.
@item
It is also easier to implement a strict language efficiently on modern
hardware. Suffice it to say that modern hardware was designed for
evaluating strict languages, so this argument is somewhat circular.
@end enumerate

There many possible styles of evaluation order; one of the great
merits of programming declaratively is that the order of evaluation
does not affect the actual results of the computation.

It may, however, affect whether you get a result. Different strategies
for evaluating expressions can easily lead to differences in which
programs terminate and which do not.

One other kind of evaluation that is often considered is @emph{lazy}
evaluation. Lazy evaluation means simply that expressions are only
evaluated @emph{when needed}. Lazy evaluation has many potential
benefits: it certainly enables some very elegant programming
techniques.

@quotation
The modern name for lazy evaluation in functional programming
languages is @emph{normal order evaluation}. This evaluation order
amounts to evaluating all expressions in a strictly left-to-right
order. Surprisingly, this amounts to a form of lazy evaluation because
the function is evaluated before its arguments.
@end quotation

Essentially for the reasons noted above, @Star{} does not use lazy
evaluation; however, as we shall see, there are features that allow us
to recover some of the power of lazy evaluation.@footnote{Even
predominantly lazy languages like Haskell have features which enforce
strict evaluation. It reduces to a question of which is the
@emph{default} evaluation style.}

The other dimension in evaluation order relates to the rewrite
equations used to define functions. Here, @Star{} uses an in-order
evaluation strategy: the equations that make up the definition of a
function are tried in the order that they are written -- with the one
exception being any default equation which is always tried last.

@node Another look at types
@section Another look at types
Organizing data is fundamental to any programming language. @Star{}'s
data types are organized around the algebraic data type.

@node Quantifier types
@subsection Quantifier types

A @emph{generic} type is one which has one or more type variables in
it. For example, the type expression:
@example
(x,x)=>boolean
@end example
is such a generic type (assuming that @code{x} is a type variable --
see below).

All type variables must be bound by a quantifier in some enclosing
scope. If a type variable is not bound within a particular type
expression, it is considered @emph{free} in that type expression.

A @emph{quantified type} is a type that introduces (i.e., binds) a
type variable. There are two quantifiers: a universal quantifier and
an existential quantifier.

The most common quantifier is the @emph{universal quantifier} and
universally quantified types correspond closely to generic types in
other languages.

Universally quantified types are often used to denote function types
and collection types. For example, the type
@example
all x ~~ (x,x)=>boolean
@end example
denotes the type of a generic binary function that returns a boolean
value. The standard type for the equality predicate @code{==} is
similar to this type.

A universally quantified type should be read as `for all possible
values' of the bound variable. For example, this function type should
be read as denoting functions that:

@quotation
for any possible type -- @code{x} -- the function takes two such
@code{x}'s and returns a @code{boolean}.
@end quotation

@Star{} also supports @emph{existentially} quantified types -- these
are useful for denoting the types of modules and/or abstract data
types. For example, the type expression:

@example
exists e ~~ @{ f1 : e @}
@end example
denotes the type of a record with a field -- @code{f1} -- which has a
type. The analogous reading for this type expression would be:

@quotation
there is a type -- @code{x} -- such that the record has a field --
called @code{f1} -- of this type.
@end quotation

As may be guessed, this is kind of obscure: @code{f1} has a type, but
we don't know much else about it!

However, existential types can be very useful, especially in
combination with record types like this one. We will leave our more
detailed exploration of existential types for later.

@node Contract constrained types
@subsection Contract constrained types
We noted above that the type of the standard equality predicate was
almost the same as:
@example
all x ~~ (x,x)=>boolean
@end example
This type denotes a universally quantified function type that can be
applied to arguments of any given type. However, equality in a normal
programming language is @emph{not} universal: not all values admit to
being reliably tested for equality. A great example of such a
limitation are functions -- equality between functions is not well
defined.@footnote{Strictly, equality of functions is not decidable.}

To capture this, we need to be able to constrain the scope of the
quantifier; specifically to those argument types that do admit
equality.

We can do this by adding a @emph{contract constraint} to the type --
the constraint states that equality must be defined for the type. We
do this by prepending an @code{equality} constraint clause to the
type:
@example
all x ~~ equality[x] |: (x,x)=>boolean
@end example
The @code{equality[x] |:} clause states that the type variable
@code{x} must satisfy the @code{equality} contract.

What is implied when we state this? This is captured in the definition
of the contract itself, in this case:
@example
contract all t ~~ equality[t] ::= @{
  (==) : (t,t) => boolean.
@}
@end example
In effect, the @code{equality} contract states that there must be a
single function defined -- @code{==} -- that is defined for any type that
claims to satisfy the @code{equality} contract.

If this seems a little circular, it is not. The @code{equality}
contract is effectively saying that the @code{==} function must be
defined for the type; and we constrain function (and other) types with
the @code{equality} contract constraint when we need to ensure that
@code{==} is defined.

We provide evidence for contracts through the @code{implementation}
statement. This declares that a given type satisfies a contract by
providing implementations for the functions in the contract.

For example, we can provide evidence that the @code{equality} contract
applies to strings using a built-in primitive to actually implement
equality for strings:
@example
implementation equality[string] => @{
  s1 == s2 => _string_eq(s1,s2).
@}
@end example
We shall explore more fully the power of this form of type constraint
in later sections and chapters. For now, the core concept is that
quantified types can be constrained to allow very precise formulations
of types.

@node Algebraic data types
@subsection Algebraic data types
An algebraic data type definition achieves several things
simultaneously: it introduces a new type into scope, it gives an
enumeration of the legal values of the new type and it defines both
constructors for the values and it defines patterns for decomposing
values. This is a lot for a single statement to do!

For example, we can define a type that denotes a point in a
two-dimensional space:
@example
point ::= cart(float,float).
@end example
This kind of statement is called a @emph{type definition statement}
and is legal in the same places that a function definition is legal.

The new type that is named by this statement is point; so, a variable
may have point type, we can pass point values in functions and so on.

The constructor cart allows us to have expression that allow new
point structures to be made:
@example
cart(3.4,2.1)
@end example
@code{cart} is also the name of a @emph{pattern} operator that we can
use to take apart point values. For example, the euclid function
computes the Euclidian distance associated with a point:
@example
euclid:(point)=>float.
euclid(cart(X,Y)) => sqrt(X*X+Y*Y).
@end example
Of course, this particular @code{point} type is based on the
assumption that point values are represented in a cartesian coordinate
system. One of the more powerful aspects of algebraic data types is
that it is easy to introduce multiple alternate forms of data. For
example, we might want to support two forms of point: in cartesian
coordinates and in polar coordinates. We can do this by introducing
another case in the type definition statement:
@example
point ::= cart(float,float)
        | polar(float,float).
@end example
Of course, our @code{euclid} function also needs updating with the new
case:
@example
euclid:(point)=>float.
euclid(cart(X,Y)) => sqrt(X*X+Y*Y).
euclid(polar(R,T)) => R.
@end example
@code{cart} and @code{polar} are called @emph{constructor
functions}. The term @emph{constructor} refers to the common
programming concept of constructing data structures. They are called
functions because, logically, they @emph{are} functions.

For example, we might give a type to @code{polar}:
@example
polar : (float,float)=>point
@end example
In fact, constructor functions are one-to-one functions. Variously
known as @emph{free functions} (in logic), @emph{bijections} (in
Math), one-to-one functions are guaranteed to have an inverse. This is
the logical property that makes constructor functions useful for
representing data.

Of course, we are talking of a @emph{logical} property of constructor
functions. Internally, when implementing functional languages, the
values returned by constructor functions are represented using data
laid out in memory -- just like in any other programming language.

Constructor functions have a special type; so the correct type of
@code{polar} is given by:
@example
polar : (float,float)<=>point
@end example
The double arrow representing the fact that constructor functions are
bijections.

In addition to constructor functions, an algebraic type definition can
introduce two other forms of data: @emph{enumerated symbols} and
@emph{record functions}. Enumerated symbols are quite useful in
representing symbolic alternatives. The classic example of an
enumerated type is @code{daysOfWeek}:
@example
daysOfWeek ::= .monday
             | .tuesday
             | .wednesday
             | .thursday
             | .friday
             | .saturday
             | .sunday.
@end example
Another example is the standard @code{boolean} type which is defined:
@example
boolean ::= .true | .false.
@end example
Unlike enumerated symbols in some languages, there is no numeric value
associated with an enumeration symbol: an enumerated symbol `stands
for' itself only. The reason for this will become clear in our next
type definition which mixes enumerated symbols with constructor
functions:
@example
sTree ::= .sEmpty | sNode(sTree,string,sTree)
@end example
In addition to mixing the enumerated symbol (@code{.sEmpty}) with the
@code{sNode} constructor, this type is @emph{recursive}: in fact, this
is a classic binary tree type where the labels of the non-empty nodes
are strings. (We shall see shortly how to generalize this).

Whenever you have a recursive type, its definition must always include
one or more cases that are not recursive and which can form the base
case(s). In that sense, an enumerated symbol like @code{.sEmpty} plays
a similar role that null does in other languages; except that
@code{.sEmpty} is only associated with the sTree type.

We can use @code{sTree} to construct binary trees of string value; for
example:
@example
sNode(sNode(.sEmpty,"alpha",.sEmpty),
      "beta",
      sNode(.sEmpty,"gamma",.sEmpty))
@end example
denotes the tree in:

@float Figure,binaryStringTree
@caption{A Binary string Tree}
@image{images/sTree}
@end float

@quotation
One of the hallmarks of languages like @Star{} is that @emph{every}
value has a legal syntax: it is possible to construct an expression
that denotes a literal value of any type.
@end quotation

Just as we can define @code{sTree} values, so we can also define
functions over @code{sTree}s. For example, the @code{check} function
returns @code{true} if a given tree contains a particular search term:
@example
check(.sEmpty,_) => .false.
check(sNode(L,Lb,R),S) => Lb==S || check(L,S) || check(R,S).
@end example
Here we see several new aspects of @Star{} syntax:

@itemize
@item
An empty pattern -- marked by @code{_} -- matches anything. It is
called the @emph{anonymous pattern} and is used whenever we don't care
about the actual content of the data.
@item
The @code{||} disjunction is a @emph{short-circuit} disjunction; much
like @code{||} in languages like Java. Similarly, conjunction
(@code{&&}) is also short-circuiting.
@item
Functions can be recursive; including @emph{mutually recursive}: there
is no special requirement to order function definitions in a program.
@end itemize

We can use @code{sTest} to check for the occurrence of particular
strings:
@example
T .= sNode(sNode(.sEmpty,"alpha",.sEmpty),
          "beta",
          sNode(.sEmpty,"gamma",.sEmpty));

show check(T,"alpha");          -- results in true
show check(T,"delta");          -- results in false
@end example

@node Functions as values
@section Functions as values
The second principle of functional programming is that functions are
first class. What that means is that we can have functions that are
bound to variables, passed into functions and returned as the values
of functions. In effect, a function is a legal @emph{expression} in
the language. It also means that we can have @emph{function types} in
addition to having types about data.

We can see this best by looking at a few examples. One of the benefits
of passing functions as arguments to other functions is that it makes
certain kinds of parameterization easy. For example, suppose that you
wanted to generalize @code{check} to apply an arbitrary test to each
node -- rather than just looking for a particular string.

We will first of all define our @code{fTest} function itself:
@example
fTest:(sTree,(string)=>boolean)=>boolean.
fTest(.sEmpty,_) => .false.
fTest(sNode(L,Lb,R),F) => F(Lb) || fTest(L,F) || fTest(R,F).
@end example
The substantial change here is that, rather than passing a string to
look for, we pass @code{fTest} a boolean-valued function to apply;
within @code{fTest} we replace the equality test @code{Lb==S} with a
call @code{F(Lb)}.

Notice that the type annotation for @code{fTest} shows that the type
of the second argument is a function type -- from @code{string} to
@code{boolean}.

Given @code{fTest}, we can redefine our earlier @code{check} function
with:
@example
check(T,S) => fTest(T,(X)=>X==S).
@end example
We have a new form of expression here: the @emph{anonymous function}
or @emph{lambda expression}. The expression
@example
(X)=>X==S
@end example
denotes a function of one argument, which returns @code{true} if its
argument is the same value as @code{S}.

Interestingly, it would be difficult to define a top-level function
that is equivalent to this lambda because of the occurrence of the
variable @code{S} in the body of the lambda. This is an example of a
@emph{free variable}: a variable that is mentioned in the body of a
function but which is defined outside the function. Because @code{S}
is free, because it is not mentioned in the arguments, one cannot
simply have a function which is equivalent to the lambda as a
top-level function.

Free variables are a powerful feature of functional programming
languages because they have an @emph{encapsulating} effect: in this
case the lambda encapsulates the free variable so that the
@code{fTest} function does not need knowledge of @code{S}.

@node Functions and closures
@subsection Functions and closures
If a function is an expression, what is the value of the function
expression? The conventional name for this value is @emph{closure}:

@table @emph
@item Closure
A closure is a structure that is the value of a function expression
and which may be applied to arguments.
@end table

It is important to note that, as a programmer, you will never `see' a
closure in your program. It is an implementation artifact in the same
way that the representation of floating point numbers is an
implementation artifact that allow computers to represent fractional
numbers but which programmers (almost) never see explicitly in
programs.

Pragmatically, one of the important roles of closures is to capture
any free variables that occur in the function. Most functional
programming languages implement functions using closure
structures. Most functional programming languages do not permit direct
manipulation of the closure structure: the only thing that you can
@emph{do} with a closure structure is to use it as a function.

@quotation
In the world of programming languages, there is a lot of confusion
about closures. Sometimes you will see a closure referring to a
function that captures one or more free variables.
@end quotation

@node Let binding
@subsection Let binding

We noted that it is difficult to achieve the effect of the
@code{(X)=>X==S} lambda expression with named functions. The reason is
that the lambda is @emph{not} defined in the same way that named
functions are defined -- because it occurs as an expression not as a
statement. If we wanted to define a named function which also captures
@code{S}, we would have to be able to define functions inside
expressions.

There is an expression that allows us to do this: the @code{let}
expression. A @code{let} expression allows us to introduce local
definitions anywhere within an expression. We can define our lambda as
the named function @code{isS} using the @code{let} expression:
@example
let@{
  isS(X) => X==S
@} in isS
@end example
The region between the braces is a @emph{definition environment} and
@emph{any} definition statement may be in such an environment. We can
define check using a @code{let} expression:
@example
check(T,S) => fTest(T,
    let@{
      isS(X) => X==S.
    @} in isS)
@end example
This is a somewhat long-winded way of achieving what we did with the
anonymous lambda function -- we would not normally recommend this way
of writing the @code{check} function as it is significantly more
complicated than our earlier version. However, there is a strong
inter-relationship between anonymous lambdas, let expressions and
variable definitions. In particular, these are equivalent:
@example
let@{
  isS = (X) => X==S
@} in isS
@end example
and
@example
(X)=>X==S
@end example
Apart from being long-winded, the @code{let} expression is
significantly more flexible than a simple lambda. It is much easier
within a @code{let} expression to define functions with more than one
rewrite equation; or to define multiple functions. We can even define
types within the let binding environment.

Conversely, lambda functions are so compact because they have strong
limitations: you cannot easily define a multi-rewrite equation
function with a lambda and you cannot easily define a recursive
function as a lambda.

In short, we would use a @code{let} expression when the function being
defined is at all complex; and we would use a lambda when the function
being defined is simple and small.

@node Non-recursive Let Binding
@subsubsection Non-recursive Let Binding

@noindent
Assembling functions in this way, either by using anonymous lambdas or
by using @code{let} expressions, is one of the hallmarks of functional
programming.

@Star{} actually has @emph{two} forms of let expressions -- the one we
have used so far allows function definitions to be recursive -- and
mutually recursive.

Sometimes, it is useful to have let expressions which are not
recursive. For that we use a special form of brace notation:
@example
let@{.
  isS = (X) => X==S
.@} in isS
@end example

The extra periods signal that the definitions within the @emph{let
environment} may not refer to each other. While that may seem to be
inconvenient, in fact, there are scenarios when this is more
convenient; we will see this more when we look more closely at
contracts.

@node Generic types
@subsection Generic types

What actually makes @code{fTest} above more constrained than it could
be is the type definition of @code{sTree} itself. It too is
unnecessarily restrictive: why not allow trees of any type? We can,
using the type definition for @code{tree}:
@example
all t ~~ tree[t] ::= .tEmpty | tNode(tree[t],t,tree[t]).
@end example
Like the original @code{sTree} type definition, this statement
introduces a new type: @code{tree[t]} which can be read as @emph{tree
of something}. The name @code{tree} is not actually a type identifier
-- although we often refer to the tree type -- but it is a @emph{type
constructor}.

In an analogous fashion to constructor functions, a type constructor
constructs types from other types. Type constructors are even
bijections -- one-to-one functions from types to types.

@quotation
Unlike constructor functions though, type functions play no part in
the run-time evaluation of programs.
@end quotation

The identifier @code{t} in the type definition for @code{tree} denotes
a @emph{type variable}. Again, similarly to regular variables and
parameters, a type variable denotes a single unspecified type. The
role of the type variable @code{t} is like a parameter in a function:
it identifies the unknown type and its role.

The @code{tree} type is a universally quantified type. What that means
is that instead of defining a single type it defines a family of
related types: for example:
@example
tree[string]
tree[integer]
@dots{}
@end example

are @code{tree} types. We can even have trees of trees:
@example
tree[tree[string]]
@end example
We capture this genericity of the tree type by using a @emph{universal
quantifier}:
@example
all t ~~ tree[t]
@end example
What this type expression denotes is a set of possible types: for any
type @code{t}, @code{tree[t]} is also a type. There are infinitely
many such types of course.

The @code{all} quantifier is important: as in logic, there are two
kinds of quantifiers: the @emph{universal} quantifier all and the
@emph{existential} quantifier exists.

The types of the two constructors introduced in the @code{tree} type
definition are similarly quantified:
@example
tEmpty:all t ~~ ()<=>tree[t].

tNode:all t ~~ (tree[t],t,tree[t)]<=>tree[t].
@end example
The type @code{tree[t]} on the right hand side of @code{.tEmpty}'s type
annotation raises a couple of interesting points:

@enumerate
@item
This looks like a type annotation with no associated definition. The
fact that the @code{.tEmpty} symbol was originally introduced in a type
definition is enough of a signal for the compiler to avoid looking for
an implementation for the name.

@item
The type of @code{tEmpty} also takes the form of a constructor type --
using the @code{<=>} operator.

There is a subtle difference between the type of @code{tEmpty} vs
@code{.tEmpty}. The latter actually denotes the application of the
@code{tEmpty} constructor -- as though it had been written @code{tEmpty()}.

@item
The type of a literal @code{.tEmpty} expression -- assuming that no
further information is available -- will be of the form
@code{tree[t34]} where @code{t34} is a `new' type variable not
occurring anywhere else in the program. In effect, the type of
@code{.tEmpty} is tree of @emph{some type} @code{t34} where we don't
know anything more about @code{t34}.
@end enumerate

@node Generic functions
@subsection Generic functions
Given this definition of the tree type, we can construct a more
general form of the tree test function; which is almost identical to
fTest:@footnote{This time too, we must use an explicit type
annotation.}
@example
test:all t ~~ (tree[t],(t)=>boolean)=>boolean.
test(.tEmpty,_) => .false.
test(tNode(L,Lb,R),F) => F(Lb) || test(L,F) || test(R,F).
@end example
and our original string check function becomes:
@example
check(T,S) => test(T,(X) => X==S)
@end example
The type of check is also more generic:
@example
check:all t ~~ (tree[t],t)=>boolean
@end example
I.e., @code{check} can be used to find any type of element in a tree
-- providing that the types align of course.

Actually, this is not the correct the type for @code{check}. This is
because we do not, in general, know that the type can support
equality. The precise type for @code{check} should take this into
account:
@example
check:all t ~~ equality[t] |: (tree[t],t) => boolean
@end example

@node Going further
@section Going further
Although better than the original @code{sTest} program there is still
one major sense in which the @code{test} program is not general enough. We
can see this by looking at another example: a function that counts
elements in the tree:
@example
count:all t ~~ (tree[t]) => integer.
count(.tEmpty) => 0.
count(tNode(L,_,R)) => count(L)+count(R)+1
@end example
This code is very similar, but not identical, to the @code{test}
function.

The issue is that @code{test} is trying to do two things
simultaneously: in order to apply its test predicate to a binary tree
it has to implement a walk over the tree, and it also encodes the fact
that the function we are computing over the tree is a boolean-value
function.

We often need to do all kinds of things to our data structures and
writing this kind of recursion over and over again is tedious and
error prone. What we would like to do is to write a single
@emph{visitor} function and specialize it appropriately when we want
to perform a specific function.

This principle of separating out the different aspects of a system is
one of the core foundations of good software engineering. It usually
goes under the label @emph{separation of concerns}. One of the
beautiful things about functional programming is that it directly
supports such good architectural practices.

Since this visitor may be asked to perform any kind of computation on
the labels in the tree we will need to slightly generalize the type of
function that is passed to the visitor. Specifically, the type of
function should look like:
@example
F : (t,a)=>a
@end example
where the @code{a} input represents accumulated state, @code{t}
represents an element of the tree and the result is another
accumulation.

Using this, we can write a @code{tVisit} function that implements tree
walking as:
@example
tVisit:all a,t ~~ (tree[t],(t,a)=>a,a)=>a.
tVisit(.tEmpty,_,A) => A.
tVisit(tNode(L,Lb,R),F,A) => tVisit(R,F,F(Lb,tVisit(L,F,A))).
@end example
Just as the accumulating function acquires a new `state' parameter, so
the @code{tVisit} function also does. The @code{A} parameter in the
two equations represents this accumulated state.

The second rewrite equation for @code{tVisit} is a bit dense so let us
open it out and look more closely. A more expanded way of writing the
@code{tVisit} function would be:
@example
tVisit(.tEmpty,_,A) => A.
tVisit(tNode(L,Lb,R),F,A) => let@{
    A1 = tVisit(L,F,A).
    A2 = F(Lb,A1).
  @} in tVisit(R,F,A2)
@end example
where @code{A1} and @code{A2} are two local variables that represent
the result of visiting the left sub-tree and applying the accumulator
function respectively. We have used the let expression form to make
the program more obvious, rather than to introduce new functions
locally; but this is a legitimate role for let expressions.

The @code{tVisit} function knows almost nothing about the computation
being performed, all it knows about is how to walk the tree and it
knows to apply functions to labels in the tree.

Given @code{tVisit}, we can implement our original @code{check} and
@code{count} functions as one-liners:
@example
check(T,S) => tVisit(T,(X,A)=>(A || X==S),.false).
count(T) => tVisit(T,(X,A)=>A+1,0).
@end example
The lambda that is embedded in the definition of check bears a little
closer scrutiny:
@example
(X,A)=>(A || X==S)
@end example
In this lambda, we return @code{A} -- if it is @code{true} -- or we
return the result of the test @code{X==S}. This is a common pattern in
such programs: the accumulator @code{A} acts as a kind of state
parameter that keeps track of whether we have already found the value.

@quotation
Functional programs are not actually @emph{state-free}; often quite
the opposite. However, the state in a functional program is never
@emph{hidden}. This is the true distinction between functional and
regular procedural programs.
@end quotation

Notice that we have effectively hidden the recursion in the function
definitions of @code{check} and @code{count} -- all the recursion is
encapsulated within the @code{tVisit} function.

@quotation
One of the unofficial mantras of functional programming is @emph{hide
the recursion}.
@end quotation

The reason we want to hide recursions that this allows the designer of
functions to focus on @emph{what} is being computed rather than
focusing on the structure of the data and, furthermore, this allows
the implementation of the visitor to be @emph{shared} by all users of
the tree type.

Notice that, while @code{a} and @code{t} are type variables, we did
not put an explicit quantifier on the type of @code{F}. This is
because the quantifier is actually put on the type of @code{tVisit}
instead:
@example
tVisit:all a,t ~~ (tree[t],(t,a)=>a,a)=>a
@end example
Just like regular variables, type variables have scope and points of
introduction. Also like regular variables, a type variable may be
@emph{free} in a given type expression; although it must ultimately be
@emph{bound} by a quantifier.

@node Going even further
@subsection Going even further
We have focused so far on generalizing the visitor from the
perspective of the tree type. But there is another sense in which we
are still @emph{architecturally entangled}: from the perspective of
the @code{check} and @code{count} functions themselves.

In short, they are both tied to our @code{tree} type. However, there
are many possible collection data types; @Star{} for instance has some
5 or 6 different standard collection types. We would prefer not to
have to re-implement the @code{check} and @code{count} functions for
each type.

The good news is that, using contracts, we can write a single
definition of @code{check} and @code{count} that will work for a range
of collection types.

Let us start by defining a contract that encapsulates what it means to
visit a collection:
@example
contract all c,t ~~ visitor[c->>t] ::= @{
  visit:all a ~~ (c,(t,a)=>a,a)=>a
@}
@end example
This @code{visitor} contract defines a single function that embodies
what it means to @emph{visit} a collection structure. There are quite
a few pieces here, and it is worth examining them carefully.

A contract header has a template that defines a form of @emph{contract
constraint}. The clause
@example
visitor[c ->> t]
@end example
is such a constraint. The sub-clause
@example
c ->> t
@end example
refers to two types: @code{c} and @code{t}. The presence of the
@code{->>} term identifies the fact that @code{t} @emph{depends} on
@code{c}.

The @code{visitor} contract itself is about the collection type
@code{c}. But, within the contract, we need to refer to both the
collection type and to the type of elements in the collection: the
@code{visit} function is over the collection, it applies a function to
elements of the collection.

Furthermore, as we design the contract, we @emph{do not know} the
exact relationship between the collection type and the element
type. For example, the collection type may be generic in one argument
type -- in which case the element type is likely that argument type;
conversely, if the type is @emph{not} generic (like @code{string}
say), then we have no direct handle on the element type.

We @emph{do know} that within the contract the element type is
@emph{functionally determined} by the collection type: if you know the
collection type then you should be able to figure out the element
type.

We express this dependency relationship with the the @code{c ->> t}
form: whatever type @code{c} is, @code{t} must be based on it.

The body of the contract contains a single type annotation:
@example
visit:all a ~~(c,(t,a)=>a,a)=>a
@end example
This type annotation has three type variables: the types @code{c} and
@code{t} come from the contract header and @code{a} is local to the
signature. What the signature means is

@quotation
Given the visitor contract, the @code{visit} function is from the
collection type @code{c}, a function argument and an initial state and
returns a new accumulation state.
@end quotation

It is worth comparing the type of @code{visit} with the type of
@code{tVisit}:
@example
tVisit:all t,a ~~(tree[t],(t,a)=>a,a)=>a
@end example
The most significant difference here is that in @code{tVisit} the type
of the first argument is fixed to @code{tree[t]} whereas in
@code{visit} it is left simply as @code{c} (our collection type).

Given this contract, we can re-implement our two @code{check} and
@code{count} functions even more succinctly:
@example
check(T,S) => visit(T, (X,A)=>A || X==S,.false)
count(T) => visit(T, (X,A)=>A+1,0)
@end example
These functions will apply to @emph{any} type that satisfies -- or
implements -- the @code{visitor} contract. This is made visible in the
revised type signature for @code{count}:
@example
count:all c,t ~~ visitor[c->>t] |: (c)=>integer.
@end example
This type is an example of a @emph{constrained type}. It is generic in
@code{c} and @code{t} but that generality is constrained by the
requirement that the @code{visitor} contract is appropriately
implemented. The eagled-eyed reader will notice that @code{count} does
not actually depend on the type of the elements in the collection:
this is what we should expect since @code{count} does not actually
care about the elements themselves.

The type signature for @code{check}, however, does care about the
types of the elements:
@example
check:all c,t ~~
  visitor[c->>t], equality[c] |: (c,t)=>boolean
@end example
This type annotation now has two contract constraints associated with
it: the collection must be something that is visitable and the
elements of the collection must support @code{equality}.

Given the work we have done, we can implement the @code{visitor}
contract for our @code{tree[t]} type quite straightforwardly:
@example
implementation all t ~~ visitor[tree[t]->>t] => @{
  visit = tVisit
@}
@end example
Notice that header of the implementation statement provides the
connection between the collection type (which is @code{tree[t]}) with
the element type (@code{t}). The clause
@example
visitor[tree[t]->>t]
@end example
is effectively a declaration of that connection.

Now that we have disconnected @code{visit} from @code{tree} types, we
can extend our program by implementing it for other types. In
particular, we could also implement the visitor for the @code{sTree}
type:
@example
implementation visitor[sTree ->> string] => @{
  visit = sVisit
@}
@end example
however, we leave the definition of @code{sVisit} as a simple exercise
for the reader.

Our final versions of @code{count} and @code{check} are now quite
general: they rely on a generic implementation of the @code{visit}
function to hide the recursion and are effectively independent of the
actual collection types involved.

If we take a second look at our visitor contract we can see something
quite remarkable: it counts as a definition of the famous
@emph{visitor pattern}. This is remarkable because, although visitor
patterns are a common design pattern in OO languages, it is often hard
in those languages to be crisp about the semantics of visiting; in
fact, they are called patterns because they represent patterns of use
which may be encoded in Java (say) whilst not necessarily being
definable in them.

The combination of contract and implementation represents a quite
formal way of defining patterns like the visitor pattern.

There is something else here that is quite important too: we are able
to define and implement the visitor contract @emph{without} having to
modify in any way the type definition of @code{tree} or
@code{sTree}. From a software engineering point of view this is quite
important: we are able to gain all the benefits of interfaces without
needing to entangle them with our types. This becomes critical in
situations where we are not able to modify types -- because they don't
belong to us and/or we don't have access to the source.

@node Polymorphic arithmetic
@section Polymorphic arithmetic
There are other ways in which programs can be polymorphic. In
particular, let us focus for a while on arithmetic. One of the issues
in arithmetic functions is that there are many different kinds of
numbers. Pretty much every programming language distinguishes several
kinds of numbers; for example, Java distinguishes byte, short, int,
long, float, double, BigInteger and BigDecimal -- and this does not
count the wrapped versions. Other languages have even more choice.

One question that might seem relevant is why? The basic answer is that
different applications call for different properties of numbers and no
one numeric type seems to fit all needs. However, that variety comes
at a cost: when we use numbers we tend to have to make too early a
choice for the numeric type.

For example, consider the @code{double} function we saw earlier:
@example
double(X) => X+X
@end example
What type should @code{double} have? In particular, what should the
type of @code{+} be? Most people would be reluctant to use different
arithmetic operators for different types of numbers.@footnote{Although
some languages -- such as SML -- do require this.} This is resolved by
relying on contracts for the arithmetic operations.

The result is that the most appropriate type signature for
@code{double} is exquisitely tuned:
@example
double:all t ~~ arith[t] |: (t)=>t
@end example
This type is precisely the most general type that @code{double} could
have. Any further constraints result in making a potentially premature
choice for the numeric type.

If we take another look at our original @code{fact} function:
@example
fact(0) => 1
fact(N) => N*fact(N-1)
@end example
this is constrained to be a function from @code{integer} to
@code{integer} because we introduced the literal integers @code{0} and
@code{1}. However, the @code{arith} contract contains synonyms for
these very common literals. Using @code{zero} and @code{one} allow us
to be abstract in many arithmetic functions:
@example
genFact:all a ~~ arith[a] |: (a)=>a.
genFact(zero) => one.
genFact(N) => N*genFact(N-one).
@end example
We call out @code{zero} and one for special treatment because they
occur very frequently in numerical functions. We can introduce other
numeric literals without compromising our type by using
@emph{coercion}; although it is more clumsy:
@example
factorialC:all t ~~
  arith[t],coercion[integer,t] |: (t)=>t.
factorialC(N) where N==0::t => 1::t.
factorialC(N) => N*factorialC(N-1::t).
@end example
The expressions @code{0::t} and @code{1::t} are coercions from
@code{integer} to @code{t}.

Of course, coercion is also governed by contract, a fact represented
in the type signature by the coercion contract constraints on the type
of @code{t}.

In any case, using these techniques, it is possible to write numeric
functions without unnecessarily committing to specific number
types. That in turn helps to make them more useful.

@node Optional computing
@subsection Optional computing
There are many situations where it is not possible to guarantee that a
computation will succeed. The simplest examples of this include
scenarios such as accessing external files; but may also apply to
getting the first element of a list or the label of a @code{tree}
node. The great unknown of accessing elements of a collection is `is
it there?'. Its not guaranteed of course, and we need to be able to
handle failure.

Many languages employ the concept of a special @code{null} value to
denote some of these cases -- like a @code{someOne} not having a
@code{spouse}. However, the special @code{null} value brings its own
problems: the type of @code{null} is problematic (it is a legal value
for every type) and there are many situations where @code{null} is
never possible.

We address this by handling those situations where failure is possible
differently than where it is not. Specifically, we do this via the
@code{option} type.

The type definition of @code{option} is quite straightforward:
@example
all t ~~ option[t] ::= .none | some(t).
@end example
where @code{none} is intended to denote the non-existence of a value
and @code{some} denotes an actual value.

The @code{option} type is intended to be used in cases where functions
are known to be partial.@footnote{A partial function does not have a
value across the whole range of its arguments.} An @code{option}
return type signals that the function may not always have a value.

Normal pattern matching can be used to access a value wrapped in a
@code{some}; for example, to access someone's @code{spouse} we can use
the condition:
@example
isMarriedTo(P,J) where some(JJ).=P.spouse => J==JJ.
isMarriedTo(_,_) default => .false.
@end example
The important detail here is that all access to a @code{option}
wrapped value is gated by some form of pattern matching and that,
normally, this takes place in a condition.

@quotation
The condition
@example
some(JJ).=P.spouse
@end example
represents a pattern matching condition: it is satisfied if
@code{P.spouse} matches @code{some(JJ)} and has the additional effect
of defining and binding the variable @code{JJ} to the matched spouse.
@end quotation

@node Special syntax for @code{option}al values
@subsection Special syntax for @code{option}al values
Of course, the code above @emph{is} kind of clumsy. There is a range
of operators to make using @code{option} values more pleasant.

The most important of these is the @code{^=} operator which combines
the @code{.=} with the @code{some} match. Using this, the
@code{isMarriedTo} function becomes:
@example
isMarriedTo(P,J) where JJ^=P.spouse => J==JJ.
isMarriedTo(_,_) default => .false.
@end example
The meaning of @code{^=} is similar to the pattern match condition
@code{.=}; except that the pattern is assumed to be for a @code{some}
value.

While the @code{^=} operator@footnote{Read as `has a value'.} is very
useful in unpacking an optional value, the @code{^|} operator allows
us to handle cases where we always need to be able to give some kind
of value. For example, normally a @code{map} returns none if an entry
is not present. However, a @emph{cache} is structured differently: if
a value is not present in a cache then we must go fetch it:

@example
cacheValue(K) => cache[K] ^| fetch(K)
@end example

We can also apply a match @emph{in line} to an @code{option}al value. The
@code{^} operator allows a pattern to be formed by applying a
@code{option} valued function directly in place. For example, the
equation:
@example
head(first^(1)) => "alpha"
@end example
is equivalent to:
@example
head(X) where some(1).=first(X) => "alpha"
@end example
We will see more examples of this when we look more closely at
sequences and collections processing.

Finally, we can promote an optional expression into an enclosing
expression (which is therefore also optional). For example, in the
expression:

@example
nameOf(^P.spouse)
@end example
if @code{P.spouse} is @code{none}, then the value returned by the
@code{nameOf} expression is also @code{none}, otherwise it takes the
form:
@example
some("P_spouse_name")
@end example
(assuming that @code{nameOf} is defined for @code{Person}s.)

Overall, the @code{option} type is part of an elegant approach to
nullability that is easily incorporated into the type system.

@node A word about type inference
@section A word about type inference
We have seen some powerful forms of types in this chapter: recursive
types defined using algebraic type definitions, generic types and even
function types. Recall also that we only require programmers to
explicitly declare the types of quantified variables and functions. It
is worth pausing a little to see how this might be done.

Recall our original @code{fact} function:
@example
fact(0) => 1.
fact(N) => N*fact(N-1).
@end example
The compiler is able to compute the types of the various variables
automatically through a process known as @emph{type inference}. Type
inference may seem magical, but is actually (mostly) quite simple. Let
us take a look at the expression:
@example
N-1
@end example
which is buried within the recursive call in @code{fact}. Although it
looks like a special operator, arithmetic expressions are not special;
the @code{-} function is just a function from numbers to numbers; its
type is given by:@footnote{We put the (-) in parentheses to highlight
the use of an operator as a normal symbol.}
@example
(-) : all t ~~ arith[t] |: (t,t)=>t
@end example
However, we should simplify this type a little in order to make the
explanation of type inference a little simpler. In what follows, we
assume that the type of (@code{-}) is:
@example
(-) : (integer,integer)=>integer
@end example
Type inference proceeds by using special @emph{type inference rules}
which relate expressions to types, in this case the applicable rule is
that a function application is consistent if the function's parameter
types are consistent with the types of the actual arguments. If they
are consistent, then the type of the function application is the
return type of the function.

The type inference process initially gives every variable an unknown
type -- represented by a new type variable not appearing anywhere
else. For our tiny @code{N-1} example, we will give @code{N} the type
@var{t@sub{N}}.

The (@code{-}) function has two arguments whose types can be expressed
as a tuple of types:
@example
(integer,integer)
@end example
and the types of the actual arguments are also a tuple:
@example
(@var{t@sub{N}},integer)
@end example
In order for the expression to be type correct, the actual types of
the arguments must be consistent with the expected types of the
function; which we can do by making them @emph{the same}. There is a
particular process used to do this -- called @emph{unification}.

@float Figure,minusTypeFig
@caption{Inferring the Type of N-1}
@image{images/minustype}
@end float

@table @emph
@item Unification
An algorithm that replaces variables with values in such a way as to
make two terms identical.
@end table

Unification matches equals with equals and handles (type) variables by
substitutions -- for example, we can make these two type expressions
equal by @emph{binding} the type variable @var{t@sub{N}} to
@code{integer}.

We initially picked the type of @code{N} to be an arbitrary type
variable, but the process of checking consistency leads us to refine
this and make the type concrete. I.e., the use of @code{N} in a
context where an integer is expected is enough to allow the compiler
to infer that the type of @code{N} is indeed @code{integer} and not
@var{t@sub{N}}.

Of course, if there are multiple occurrences of @code{N} then each of
those occurrences must also be consistent with integer; and if an
occurrence is not consistent then the compiler will report an error --
a given expression may only have one type.

The bottom line is that types are based on a combination of
unification for comparing types and a series of type rules that have
the effect of introducing @emph{constraints} on types based on which
language features are present in the text. The type checker is really
a constraint solver: if the constraints are not satisfiable (for
example by trying to `call' a variable and add a number to it) then
there is a type error in the program.

The magic of type inference arises because it turns out that solving
these constraints is sufficient for robustly type checking programs.

A sharp-eyed reader will notice that @Star{}'s type system is
different in nature to that found (say) in OO languages. In @Star{}'s
type system, types are considered to be consistent only if they are
@emph{equal}. This is quite different to the notion of consistency in
OO languages where an argument to a function is consistent if its type
is a @emph{sub-type} of the expected type.

However, we would note that the apparent restriction to the type
system imposed by type equality is much less severe in practice than
in theory -- and that OO languages' type systems also often
incorporate some of the same restrictions.

@node Why is type inference restricted?
@subsection Why is type inference restricted?

@noindent
We have stated a few times that the type system only infers types of
variables that are @emph{not} quantified. In fact, it is fairly
straightforward to build a type inference system that can infer
quantified types. For example, such a complete type inference system
would infer from these equations:

@example
conc(.nil,x)=>x.
conc(cons(h,tl),x) => cons(h,conc(tl,x))
@end example
the generalized type for @code{conc}:
@example
conc:all t ~~ (cons[t],cons[t])=>cons[t].
@end example
However, several technical and non-technical considerations stay our
hand at building such a type inference system:

@itemize
@item
There are still types that cannot be correctly inferred; and would
therefore require explicit type annotations to correctly type the
program.
@item
Having explicit type annotations is 'good style' in general and
definitely aids in debugging type errors.
@item
Being explicit about quantification makes unification of quantified
types simpler and more reliable.
@end itemize

On the other hand, requiring type annotations for @emph{every}
variable would be extremely tedious and verbose. An extreme version of
this policy would require the @code{conc} program above to be written:

@example
conc:all t ~~ (cons[t],cons[t])=>cons[t].
conc(.nil,x:list[t])=>x.
conc(cons(h:t,tl:list[t]),x:list[t]) => cons(h,conc(t,x))
@end example
The design of @Star{} strikes a balance between useability and rigor:
most variables do not require explicit type annotations. We require
them only when something 'special' is being indicated; one of those
special circumstances is when defining a generic function.

Even there, there are many situations where explicit type annotations
are not needed: for example when defining a field in a record, or a
function in the implementation of a contract, there already is a type
that the type system can use to verify the program.

So, the precise rule for type inference is:

@quotation
If the type of a variable is known from context, then use that type to
verify the type of any value the variable may be bound to. Otherwise,
use type inference on the value to infer a type for the variable but
do not attempt to generalize it by adding quantifiers to the inferred
type.
@end quotation

We are only able to scratch the surface of the type system here. It is
certainly true that -- like many modern functional languages --
@Star{}'s type system is complex and subtle. The primary motivation
for this complexity is to reduce the burden for the programmer: by
being able to infer types automatically, and by being able to address
many programming subtleties, the type system comes to be seen as the
programmer's friend rather than as an obstacle to be `gotten around'.

@node Are we there yet?
@section Are we there yet?

The straightforward answer to this is no. There is a great deal more
to functional programming than can be captured in a few pages. In
particular, we have not touched on one of the foundations of modern
functional programming -- @emph{Category Theory}.

However, we have covered some of the key features of functional
programming -- particularly as it applies to @Star{}. In subsequent
chapters we will take a closer look at collections, at modular
programming, at concurrency and even take a pot shot at Monads.

If there is a single idea to take away from this chapter it should be
that functional programming is natural. If there is a single piece of
advice for the budding functional @Star{} programmer, it should be to
@emph{hide the recursion}. If there is a single bit of comfort to
offer programmers it should be that @emph{Rome was not built in a
day}.

In the next chapter we look at collections, one of the richest topics
in programming.
