= Collections

Modern programming -- whether it is OO programming, functional
programming or just plain C programming -- relies on a rich standard
library. Given that nearly every program needs to be able to manage
collections of _things_, the central pearl of any standard
library is the _collections_ library. Recalling our mantra of
hiding recursion; a well designed collections library can make a huge
difference to the programmer's productivity, often by hiding many of
the recursions and iterations required to process collections.

The collections architecture has four main components:

. a range of standard collection types -- including cons lists, first-in
first-out queues, ideal hash trees and sets;
. a range of standard functions -- mostly defined in contracts -- that
define the core capabilities of functions over collections;
. special notations that make programming with collections in a type
independent way more straightforward; and
. the final major component of the collections architecture is
_queries_. There is a simple yet powerful set of features aimed at
simplifying querying collections.

The query component is sufficiently involved to merit a chapter of its own:
<<choices>>.

== Sequence notation

A sequence is an ordered collection; a sequence expression is an
expression involving a complete or partial enumeration of the values
in the collection. Star has a straightforward notation for expressing
sequences of any underlying type; for example, a ``cons`` sequence of
integers from 1 through 5 can be written:
[source,star]
----
[1, 2, 3, 4, 5]:cons[_]
----
In situations where we do not know or do not wish to specify the
collection type, we can write instead:
[source,star]
----
[1, 2, 3, 4, 5]
----

This term -- it could be either an expression or a pattern -- denotes
the sequence _without_ specifying the underlying collection
type. The difference in the types of the two terms is telling:
[source,star]
----
cons[integer]
----
and
[source,star]
----
sequence[c->>integer] |= c
----
respectively -- where ``c`` is a type variable. The first is a
concrete type expression, the second is a constrained type -- in this
case ``c`` must implement the ``sequence`` contract.

Although the second type expression is longer, and a bit more complex
to read, it is also actually less constraining. The type expression
``cons[integer]`` does not allow for variation of the underlying
collection type; the second type expression allows the term to be used
in contexts that require different concrete types.

=== Partial sequence notation

The sequence notation also allows for the specification of partial
sequences; this is particularly useful in writing functions that
construct and traverse sequences. The sequence term:
[source,star]
----
[1,2,..X]
----
denotes the sequence whose first two elements are ``1`` and
``2`` and whose remainder is denoted by the variable ``X`` --
which must also be a sequence of the appropriate type.

There is a strong relationship between the normal sequence notation
and the partial sequence notation. In particular, the sequence
expression
[source,star]
----
[1,2]
----
is equivalent to:
[source,star]
----
[1,..[2,..[]]]
----

The major benefit of general sequence notation is that it allows us to
construct programs involving collections that are independent of type
_and_ to do so in a syntax which is concise.

For example, we can use sequence notation to write functions over
sequences; such as the ``concat`` function that concatenates two
sequences:
[source,star]
----
concat:all c,e ~~ stream[c->>e], sequence[c->>e] |= (c,c)=>c.
concat([],X) => X.
concat([E,..X],Y) => [E,..concat(X,Y)].
----
This function will work equally well with ``cons`` lists, strings,
even your own collection types. All that is required is that there is
an implementation of the ``stream`` and the ``sequence``
contracts for the actual type being concatenated.

[aside]
There are two contracts here: the ``stream`` contract is used when
decomposing sequences and the ``sequence`` contract is used when
building them.

=== The ``stream`` and ``sequence`` contracts

Underlying the sequence notation are two contracts: the ``sequence``
contract and the ``stream`` contract. These contracts contains
type signatures that can be used to construct and to match against
sequence values. The sequence notation is realized by the compiler
translating sequence terms to a series of calls to those functions.

The standard ``stream`` contract is
[source,star]
----
contract stream[t->>e] ::= {
  _eof:()=>boolean.   -- is stream empty
  _hdtl:(t)=>option[(e,t)]. -- match front of stream
}
----
and the standard ``sequence`` contract is:
[source,star]
----
contract stream[t->>e] ::= {
  _nil:t.     -- empty stream
  _cons:(e,t)=>t. -- add to front
}
----

The entries in the ``sequence`` contract should be fairly self-evident:

``_nil``:: is the empty sequence;
``_cons``:: is a function that `'glues'` a new element to the front of
the sequence.

The compiler uses these functions to transform sequence expressions
into function calls.

For example, the sequence expression:
[source,star]
----
[1,2,3]
----
is transformed into
[source,star]
----
_cons(1,_cons(2,_cons(3,_nil)))
----
If a sequence expression has an explicit type annotation on it, as in:
[source,star]
----
[1,2]:cons[_]
----
then the annotation is all that is needed to force the compiler to
treat the result as a concrete ``cons`` list. Type inference does the rest
of the hard work.footnote:[The type expression ``_`` is a special
type that denotes an anonymous type: each occurrence of the type
expression denotes a different unknown type. It is useful in
situations, like this one, where only some of the type information is
known.]

=== Stream patterns

The ``stream`` contract is used in _patterns_ to match and
decompose sequences. For example, the signature for ``hdtl`` --
which is used to decompose sequences into a head and tail -- is:
[source,star]
----
_hdtl:(t)=>option[(e,t)].
----
This function will be applied to a sequence in the attempt to split it
into a head and remainder. The question is how can a function be used
in a pattern?

The term ``[1,2,..X]`` _as a pattern_ is rewritten as:
[source,star]
----
_hdtl^(1,_hdtl^(2,X))
----
where the ``^`` is syntactic sugar for the more elaborate form:
[source,star]
----
S0 where (1,S1) ?= _hdtl(S0) && (2,X)?=_hdtl(S1)
----
which is, in turn, syntactic sugar for:
[source,star]
----
S0 where .some((1,S1)) .= _hdtl(S0) && .some((2,X)) .= _hdtl(S1)
----
I.e., the sequence pattern becomes a series of progressive
decompositions of the stream; at each stage an ``option``-valued
function is applied to peel off elements from the stream.

We can now straightforwardly give the translation for sequence
patterns. Syntactically, there is no distinction between sequence
expressions and stream patterns -- what distinguishes them is context:
stream patterns show up as patterns in functions and sequence
expressions show up in the expression context.

A stream pattern, as in the pattern ``[E,..X]`` for the non-empty
case in ``concat``:
[source,star]
----
concat([E,..X],Y) => [E,..concat(X,Y)]
----
is transformed into the pattern:
[source,star]
----
_hdtl^(E,X)
----
and the entire ``concat`` equation becomes:
[source,star]
----
concat(_hdtl^(E,X),Y) => _cons(E,concat(X,Y))
----
which, as we noted above, is actually equivalent to:
[source,star]
----
concat(S0,Y) where .some((E,X)).=S0 =>
  _cons(E,concat(X,Y)).
----

The ``sequence`` and ``stream`` contracts are two of the most
important and commonly used contracts. As we shall see further, many
of the standard collections functions are built on top of it.

NOTE: We have two contracts -- one for composing and another for decomposing
sequences -- because not all collections are equally amenable to
decomposing and/or composing. For example, the ``map`` type we
describe below does not have a natural notion of decomposing (because
ordering within a ``map`` is not preserved); even though it does
have a natural form of describing actual ``map`` collections.

=== Notation and contracts

One of the distinctive features of the sequence notation is that it is
an example of _syntax_ that is underwritten by a semantics
expressed as a _contract_. 

This has a parallel in modern OO languages like Java and C# where
important contracts are expressed as interfaces rather than concrete
types. However, we extend the concept by permitting special notation
as well as abstract interfaces -- as many mathematicians understand, a
good notation can sometimes make a hard problem easy. We further
separate interfaces from types by separating the type definition from
any contracts that may be implemented by it.

The merit of this combination of special syntax and contracts is that
we can have the special notation expressing a salient concept -- in
this case the sequence -- and we can realize the notation without
undue commitment in its lower-level details. In the case of sequence
notation, we can have a notation of sequences without having to commit
to the type of the sequence itself.

== Indexing

Accessing collections conveniently is arguably more important than a
good notation for representing them. There is a long standing
traditional notation for accessing arrays:
[source,star]
----
L[ix]
----
where ``L`` is some array or other collection and ``ix`` is an
integer offset into the array. We use a notation based on this
for accessing collections with random indices; suitably generalized to
include dictionaries (collections accessed with non-numeric indices)
and _slices_ (contiguous sub-regions of collections).

Before we explore the indexing notation it is worth looking at the
contract that underlies it -- the ``indexed`` contract.

=== The indexed contract

The indexed contract captures the essence of accessing a collection in
a random-access fashion. There are functions in the contract to access
a directly accessed element, to replace and to delete elements from
the collection:

[source,star]
----
contract all s,k,v ~~ indexed[s->>k,v] ::= {
  _index:(s,k)=>option[v].
  _put:(s,k,v)=>s.
  _remove:(s,k)=>s.
  _empty:s
}
----
There are several noteworthy points here:

* the form of the contract itself; the signature for ``_index`` which
accesses elements;
* the signatures for ``_put``  and ``_remove``
which return new collections rather than modifying them in-place; and
* the ``_empty`` collection is not associated with a function type.

Recall that the ``stream`` contract had the form:
[source,star]
----
contract all s, e ~~ stream[s->>e] ::= ...
----
the ``s->>e`` clause allows the implementation of the contract to
functionally determine (sic) the type of the elements of the
collection.

In the case of ``indexed``, the contract form determines _two_
types denoted by ``k`` and ``v``. The type ``k`` denotes the
type of the key used to access the collection and ``v`` denotes the
type of the elements of the collection. Each individual implementation
of indexed is free to specify these types; usually in a way that best
reflects the natural structure of the collection.

For example, the implementation of ``indexed`` for strings starts:
[source,star]
----
implementation indexed[string ->> integer,char] => ...
----
reflecting the fact that the natural index for strings is integer and
the natural element type is ``char`` (neither being explicitly part of
the string type name).

On the other hand, the implementation of ``indexed`` for the
concrete type ``map`` starts:
[source,star]
----
implementation all k,v ~~ indexed[map[k,v] ->> k,v] => ...
----
reflecting the fact that dictionaries are naturally generic over both
the key and value types.

If we look at the signature for ``_index`` we can see that this
function does not directly return a value from the collection, but
instead returns an ``option`` value. This bears further
explanation.

The great unknown of accessing elements of a collection is `is it
there?'. Its not guaranteed of course, and we need to be able to
handle failure. In the case of the ``_index`` function, its
responsibility is to either return a value wrapped as a ``some``
value -- if the index lookup is successful -- or the signal
``none`` if the index lookup fails. Just to be clear: ``_index``
can act both as a lookup _and_ as a test for membership in the
collection.

==== Adding and removing elements

The function ``_put`` is used to add an element to a collection associating it
with a particular index position; and the function ``_remove`` removes an
identified element from the collection.footnote:[The ``_put`` function is
expected to replace an element if that key presented already has an associated
value.]

These functions have a property often seen in functional programming
languages and not often seen elsewhere: they are defined to return a
complete new collection rather than simply side-effecting the
collection. This is inline with an emphasis on _persistent data
structures_footnote:[A persistent structure is one which is never
modified -- changes are represented by new structures rather than
modifiying existing ones.] and on _declarative programming_.

One might believe that this is a bit wasteful and expensive -- returning new
collections instead of side-effecting the collection. However, that is something
of a misconception: modern functional data structures have excellent
computational properties and approach the best side-effecting structures in
efficiency. At the same time, persistent data structures have many advantages --
but, perhaps the most important is that it tends to lead to fewer programming
errors by applications programmers.

[aside]
It should also be stressed that the ``indexed`` contract allows and
encourages persistence but does not _enforce_ it. It is quite
possible to implement indexing for data structures that are not
persistent.

===  The index notation

Given the indexed contract we can now show the specific notation for
accessing elements of a collection. Accessing a collection by index
follows conventional notation:
[source,star]
----
C[ix]
----
will access the collection ``C`` with element identified by
``ix``. For example, given a ``map`` ``D`` of strings to
strings, we can access the entry associated with “alpha” using:
[source,star]
----
D["alpha"]
----
Similarly, we can access the third character in a string ``S`` using:
[source,star]
----
S[2]
----
As might be expected, given the discussion above, the type of an index
expression is optional.

The most natural way of making use of an index expression is to use it
in combination with a ``?=`` condition or an ``^|`` expression
-- which allows for smooth handling of the case where the index
fails. For example, we might have:

[source,star]
----
nameOf(F) where N ?= names[F] => N.
nameOf(F) default => ...
----

We also have a specific notation to represent modified
collections. For example, the expression
[source,star]
----
D["beta"->"three"]
----
denotes the map ``D`` with the entry associated with ``"beta"``
replaced by the value ``"three"``. Note that the value of this
expression is the updated map.

For familiarity's sake, we also suppose a form of assignment for the
case where the collection is part of a read-write variable. The
action:
[source,star]
----
D["beta"] := "three"
----
is entirely equivalent to:
[source,star]
----
D := D["beta"->"three"]
----
always assuming that the type of ``D`` permits assignment.

Similarly, the expression:
[source,star]
----
D[~"gamma"]
----
which denotes the map ``D`` where the value associated with the key
``"gamma"`` has been removed.

Although, in these examples, we have assumed that ``D`` is a map
value (which is a standard type); in fact the index
notation does not specify the type. As with the sequence notation, the
only requirement is that the ``indexed`` contract is implemented
for the collection being indexed.

In particular, as well as the ``map`` type, index notation is
supported for the built-in ``cons`` list type, and is even supported for
the ``string`` type.

In addition to the indexed access notation described so far, there is
also a variant of the sequence notation for constructing indexed
literals (aka dictionaries). In particular, an expression of the form:
[source,star]
----
{"alpha"->1, "beta"->2, "gamma"->3}
----
is equivalent to a sequence of tuples, or to:
[source,star]
----
_put(_put(_put(_empty,"gamma",3),"beta",2),"alpha",1)
----

=== Implementing indexing

Of course, we can also implement indexing for our own types. For example,
before, when looking at generic types we saw the tree type:

[source,star]
----
all t ~~ tree[t] ::= .tEmpty | .tNode(tree[t],t,tree[t]).
----
We can define an implementation for the ``indexed`` contract for this type
-- if we arrange for the tree to be a tree of key-value pairs:
[source,star]
----
implementation all k,v ~~
    comp[k], equality[k] |= indexed[tree[(k,v)]->>k,v] => {
  _index(T,K) => findInTree(T,K).
  _put(T,K,V) => setKinTree(T,K,V).
  _remove(T,K) => removeKfromTree(T,K).
}
----
The form of the type expression ``tree[(k,v)]`` is required to
avoid confusion -- ``tree`` takes a single type argument that, in
this case, is a tuple type. The extra set of parentheses ensures that
``tree`` is not interpreted (incorrectly) as a type that takes two
type arguments.

With this statement in scope, we can treat appropriate ``tree``
expressions as though they were regular arrays or dictionaries:
[source,star]
----
T = .tNode(.tEmpty,("alpha","one"),.tEmpty)
assert "one" ?= T["alpha"].
U = T["beta"->"two"]. -- Add in "beta"
assert "one" ?= U["alpha"].
----
The implementation statement relies on another feature of the
type system -- we need to constrain the implementation of indexed to a
certain subset of possible instances of ``tree`` types -- namely, those where
the element type of the tree is a _pair_ -- a two-tuple -- and
secondly we require that the first element of the pair is comparable
-- i.e., it has the ``comp`` contract defined for it.

This is captured in the contract clause of the implementation
statement:
[source,star]
----
implementation all k,v ~~ comp[k], equality[k] |=
      indexed[tree[(k,v)]->>k,v] => ...
----
This implementation contract qualifier is fairly long, and the type
constraints are fairly complex; but it is exquisitely targeted at
precisely the right kind of tree without us having to make any
unnecessary assumptions.footnote:[It is also true that most
programmers will not be constructing new implementations of the
indexed contract very frequently.]

Implementing the ``indexed`` contract requires us to implement three
functions: ``findInTree``, ``setKinTree`` and
``removeKfromTree``. The ``findInTree`` function is quite
straightforward:
[source,star]
----
findInTree:all k,v ~~ equality[k], comp[k] |= (tree[(k,v)],k)=>option[v].
findInTree(.tEmpty,_) => .none.
findInTree(.tNode(_,(K,V),_),K) => .some(V).
findInTree(.tNode(L,(K1,_),_),K) where K1>K => findInTree(L,K).
findInTree(.tNode(_,(K1,_),R),K) where K1<K => findInTree(R,K).
----
Notice that each _label) in the tree is a 2-tuple -- consisting of the
key and the value. This function is also where we need the key type to
be both comparable and supporting equality. The comparable constraint
has an obvious source: we perform inequality tests on the key.

The ``equality`` constraint comes from a slightly less obvious
source: the repeated occurrence of the ``K`` variable in the second
equation. This repeated occurrence means that the equation is
equivalent to:
[source,star]
----
findInTree(.tNode(_,(K,V),_),K1) where K==K1 => .some(V).
----
We leave the implementations of ``setKinTree`` and ``removeKfromTree`` as an
exercise for the reader.

With this implementation, we can not only write indexed expressions, but we also have a notation for ``tree`` literals:

[source,star]
----
{ "alpha" -> 1, "beta"->2, "gamma"->3} : tree[_]
----

We use the type annotation here to mke it clear that we want a collection based
on the `tree` type. Without it, teh type checker would have to infer it -- which
is not always possible.


=== Index slices

Related to accessing and manipulating individual elements of
collections are the _indexed slice_ operators. An indexed slice
of a collection refers to a bounded subset of the collection. The
expression:
[source,star]
----
C[fx:tx]
----
denotes the subsequence of ``C`` starting with -- and including --
the element indexed at ``fx`` and ending -- but _not_
including the element indexed at ``tx``.

As might be expected, the index slice notation is also governed by a
contract -- the ``slice`` contract. This contract defines the
core functions for slicing collections and for updating subsequences
of collections:
[source,star]
----
contract all s,k ~~ slice[s->>k] ::= {
  _slice:(s,k,k)=>s.
  _splice:(s,k,k,s)=>s.
}
----
The ``_slice`` function is used extract a slice from the
collection, and ``_splice`` is used to replace a subset of the
collection with another collection.

////
Like the indexing notation, there is notation for the three
cases:
[source,star]
----
C[fx:]
----
denotes the tail of the collection -- all the elements in ``C``
that come after ``fx`` (including ``fx`` itself);footnote:[The
complement of the tail slice is simple: ``C[0:tx]``].
////

[source,star]
----
C[fx:tx->D]
----
denotes the result of splicing ``D`` into ``C``. This last form
has an additional incarnation -- in the form of an assignment
statement:
[source,star]
----
C[fx:tx] := D
----
This action is equivalent to the assignment:
[source,star]
----
C := _splice(C,fx,tx,D)
----
which, of course, assumes that ``C`` is defined as a read/write
variable.

The slice notation is an interesting edge case in domain specific
languages. It is arguably a little obscure, and, furthermore, the use
case it represents is not all that common. On the other hand, without
specific support, the functionality of slicing is hard to duplicate
with the standard indexing functions.

== Doing stuff with collections

One of the most powerful features of collections is the ability to
treat a collection as a whole. We have already seen a little of this
in our analysis of the visitor pattern <<goingEvenFurther>>. Of
course, the point of collections is to be able to operate over them as
entities in their own right. As should now be obvious, most of the
features we discuss are governed by contracts and it is paradigmatic
to focus on contract specifications rather than specific
implementations.

The number of things that people want to do with collections is only
limited by our imagination; however, we can summarize a class of
operations in terms several patterns:

* Filtering
* Transforming into new collections
* Summarizing collections
* Querying collections

Each of these patterns has some support from the standard repertoire
of functions.

=== Filtering

The simplest operation on a collection is to subset it. The standard
filter function -- ``^/`` -- allows us to do this with some
elegance. Using filter is fairly straightforward; for example, to
remove all odd numbers from a collection we can use the expression:
[source,star]
----
Nums^/((X)=>divides(2,X)
----
For example, if ``Nums`` were the `cons` list:
[source,star]
----
[1,2,3,4,5,6,7,8,9]:cons[_]
----
then the value of the filter expression would be
[source,star]
----
[2,4,6,8]
----
The right hand argument to ``^/`` is a _predicate_: a function
that returns a ``boolean`` value. The ``^/`` function (which is
part of the standard ``filter`` contract) is required to apply the
predicate to every element of its left hand argument and return a
_new_ collection of every element that satisfies the
predicate.footnote:[The original collection is unaffected by the
filter.]

The function `divides` is a predicate that is true if the first number divides
into the second:

[source,star]
----
divides(K,X) => (try X%K==0 catch { _ => .false}).
----

[NOTE]
****
We wrap the modulo check in an exception handler because the modulo operator ``%`` can fail (if dividing by zero).

Although we known that that can never happen, that kind of semantic knowledge
cannot be built into Star's type system.
****

The ``^/`` operator allows us to represent many filtering
algorithms whilst not making any recursion explicit. However, not all
filters are easily handled in this way; for example, a prime number
filter _can_ be written

[source,star]
----
N^/isPrime
----
but such an expression is likely to be very expensive (the
`isPrime` test is difficult to do well).

==== The ``filter`` contract

As noted above, the ``^/`` function is governed by a contract, the
``filter`` contract:
[source,star]
----
contract all c,e ~~ filter[c->>e] ::= {
  (^/):(c,(e)=>boolean) => c.
}
----

=== The sieve of Erastosthneses

One of the classic algorithms for finding primes that can be expressed
using filters is the so-called _sieve of Eratosthenes_. This
algorithm works by repeatedly removing multiples of primes from a list
of natural numbers. We cannot (yet) show how to deal with infinite
lists of numbers but we can capture the essence of this algorithm
using a cascading sequence of filter operations.

The core of the sieve algorithm involves taking a list of numbers and
removing multiples of a given number from the list. This is very
similar to our even-number finding task, and we can easily define a
function that achieves this:
[source,star]
----
filterMultiples(K,N) => N^/((X)=>~divides(K,X)).
----

The overall Eratosthenes algorithm works by taking the first element
of a candidate list of numbers as the first prime, removing multiples
of that number from the rest, and recursing on the result:
[source,star]
----
sieve([N,..rest]) => [N,..sieve(filterMultiples(N,rest))].
----
There is a base case of course, when the list of numbers is exhausted
then we have no more primes:
[source,star]
----
sieve([]) => [].
----
The complete prime finding program is hardly larger than the original
filter specification:
[source,star]
----
primes(Max) => let{.
  sieve([]) => [].
  sieve([N,..rest]) => [N,..sieve(filterMultiples(N,rest))].

  filterMultiples(K,N) => N^/~divides(K,X).

  divides(K,X) => (try X%K==0 catch { _ => .false}).

  iota(Mx,St) where Mx>Max => [].
  iota(Cx,St) => [Cx,..iota(Cx+St,St)].
.} in [2,..sieve(iota(3,2))]
----
The ``iota`` function is used to construct a list of numbers, in
this case the integer range from ``3`` through to Max with an
increment of ``2``. We start the ``sieve`` with ``2`` and the
list of integers with ``3`` since we are making use of our prior
knowledge that ``2`` is prime.

It should be emphasized that this implementation of the sieve of Eratosthenes
hardly counts as an efficient algorithm for finding primes. For one thing, it
requires that we start with a list of integers; most of which will be
discarded. In fact, each `'sweep'` of the list of numbers results in a new list of
numbers; many of which too will eventually be discarded. Furthermore, the
``filterMultiples`` function examines every integer in the list; it does not
make effective use of the fact that successive multiples occupy predictable
slots in the list of integers.

In fact, building a highly optimized version of the sieve of
Eratosthenes is not actually the main point here -- our purpose is to
illustrate the power of collections processing functions.

We might ask whether the ``sieve`` function can also be expressed
as a filter. The straightforward answer is that it cannot: the sieve
_is_ a kind of filter, but the predicate being applied depends on
the entire collection; not on each element. The standard filter
function does not expose the entire collection to the
predicate. However, we will see at least one way of achieving the
sieve without any explicit recursion below when we look at folding
operations.

=== Mapping to make new collections

One of the limitations of the filter function is that it does not
create new elements: we can use it to subset collections but we cannot
transform them into new ones. The ``fmap`` function -- part of the
``functor`` contract -- can be used to perform many transformations
of collections.

For example, to compute the lengths of strings in a list we can use
the expression:
[source,star]
----
fmap(size,["alpha","beta","gamma"]:cons[_])
----
which results in the `cons` list:
[source,star]
----
[5,4,5]
----
The ``fmap`` function is defined via the ``functor`` contract --
thus allowing different implementations for different collection
types:
[source,star]
----
contract all c/1 ~~ functor[c] ::= {
  fmap:all a,b ~~ ((a)=>b,c[a])=>c[b].
}
----
Notice how the contract specifies the collection type -- ``c`` --
without specifying the type of the collection's element type. We are
using a different technique here than we used for the ``stream``
and ``filter`` contracts. Instead of using a functional dependency
to connect the type of the collection to the type of the element, we
denote the type of the input and output collections using a _type
constructor_ variable as in ``c[a]`` and ``c[b]``.footnote:[This
also means that the collection type in ``fmap`` must be generic: it
is not possible to implement ``functor`` for strings.]

We are also using a variant of the quantifier. A quantified type
variable of the form ``c/1`` denotes a type constructor variable
rather than a regular type variable. In this case, ``c/1`` means
that the variable ``c`` must be a type constructor that takes one
argument.

The reason for this form of contract is that ``functor`` implies
creating a new collection from an old collection; with a possibly
different element type. This is only possible if the collection is
generic and hence the type expressions ``c[a]`` for the second
argument type of ``fmap`` and ``c[b]`` for its return type.

One might ask whether we could not have used functional dependencies
in a similar way to ``stream`` and ``filter``; for example,
a contract of the form:
[source,star]
----
contract all c,e,f ~~ mappable[c->>e,f] ::=  {
  mmap:((e)=>f,c)=>c.
}
----
However, _this_ contract forces the types of the result of the
``mmap`` to be identical to its input type, it also allows the
implementer of the ``mappable`` contract to fix the types of the
collection elements -- not at all what we want from a ``fmap``.

It is not all that common that we need to construct a list of sizes of
strings. A much more realistic use of ``fmap`` is for
_projection_.

For example, suppose we wanted to compute the average age of a collection of
people, which is characterized by the type definition:

[source,star]
----
person ::= someOne{
  name:string.
  age:()=>float.
}
----

Suppose that we already had a function ``average`` that could
average a collection of numbers; but which (of course) does not
understand people. We can use our average by first of all projecting
out the ages and then applying the average function:

[source,star]
----
average(fmap((X)=>X.age(),People))
----

In this expression we project out from the ``People`` collection
the ages of the people and then use that as input to the average
function.

==== More Type Inference Magic

There is something a little magic about the lambda function in the above
expression: how does the type checker '`know`' that ``X`` can have a
field ``age`` in it? How much does the type checker know about
types anyway?

In this particular situation the type checker could infer the type of
the lambda via the linking between the type of the ``fmap``
function and the type of the ``People`` variable. However, the type
checker is actually capable of giving a type to the lambda even
without this context. Consider the function:
[source,star]
----
nameOf(R) => R.name
----
This function takes an arbitrary record as input and returns the value
of the ``name`` field. The ``nameOf`` function _is_ well
typed, its type annotation just needs a slightly different form than
that we have seen so far:footnote:[Note that the type system
will not _infer_ this generalized type.]
[source,star]
----
nameOf:all r,n ~~ r <~ {name:n} |= (r)=>n
----
This is another example of a _constrained type_: in this case,
the constraint on ``r`` is that it has a field called ``name``
whose type is the same as that returned by ``nameOf`` itself.

The type constraint:
[source,star]
----
r <~ {name:n}
----
means that any type bound to ``r`` must have a ``name`` field
whose type is denoted by the type variable ``n`` in this case.

With this type signature, we can use ``nameOf`` with any type that
that a ``name`` field. This can be a record type; it can also be a
type defined with an algebraic type definition that includes a record
constructor.

=== Compressing collections

Another way of using collections is to summarize or aggregate over
them. For example, the ``average`` function computes a single
number from an entire collection of numbers -- as do many of the other
statistical functions. We can define average using the standard
``foldLeft`` function, which is part of the standard ``folding``
contract:

[source,star]
----
average(C) => (try
    foldLeft((+),0,C)::float/size(C)::float
  catch
    {_ => 0.0})
----

This definition of the ``average`` function is about as close to a
specification of average as is possible in a programming language.

NOTE: the use of coercion here -- coercing both the result of the
``foldLeft`` and ``size`` to float. The reason for doing this is
that functions like ``average`` are '`naturally`' real
functions.footnote:[Real as in the ℝeal numbers.] Without the explicit
coercion, averaging a list of integers will also result in an integer
value -- which is likely to be inaccurate.

Of course, in our definition of ``average`` we need to coerce
_both_ the numerator and denominator of the division because
Star does not have implicit coercion.

Note also our use of a ``try``-``catch`` expression to catch (sic) the case
where we attempt to take the average of an empty collection; and for the
somewhat unlikely potential to take the average of sufficiently large `integer`
values to prevent a reilable coercion to floating point.

The ``foldLeft`` function applies a binary function to a
collection: starting from the first element and successively `adding
up' each of the elements in the collection using the supplied
operator.

.Left Folding a Collection
image::leftfold.png[]

As we noted above, ``foldLeft`` is part of the ``folding``
contract. Like the ``functor`` contract, this uses some more subtle
type expressions:
[source,star]
----
contract all c,e ~~ folding[c->>e] ::= {
  foldRight:all x ~~ (((e,x)=>x),x,c) => x.
  foldLeft:all x ~~ (((e,x)=>x),x,c) => x.
}
----
The ``folding`` contract uses quantifiers in two places: once in
the contract specification and once in the type signature for
``foldLeft`` (and ``foldRight``). What we are trying to express
here is that any implementation of ``folding`` must allow for a
generic function to process the collection.

The ``foldLeft`` (and ``foldRight``) functions have an
'`accumulator`' (of type ``x``) which need not be the same as the
type of the elements of the collection.footnote:[We saw something
similar with the visitor pattern.] This argument acts as a kind of
linking thread during the entire computation -- and represents the
returned value when the fold is complete.

But we can do much more than computing averages with a fold. Recall
that when we realized the sieve of Eratosthenes, we still had a
recursive structure to the program. Furthermore, the way our original
program was written each filter results in a new list of numbers being
produced. Instead of doing this, we can construct a cascade of filter
functions - each level in the cascade being responsible for eliminating
multiples of a specific prime.

The complete cascade filters by checking each level of the cascade:
for example, after encountering 3, 5 and 7, there will be a cascade of
three functions that check each incoming number: one to look for
multiples of 3, one for multiples of 5 and one for multiples of
7. When we encounter the next prime (11) then we glue on to the
cascade a function to eliminate multiples of 11.

Consider the task of adding a filter to an existing cascade of
filters. What is needed is a new function that combines the effect of
the new filter with the old one. The ``cascade`` function takes a
filter function and a prime as arguments and constructs a new function
that checks both the prime _and_ the existing filter:
[source,star]
----
cascade:((integer)=>boolean,integer)=>((integer)=>boolean).
cascade(F,K) => (X)=>F(X) && ~divides(K,X).
----

[aside]
This is a truly higher-order function: it takes a function as argument
and returns another function.

Given ``cascade``, we can reformulate the ``sieve`` function
itself as a ``foldRight`` -- at each new prime step we '`accumulate`'
a new cascaded filter function:
[source,star]
----
stp:(integer,(integer)=>boolean)=>((integer)=>boolean).
stp(X,F) where F(X) => cascade(F,X).
stp(X,F) => F.
----
At each step in the fold we want to know whether to continue to
propagate the existing filter or whether to construct a new filter.

The ``sieve`` function itself is now very short: we simply invoke
``foldRight`` using ``stp`` and an initial '`state`' consisting of
a function that checks for odd numbers:
[source,star]
----
sieve(C) => foldRight(stp,(K)=>K%2=!=0,C).
----

This version of ``sieve`` is not quite satisfactory as, while it does find prime
numbers, it does not report them. A more complete version has to also accumulate
a list of primes that are found. We can do this by expanding the accumulated
state to include both the cascaded filter function and the list of found
primes. Instead of the previous ``stp`` function, we modify it to create the
``step`` function:

[source,star]
----
step:((list[integer],(integer)=>boolean),integer) => (list[integer],(integer)=>boolean).
step(X,(P,F)) where F(X) => ([X,..P],cascade(F,X)).
step(_,(P,F)) => (P,F).
----
and the initial state has an empty list:
[source,star]
----
sieve(C) is fst(foldRight(step,([],(K)=>K%2=!=0),C)).
----
where ``fst`` and ``snd`` are standard functions that pick the
left and right hand sides of a tuple pair:
[source,star]
----
fst:all s,t ~~ ((s,t))=>s.
fst((L,R)) => L
snd:all s,t ~~ ((s,t))=>t.
snd((L,R)) => R
----
There is one final step we can make before leaving our sieve of
Eratosthenes -- we can do something about the initial list of
integers. As it stands, while the sieve program does not construct any
intermediate lists of integers, it still requires an initial list of
integers to filter. However, this particular sequence can be
represented in a very compact form -- as a ``range`` term.

``range`` terms are special forms of collections that denote ranges
of numeric values. For example, the expression
[source,star]
----
.range(0,100,2)
----
denotes the sequence of integers starting at zero, not including 100,
each succesive integer being incremented by 2.

Using a similar ``range`` term, we can denote the list of primes
less than 1000 with
[source,star]
----
primes:(integer) => cons[integer].
primes(Max) => let{.
  cascade:((integer)=>boolean,integer) => ((integer)=>boolean).
  cascade(F,K) => (X)=>(F(X) && ~divides(K,X)).

  step:(integer,(cons[integer],(integer)=>boolean)) =>
     (cons[integer],(integer)=>boolean).
  step(X,(P,F)) where F(X) => ([X,..P],cascade(F,X)).
  step(_,(P,F)) => (P,F).

  sieve:(range[integer])=>cons[integer].
  sieve(R) => fst(foldRight(step,([],(K)=>.true),R)).
.} in sieve(.range(3,Max,2)).
----

This final program has an important property: there are no explicit
recursions in it -- in addition, apart from the ``foldRight``
function, there are no recursive programs at all in the definition of
``primes``.

NOTE: Of course, it still would not count as the _most efficient_
primes finding program; but that was not the goal of this discussion.

== Different types of collection

Just as there are many uses of collections, so there are different
performance requirements for collections themselves. The most
challenging aspects of implementing collections revolves around the
cost of _adding_ to the collection, the cost of _accessing_
elements of the collection and the cost of _modifying_ elements
in the collection.

There is a strong emphasis on _persistent_ semantics for the
types and functions that make up the  collections
architecture. This is manifest in the fact, for example, that
functions that add and remove elements from collections _do not_
modify the original collection.

However, even without that constraint, different implementation
techniques for collections tend to favor some operations at the cost
of others. Hence, there are different types of collection that favor
different patterns of use.

=== The ``cons`` type

This is the simplest collection type; and is perhaps the original
collection type used in functional programming languages. It is
defined by the type declaration:

[source,star]
----
all t ~~ cons[t] ::= .nil | .cons(t,cons[t]).
----

Cons lists have the property that adding an element to the front of a list is a
constant-time operation; similarly, splitting a ``cons`` list into its head and
tail is also a constant time operation. However, almost every other operation is
significantly more expensive: putting an element on to the end of a ``cons``
list is linear in the length of the list.

The main merit of the ``cons`` list is the sheer simplicity of its
definition. Also, for small collections, its simple implementation may outweigh
the advantages that more complex collections offer.

===  The ``map`` type

Unlike the ``cons`` or ``list`` type, the ``map`` type is
oriented for access by arbitrary keys. The ``map`` is also quite
different to hash maps as found in Java (say), the ``map`` type is
_persistent_: the functions that access dictionaries such as by
adding or removing elements return new dictionaries rather than
modifying a single shared structure. However, the efficiency of
``map`` is quite comparable to Java's HashMap.

The template for the ``map`` type is:

[source,star]
----
all k,v ~~ equality[k], hash[k] |= map[k,v]
----

Notice that there is an implied constraint here: the ``map``
assumes that the keys in the map can be compared for equality, and
that they are hashable -- have a ``hash`` function.

A ``map`` value is written as a sequence of key/value pairs -- using a special
arrow term -- enclosed in braces:

[source,star]
----
{1->"alpha",2->"beta”}
----

[NOTE]: For the curious, dictionaries are implemented using techniques similar
to, as described by Bagwell in <<bagwell:2001>>. This
results in a structure with an effective O(1) cost for accessing
elements _and_ for modifying the ``map`` -- all the while
offering an applicative data structure.

=== The ``set`` type

There are many instances where a programmer needs a collection but
does not wish specify any ordering or mapping relationship. The
standard ``set`` type allows you to construct such entities.

Using a ``set`` type offers the programmer a signal that minimizes
assumptions about the structures: the set type is not ordered, and
offers no ordering guarantees. It does, however, offer a guarantee
that operations such as element insertion, search and set operations
like set union are implemented efficiently.

Like ``map``, the ``set`` type is not publicly defined using an
algebraic type definition: its implementation is private. It's type is
given by the template:

[source,star]
----
all t ~~ equality[t] |= set[t]
----

=== Other collection types

Apart from these collection types, the standard library has a range of types
that can be treated as collections:

``heap``:: The `heap` type implements a priority queue. It is based on the binomial heap described in <<okasaki:1999>>.
``multi``:: The ``multi`` tree implements a multi-level binary tree. This data
structure has teh feature that concatenation of two ``multi`` trees is a
constant time operation.
``redblack``:: The `redblack` tree implements a self-balancing binary tree.
``string``:: String values can also be interpreted as collections -- of ``char`` code points.
``vect``:: The ``vect``or type implements a so-called wide tree. The maximum
branching factor for the standard implementation is four. This has the effect of
reducing the depth of a given tree structure by half -- compared to a normal
balanced binary tree.

== Collecting it together

Collections form an important part of any modern programming
language. The suite of features that make up the collections
architecture consists of a number of data types, contracts and special
syntax that combine to significantly reduce the burden of the
programmer.

The collections facility amounts to a form of DSL -- Domain Specific
Language -- that is, in this case, built-in to the language. We shall
see later on that, like many DSLs, this results in a pattern where
there is a syntactic extension to the language that is backed by a
suite of contracts that define the semantics of the DSL.
