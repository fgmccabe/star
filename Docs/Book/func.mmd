#Functional Programming[functional-programming]

There is a perception of functional programming that it is _weird_ and _difficult_. This is unfortunate for a number of reasons; the most important being that functional programming is _not_ weirder than procedural programming and that all programmers can benefit by programming functionally.

As for being difficult, a more accurate description would be that there is a deeper _range of features_ in functional programming than in most modern programming languages: so a perception of complexity can arise simply because there is more to say about functional programming languages. However, the simplest aspects of functional programming are very simple and the ramp need not be steep.

What may be surprising to the reader who is not familiar with functional programming is that it is _old_: predating the origins of modern computing itself, that there is a huge amount that can be expressed functionally, and that functional programming is often at least as efficient and sometimes more efficient than procedural programming.

In this chapter we will show how we can utilize **Star** as a vehicle for functional programming. As a side-goal, we also hope to demystify some of the language and ideas found in functional programming.

##What is Functional Programming?

The foundations of functional programming rest on two principles:

1.  Programs are expressed in terms of functions, where a somewhat mathematical view of functions is taken — functions always produce the same output for the same input.

	This is what people mean when they say that functional programs are _declarative_.[^The term _declarative_ has a technical definition. But this captures much of the essence of declarativeness (sic).]

2. Functions are values: they can be passed as arguments to functions, returned from functions, and they can be put into and retrieved from data structures.

This last principle is what people mean when they refer to functions as being _first class_ values. It is also what is meant when we say that functional programming languages are _higher order_.

**Star** is not actually a _pure_ functional programming language; i.e., it is possible to write programs that violate the declarative principle. However, it is a _functional-first_ programming language: a declarative style is strongly encouraged. In this chapter we shall focus on writing pure functional programs in
**Star**.

##Basics

There are two variants of function — the anonymous or _lambda_ function and the named function. We have already seen some simple examples both forms; for now, we continue to focus on named functions.

It is often easier to introduce functional programming using numerical examples. [Last chapter][a-first-Star-program] we saw, for example, the `factorial` program. This is mostly because most programmers are already familiar with numbers. Continuing that tradition, here is a function that returns the sign of a number:

```
sign:(integer) => integer.
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) where X>0 => 1.
```

Each of these equations applies to different situations: the first equation applies when the input argument is negative, the second when it is exactly zero and the third when it is strictly positive. These represent the three possible cases in the definition of the sign function.

A **Star** function may be built from any number of rewrite equations; however, they must all be contiguous within the same group of statements.

Although it is good practice to ensure that equations in a function definition do not overlap, **Star** will try the equations in a function definition in the order they are written in. We could have relied on this and written `sign` using:

```
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) => 1.
```

Sometimes it is important to mark a particular equation as the _default_ case: i.e., an equation that should be used if none of the other cases apply:

```
sign(X) where X<0 => -1.
sign(0) => 0.
sign(X) default => 1.
```

An explicitly marked `default` equation does not need to be the last equation; but, wherever it is written, default equations are only attempted after all other equations have failed to apply.

###Patterns

A function is defined as a sequence of _rewrite equations_ each of which consist of a _pattern_ and an _expression_. There are three general forms of rewrite equations:

_Pattern_ `=>` _Expression_  

or

_Pattern_ `where` _Condition_ `=>` _Expression_

or

_Pattern_ `default =>` _Expression_

The left hand side of a rewrite equation consists of the pattern which determines the applicability of the equation; and the right hand side represents the value of the function if the pattern matches.

**Pattern**
: A _pattern_ represents a test or guard on a value. Patterns can be said to _succeed_ or _fail_ depending on whether the value being tested matches the pattern.

We also refer to a pattern being _satisfied_ when matching a value.[^This terminology originates from Logic — where a formula can be satisfied (made true) by observations from the world.]

The pattern in a rewrite equation is a guard on the arguments of the function call. For example, given a call

```
sign(34)
```

the patterns in the different equations of the `sign` function will be applied to the integer value `34`.

When the pattern on the left hand side of a rewrite equation succeeds then the equation _fires_ and the value of the expression on the right hand side of the equation becomes the value of the function.

As we noted earlier, patterns are ubiquitous in **Star**. They are used in equations, in variable declarations, in queries and in many other places. Here, we shall look at three main kinds of pattern, and in later sections, we look at additional forms of patterns.

**Variable Pattern**
: A _variable pattern_ is denoted by an identifier; specifically by the _first occurrence_ of an identifier.

A variable pattern always succeeds and has the additional effect of _binding_ the variable to the value being matched.

For example, the `X` in the left hand side of

```
double(X) => X+X
```

is a variable pattern. Binding `X` means that it is available for use in the right hand side of the equation — here to participate in the expression `X+X`.

The part of the program that a variable has value is called its _scope_.

*  Variables in rewrite equations always have scope ranging from the
  initial occurrence of the variable through to the whole of the right
  hand side of the equation.

*  Variable patterns are the _only_ way that a variable can get a
  value in **Star**.

*  Variables are never redeclared within a given scope. It is not
  permitted to hide a variable with a new variable that is defined
  within the natural scope of the variable.

  This is somewhat different to the scope rule for most other functional
  (and non-functional) languages — which allow outer scoped variables
  to be effectively eclipsed or hidden by inner variables.


  >The rationale for this choice is based on the observation that errors
  that arise from mistakenly hiding outer variables are often
  particularly difficult to track down.

**Literal Pattern**
: A _literal pattern_ — such as a numeric literal or a string literal — only matches the identical number or string.

Clearly, a literal match amounts to a comparison of two values: the pattern match succeeds if they are identical and fails otherwise.

Equality is based on _semantic equality_ rather than _reference equality_. What this means, for example, is that two strings are equal if they have the same sequence of characters in them, not just if they are the same object in memory.

There is no automatic coercion of values to see if they _might_ match. In particular, an `integer` pattern will only match an `integer` value and will not match a `float` value — even if the numerical values are the same. I.e., there will be no attempt made to coerce either the pattern or the value to fit.

>This, too, is based on the desire to avoid hard-to-detect bugs from leaking into a program.

**Guard Pattern**
: Sometimes known as a _semantic guard_, a guard pattern consists of a pair of a pattern and a _condition_:

>_Pattern_ `where` _Condition_

Conditions are `boolean`-valued and the guard succeeds if both the pattern matches and if the condition is _satisfied_. **Star** has a normal complement of special conditional expressions which we shall explore as we encounter the need. In the case of the equation:

```
 sign(X) where X>0
```

the guard pattern is equivalent to:

```
X where X>0
```

We can put guard pattern anywhere that a pattern is valid; and, for convenience, we can also put them immediately to the left of the rewrite equation's `is` operator.

>Notice that any variables that are bound by the pattern part of a guarded pattern are _in scope_ in the condition part of the guard.

In the pattern above, the variable `X` will be bound in the variable pattern `X` and will then be tested by evaluating the condition `X>0`.

Subsequent occurrences of variables in a pattern 'stand for' `equality` guards. For example, the equation:

```
same(X,X) => true
```

is exactly equivalent to:

```
same(X,X1) where X==X1 => true
```
or just:

```
same(X,X1) => X==X1
```

###Order of Evaluation

**Star** is a so-called _strict_ language. What that means is that arguments to functions are evaluated prior to calling the function. Most programming languages are strict; for two main reasons:

1.  It is significantly easier for programmers to predict the evaluation characteristics of a strict language.

1.  It is also easier to implement a strict language efficiently on modern hardware. Suffice it to say that modern hardware was designed for evaluating strict languages, so this argument is somewhat circular.

There many possible styles of evaluation order; one of the great merits of programming declaratively is that the order of evaluation does not affect the actual results of the computation.

>It may, however, affect whether you get a result. Different strategies for evaluating expressions can easily lead to differences in which programs terminate and which do not.

One other kind of evaluation that is often considered is _lazy_ evaluation. Lazy evaluation means simply that expressions are only evaluated _when needed_. Lazy evaluation has many potential benefits: it certainly enables some very elegant programming techniques.

Essentially for the reasons noted above, **Star** does not use lazy evaluation; however, as we shall see, there are features of **Star** that allow us to recover some of the power of lazy evaluation.[^Even predominantly lazy languages like Haskell have   features which implement strict evaluation. It reduces to a question   of which is the _default_ evaluation style.]

The other dimension in evaluation order relates to the rewrite equations used to define functions. Here, **Star** uses an in-order evaluation strategy: the equations that make up the definition of a function are tried in the order that they are written — with the one exception being any `default` equation which is always tried last.[^There is a theorem — called the _Church Rosser Theorem_ — that guarantees some independence on the order of the rewrite equations provided that the different rewrite equations that make up function definitions do not overlap. Usually, however, it is too fussy to _require_ programmers to ensure that their equations  do not overlap; hence the reliance on ordering of equations.]

## Another Look at Types
Organizing data is fundamental to any programming language. **Star**'s data types are organized around the _algebraic data type_. In addition, **Star**'s supports *quantified types* of several varieties.

###Quantified Types

A *generic* type is one which has one or more type variables in it. For example, the type expression:

```
(x,x)=>boolean
```
is such a generic type (assuming that `x` is a type variable -- see below).

All type variables must be bound by a quantifier in some enclosing scope. If a type variable is not bound, it is considered _free_ in that type expression.

A  _quantified type_ is a type that introduces (i.e., binds) a type variable. There are two quantifiers in **Star**: a universal quantifier and an existential quantifier.

The most common quantifier is the  _universal quantifier_ and universally quantified types correspond closely to generic types in other languages.

Universally quantified types are often used to denote function types and collection types. For example, the type

```
all x ~~ (x,x)=>boolean
```
is a universally quantified type that denotes the type of a generic binary function that returns a `boolean` value. The standard type for the equality predicate `==` is similar to this type.

A universal type should be read as 'for all possible values' of the bound variable. For example, this function type should be read as denoting functions that:

>for any possible type – `x` – the function takes two such `x`'s and returns a `boolean`.

**Star** also supports  _existentially_ quantified types — these are useful for denoting the types of modules and/or abstract data types. However, we will leave our exploration of existential types for later.

### Contract Constrained Types
We noted above that the type of the standard equality predicate was 'almost' the same as:

```
all x ~~ (x,x)=>boolean
```
This type denotes a universally quantified function type that can be applied to arguments of any given type. However, equality in a normal programming language is *not* universal: not all values admit to being reliably tested for equality. A great example of such a limitation are functions – equality between functions is not well defined.[^Strictly, not decidable.]

To capture this, we need to be able to constrain the scope of the quantifier; specifically to those argument types that do admit equality.

We can do this by adding a *contract constraint* to the type – the constraint states that `equality` must be defined for the type. We do this by prepending a constraint clause to the type:

```
all x ~~ equality[x] |: (x,x)=>boolean
```

The `equality[x] |:` clause states that the type variable `x` must satisfy the equality.

What is implied when we state this? This is captured in the definition of the contract itself, in this case the `equality` contract.

The `equality` contract is defined using a `contract` statement:

```
contract all t ~~ equality[t] ::= {
  (==) : (t,t) => boolean.
  hash:(t)=>integer
}
```

In effect, the `equality` contract states that there must be two functions defined – `==` and `hash`.

If this seems a little circular, it is not. The `equality` contract is effectively saying that the `==` function must be defined for the type; and we constrain function (and other) types with the `equality` contract constraint when we need to ensure that `==` is defined!

We provide evidence for contracts through the `implementation` statement. This declares that a given type satisfies a contract by providing implementations for the functions in the contract.

For example, we can provide evidence that the `equality` contract applies to `string`s using a built-in primitive to actually implement equality for strings:

```
implementation equality[string] => {
  s1 == s2 => _string_eq(s1,s2).
  hash(s) => _string_hash(s).
}
```

We shall explore more fully the power of this form of type constraint in later sections and chapters. For now, the core concept is that quantified types can be constrained to allow very precise formulations of types.

###Algebraic Data Types[algebraic-data-types]

An algebraic data type definition achieves several things simultaneously: it introduces a new type into scope, it gives an enumeration of the legal values of the new type and it defines both constructors for the values and it defines patterns for decomposing values. This is a lot for a single statement to do!

For example, we can define a type that denotes a point in a two-dimensional space:

```
type point ::= cart(float,float).
```

This kind of statement is called a _type definition statement_ and is legal in the same places that a function definition is legal.

The new type that is named by this statement is `point`; so, a variable may have `point` type, we can pass `point` values in functions and so on.

The constructor `cart` allows us to have expression that allow 'new' `point` structures to be made:

```
cart(3.4,2.1)
```

`cart` is also the name of a _pattern_ operator that we can use to take apart `point` values. For example, the `euclid` function computes the Euclidian distance associated with a `point`:

```
euclid:(point)=>float.
euclid(cart(X,Y)) => sqrt(X*X+Y*Y).
```

Of course, this particular `point` type is based on the assumption that point values are represented in a cartesian coordinate system. One of the more powerful aspects of algebraic data types is that it is easy to introduce multiple alternate forms of data. For example, we might want to support two forms of point: in cartesian coordinates and in polar coordinates. We can do this by introducing another case in the type definition statement:

```
point ::= cart(float,float)
        | polar(float,float).
```

Of course, our `euclid` function also needs updating with the new case:

```
euclid:(point)=>float.
euclid(cart(X,Y)) => sqrt(X*X+Y*Y).
euclid(polar(R,T)) => R.
```

`cart` and `polar` are called _constructor functions_. The term *constructor* refers to the common programming concept of constructing data structures. They are called functions because, logically, they _are_ functions.

For example, we can give a type to `polar`:

```
polar : (float,float)=>point
```

In fact, constructor functions are one-to-one functions. Variously known as _free functions_ (in logic), _bijections_ (in Math), one-to-one functions are guaranteed to have an inverse. This is the logical property that makes constructor functions useful for representing data.

>Of course, we are talking of a _logical_ property of constructor functions. Internally, when implementing functional languages like **Star**, the values returned by constructor functions are represented using data laid out in memory — just like any other programming language.

**Star** actually employs a special type for constructor functions; so the correct type of `polar` is given by:

```
polar : (float,float)<=>point
```

The double arrow representing the fact that constructor functions are bijections.

In addition to constructor functions, an algebraic type definition can introduce two other forms of data: _enumerated symbols_ and _record functions_. Enumerated symbols are quite useful in representing symbolic alternatives. The classic example of an enumerated type is `daysOfWeek`:

```
daysOfWeek ::= monday
           | tuesday
           | wednesday
           | thursday
           | friday
           | saturday
           | sunday.
```

Another example is the standard `boolean` type which is defined:

```
boolean ::= true | false.
```

Unlike enumerated symbols in some languages, there is no numeric value associated with an enumeration symbol: an enumerated symbol 'stands for' itself only. The reason for this will become clear in our next type definition which mixes enumerated symbols with constructor functions:

```
sTree ::= sEmpty | sNode(sTree,string,sTree)
```

In addition to mixing the enumerated symbol (`sEmpty`) with the `sNode` constructor, this type is _recursive_: in fact, this is a classic binary tree type where the labels of the non-empty nodes are `string`s. (We shall see shortly how to generalize this).

Whenever you have a recursive type, its definition must always include one or more cases that are not recursive and which can form the base case(s). In that sense, an enumerated symbol like `sEmpty` plays a similar role in **Star** has some of the same character as `null` does in other languages; except that `sEmpty` is only associated with the `sTree` type.

We can use `sTree` to construct binary trees of `string` value; for example:

```
sNode(sNode(sEmpty,"alpha",sEmpty),
      "beta",
      sNode(sEmpty,"gamma",sEmpty))
```

denotes the tree in:

![A Binary `string` Tree][sTree]

[sTree]:images/sTree.jpg width=200px

>One of the hallmarks of languages like **Star** is that _every_ value has a legal syntax: it is possible to construct an expression that denotes a literal value of any type.

Just as we can define `sTree` values, so we can also define functions over `sTree`s. For example, the `check` function returns `true` if a given tree contains a particular search term:

```
check:(sTree,string) => boolean.
check(sEmpty,_) => false.
check(sNode(L,Lb,R),S) => Lb=S || check(L,S) || check(R,S)
```

Here we see several new aspects of **Star** syntax:

*  An empty pattern — marked by `_` — matches anything. It is called the _anonymous pattern_ and is used whenever we don't care about the actual content of the data.

*  The `||` disjunction is a _short-circuit_ disjunction; much like `||` in languages like Java.  Similarly, conjunction (`&&`) is also short-circuiting.

*  Functions can be recursive. **Star** permits _mutual recursion_ just as easily: there is no special requirement to order function definitions in a program.

We can use `sTest` to check for the occurrence of particular strings:

```
T : sTree.
T = sNode(sNode(sEmpty,"alpha",sEmpty),
               "beta",
               sNode(sEmpty,"gamma",sEmpty)).
show check(T,"alpha").           -- results in true
show check(T,"delta").          -- results in false
```

##Functions as Values

The second principle of functional programming is that functions are first class. What that means is that we can have functions that are bound to variables, passed into functions and returned as the values of functions. In effect, a function is a legal _expression_ in the language. It also means that we can have _function types_ in addition to having types about data.

We can see this best by looking at a few examples. One of the benefits of passing functions as arguments to other functions is that it makes certain kinds of parameterization easy. For example, suppose that you wanted to generalize `check` to apply an arbitrary test to each node — rather than just looking for a particular `string`.

We will first of all define our `fTest` function itself:

```
fTest:(sTree,(string)=>boolean)=>boolean.
fTest(sEmpty,_) => false.
fTest(sNode(L,Lb,R),F) => F(Lb) || fTest(L,F) || fTest(R,F).
```

The substantial change here is that, rather than passing a string to look for, we pass `fTest` a `boolean`-valued function to apply; within `fTest` we replace the equality test `Lb==S` with a call `F(Lb)`.

>Notice that the type annotation for `fTest` shows that the type of the second argument is a function type – from `string` to `boolean`.

Given `fTest`, we can redefine our earlier `check` function with:

```
check(T,S) => fTest(T,(X)=>X==S)
```

We have a new form of expression here: the _anonymous function_ or _lambda expression_. The expression

```
(X)=>X==S
```

denotes a function of one argument, which returns `true` if its argument is the same value as `S`.

Free variables are a powerful feature of functional programming languages because they have an _encapsulating_ effect (the lambda encapsulates the free variable so that the `fTest` function does not need knowledge of `S`).

Interestingly, it would be difficult to define a top-level function that is equivalent to this lambda because of the occurrence of the variable `S` in the body of the lambda. This is an example of a _free variable_: a variable that is mentioned in the body of a function but which is defined outside the function. Because `S` is free, because it is not mentioned in the arguments, one cannot simply have a function which is equivalent to the lambda as a top-level function.

###Functions and Closures

If a function is an expression, what is the value of the function expression? The conventional name for this value is _closure_:

**Closure**
: A _closure_ is a structure that is the value of a function expression and which may be applied to arguments.

It is important to note that, as a programmer, you will never 'see' a closure in your program. It is an implementation artifact in the same way that the representation of floating point numbers is an implementation artifact that allow computers to represent fractional numbers but which programmers (almost) never see explicitly in programs.

Pragmatically, one of the important roles of closures is to capture any free variables that occur in the function. Most functional programming languages implement functions using closure structures. Most functional programming languages (including **Star**) do not permit direct manipulation of the closure structure: the only thing that you can _do_ with a closure structure is to use it as a function.

>In the world of programming languages, there is a lot of confusion  about closures. Sometimes you will see a closure referring to a function that captures one or more free variables.

###Let Binding Environments[let-binding-environments]

We noted that it is difficult to achieve the effect of the `(X)=>X==S` lambda expression with named functions. The reason is that the free variable `S` is defined as a parameter to the `check` function — in particular, the lambda is _not_ defined in the same way that named functions are defined. If we wanted to define a named function which also captures `S`, we would have to be able to define functions inside expressions.

There is an expression that allows us to do this: the `let` expression. A `let` expression allows us to introduce local definitions anywhere within an expression. We can define our lambda as the named function `isS` using the `let` expression:

```
let{
  isS:(string)=>boolean.
  isS(X) => X==S
} in isS
```

The region between the braces is a _definition environment_ and **Star** allows _any_ definition statement to be in such an environment. We can define `check` using a `let` expression:

```
check(T,S) => fTest(T,
  let{
    isS:(string)=>boolean.
    isS(X) => X==S.
  } in isS)
```

This is a somewhat long-winded way of achieving what we did with the anonymous lambda function. However, there is a strong inter-relationship between anonymous lambdas, `let` expressions and variable definitions. These are all equivalent:

```
let{
    isS:(string)=>boolean.
    isS(X) => X==S.
} in isS
```

```
let{
  isS:(string)=>boolean.
  isS = (X) => X==S
} in isS
```

and

```
(X)=>X==S
```

Apart from being long-winded, the `let` expression is significantly more flexible than a simple lambda. It is much easier within a `let` expression to define functions with more than one rewrite equation; or to define multiple functions. We can even define local types within a `let` binding environment.

Conversely, lambda functions are so compact because they have strong limitations: you cannot easily define a multi-rewrite equation function with a lambda and you cannot easily define a recursive function as a lambda.

In short, we would use a `let` expression when the function being defined is at all complex; and we would use a lambda when the function being defined is simple and small.

Assembling functions in this way, either by using anonymous lambdas or by using `let` expressions, is one of the hallmarks of functional programming.

##Generic Programs[generic-programs]
If we take a slightly closer look at `fTest`, we can see that it seems pretty generic: it has a function as argument that it calls in the right places; but otherwise makes few assumptions about the function it calls. However, the type annotation of `fTest` is rather specific:

```
fTest : (sTree,(string)=>boolean)=>boolean
```

which means, for example, that its second argument _must_ be a function from `string` to `boolean`. On the other hand, nothing in the actual definition of `fTest` seems to depend on `string`s.

###Generic Types[generic-types]

What actually makes `fTest` more constrained than it could be is the type definition of `sTree` itself. It too is unnecessarily restrictive: why not allow trees of any type? We can, using the type definition for `tree`:

```
all t ~~ tree[t] ::= tEmpty | tNode(tree[t],t,tree[t]).
```

Like the original `sTree` type definition, this statement introduces a new type: `tree[t]` which can be read as 'tree of something'. The name `tree` is not actually a type identifier — although we often refer to the `tree` type — but is a _type constructor_.

>In an analogous fashion to constructor functions, a type constructor constructs types from other types. Type constructors are even bijections — one-to-one functions from types to types.

The identifier `t` in the type definition for `tree` denotes a _type variable_. Again, similarly to regular variables and parameters, a type variable denotes a single unspecified type. The role of the type variable `t` is like a parameter in a function: it identifies the unknown type and its role.

The `tree` type is a universally quantified type. What that means is that instead of defining a single type it defines a family of related types: for example:

```
tree[string]
tree[integer]
...
```

are `tree` types. We can even have `tree`s of `tree`s:

```
tree[tree[string]]
```

We capture this genericity of the `tree` type by using a _universal quantifier_:

```
all t ~~ tree[t]
```

What this type expression denotes is a set of possible types: for any type `t`, `tree[t]` is also a type. There are infinitely many such types of course.

The `all` quantifier is important: as in logic, there are two kinds of quantifiers in **Star**'s type system: the _universal_ `all` and the _existential_ `exists`. We will take a deeper look at the latter in a later chapter on modular programming.

>**Star** uses context to determine whether an identifier is a type variable or a type name. Specifically, if an identifier is bound by a quantifier then it must refer to a type variable.

The types of the two constructors introduced in the `tree` type definition are similarly quantified:

```
tEmpty:all t ~~ tree[t]
.tNode:all t ~~ (tree[t],t,tree[t)]<=>tree[t].
```

The type `tree[t]` on the right hand side of `tEmpty`'s type annotation raises a couple of interesting points:

1. This looks like a type annotation with no associated definition. The fact that the `tEmpty` symbol was originally introduced in a type definition is enough of a signal for the compiler to avoid looking for a definition for the name.

2.  The type of a literal `tEmpty` expression — assuming that no further information is available — will be of the form `tree[t34]` where `t34` is a 'new' type variable not occurring anywhere else in the program. In effect, the type of `tEmpty` is `tree` of _some type_ `t34` where we don't know anything more about `t34`.

###Generic Functions

Given this definition of the `tree` type, we can construct a more general form of the tree test function; which is almost identical to `fTest`:

```
test:all t ~~ (tree[t],(t)=>boolean)=>boolean.
test(tEmpty,_) => false.
test(tNode(L,Lb,R),F) => F(Lb) || test(L,F) || test(R,F).
```

and our original string `check` function becomes:

```
check(T,S) => test(T,(X) => X==S)
```

The type of `check` can also be more generic:

```
check:all t ~~ (tree[t],t)=>boolean
```

I.e., `check` can be used to find any type of element in a tree — providing that the types align of course.

>Actually, this is not the correct the type for `check`. This is because we do not, in general, know that the type can support equality. The precise type for `check` should take this into account:
>
```
check:all t ~~ equality[t] |: (tree[t],t) => boolean
```

##Going Further[going-further]

Although better than the original `sTest` program there is still one major sense in which the `test` program is not general enough. We can see by looking at another example: a function that counts elements in the tree:

```
count:all t ~~ (tree[t]) => integer.
count(tEmpty) => 0.
count(tNode(L,_,R)) => count(L)+count(R)+1
```

This code is very similar, but not identical, to the `test` function.

The issue is that `test` is trying to do two things simultaneously: in order to apply its test predicate to a binary tree it has to implement a walk over the tree, and it also encodes the fact that the function we are computing over the `tree` is a `boolean`-value function.

We often need to do all kinds of things to our data structures and writing this kind of recursion over and over again is tedious and error prone. What we would like to do is to write a single _visitor_ function and specialize it appropriately when we want to perform a specific function.

>This principle of separating out the different aspects of a system is one of the core foundations of good software engineering. It usually goes under the label _separation of concerns_. One of the beautiful things about functional programming is that it directly supports such good architectural practices.

Since this visitor may be asked to perform any kind of computation on the labels in the `tree` we will need to slightly generalize the type of function that is passed to the visitor. Specifically, the type of function should look like:

```
F : (a,t)=>a
```

where the `a` input represents accumulated state, `t` represents an element of the `tree` and the result is another accumulation.

Using this, we can write a `tVisit` function that implements tree walking as:

```
tVisit:all a,t ~~ (tree[t],(a,t)=>a,a)=>a.
tVisit(tEmpty,_,A) => A.
tVisit(tNode(L,Lb,R),F,A) => tVisit(R,F,F(tVisit(L,F,A),Lb)).
```

Just as the accumulating function acquires a new 'state' parameter, so the `tVisit` function also does. The `A` parameter in the two equations represents this accumulated state.

The second rewrite equation for `tVisit` is a bit dense so let us open it out and look more closely. A more expanded way of writing the `tVisit` function would be:

```
tVisit(tEmpty,_,A) => A.
tVisit(tNode(L,Lb,R),F,A) => let{
      A1 : a.
      A1 = tVisit(L,F,A).
      A2 : a.
      A2 = F(A1,Lb).
    } in tVisit(R,F,A2)
```

where `A1` and `A2` are two local variables that represent the result of visiting the left sub-tree and applying the accumulator function respectively. We have used the `let` expression form to make the program more obvious, rather than to introduce new functions locally; but this is a legitimate role for `let` expressions.

The `tVisit` function knows almost nothing about the computation being performed, all it knows about is how to walk the tree and it knows to apply functions to labels in the `tree`.

Given `tVisit`, we can implement our original `check` and `count` functions as one-liners:

```
check(T,S) => tVisit(T,(A,X)=>(A || X==S),false).
count(T) => tVisit(T,(A,X)=>A+1,0).
```

> Notice that we have effectively hidden the recursion in these function definitions — all the recursion is encapsulated within the `tVisit` function. One of the unofficial mantras of functional programming is _hide the recursion_.

The reason we want to hide recursions that this allows the designer of functions to focus on _what_ is being computed rather than focusing on the structure of the data and, furthermore, this allows the implementation of the visitor to be _shared_ by all users of the `tree` type.

Notice that, while `a` and `t` are type variables, we did not put an explicit quantifier on the type of `F`. This is because the quantifier is actually put on the type of `tVisit` instead:

```
tVisit:all a,t ~~(tree[t],(a,t)=>a,a)=>a
```

Just like regular variables, type variables have scope and points of introduction. Also like regular variables, a type variable may be _free_ in a given type expression; although it must ultimately be _bound_ by a quantifier.

###Going Even Further[going-even-further]

We have focused so far on generalizing the visitor from the perspective of the `tree` type. But there is another sense in which we are still _architecturally entangled_: from the perspective of the `check` and `count` functions themselves.

In short, they are both tied to our `tree` type. However, there are many possible collection data types; **Star** for instance has some 5 or 6 different standard collection types. We would prefer not to have to re-implement the `check` and `count` functions for each type.

The good news is that, using contracts, we can write a single definition of `check` and `count` that will work for a range of collection types.

Let us start by defining a contract that encapsulates what it means to `visit` a collection:

```
contract visitor[c->>t] ::= {
  visit:all a ~~ (c,(a,t)=>a,a)=>a
}
```

This `visitor` contract defines a single function that embodies what it means to _visit_ a collection structure. There are quite a few pieces here, and it is worth examining them carefully.

A contract header has a template that defines a form of _contract constraint_. The clause

```
visitor[c ->> t]
```

is such a constraint. The sub-clause

```
        c ->> t
```

refers to two types: `c` and `t`. The presence of the `->>` term identifies the fact that `t` depends on `c`.

The `visitor` contract itself is about the collection type `c`. But, within the contract, we need to refer to both the collection type and to the type of elements in the collection: the `visit` function is over the collection, it applies a function to elements of the collection.

Furthermore, as we design the contract, we _do not know_ the exact relationship between the collection type and the element type. For example, the collection type may be generic in one argument type — in which case the element type is likely that argument type; conversely, if the type is _not_ generic (like `string` say), then we have no direct handle on the element type.

We _do know_ that within the contract the element type is _functionally determined_ by the collection type: if you know the collection type then you should be able to figure out the element type.

We express this dependency relationship with the the `c ->> t` form: whatever type `c` is, `t` must be based on it.

The body of the contract contains a single type annotation:

```
visit:all a ~~(c,(a,t)=>a,a)=>a
```

This type annotation has three type variables: the types `c` and `t` come from the contract header and `a` is local to the signature. What the signature means is

> Given the `visitor` contract, the `visit` function is from the collection type `c`, a function argument and an initial state and returns a new accumulation state.

It is worth comparing the type of `visit` with the type of `tVisit`:

```
tVisit:all t,a ~~(tree[t],(a,t)=>a,a)=>a
```

The most significant difference here is that in `tVisit` the type of the first argument is fixed to `tree[t]` whereas in `visit` it is left simply as `c` (our collection type).

Given this contract, we can re-implement our two `check` and `count` functions even more succinctly:

```
check(T,S) => visit(T, (A,X)=>A || X==S,false)
count(T) => visit(T, (A,X)=>A+1,0)
```

These functions will apply to _any_ type that satisfies — or implements — the `visitor` contract. This is made visible in the revised type signature for `count`:

```
count:all c,t ~~ visitor[c->>t] |: (c)=>integer
```

This type is an example of a _constrained type_. It is generic in `c` and `t` but that generality is constrained by the requirement that the `visitor` contract is appropriately implemented. The eagled-eyed reader will notice that `count` does not actually depend on the type of the elements in the collection: this is what we should expect since `count` does not actually care about the elements themselves.

The type signature for `check`, however, does care about the types of the elements:

```
check:all c,t ~~
    visitor[c->>t], equality[c] |: (c,t)=>boolean
```

This type annotation now has two contract constraints associated with it: the collection must be something that is visitable and the elements of the collection must support equality.

Given the work we have done, we can implement the `visitor` contract for our `tree[t]` type quite straightforwardly:

```
implementation visitor[tree[t]->>t] => {
  visit = tVisit
}
```

Notice that header of the `implementation` statement provides the connection between the collection type (which is `tree[t]`) with the element type (`t`). The clause

```
visitor[tree[t]->>t]
```

is effectively a declaration of that connection.

Now that we have disconnected `visit` from `tree` types, we can extend our program by implementing it for other types. In particular, we could also implement the `visitor` for the `sTree` type:

```
implementation visitor[Tree ->> string] => {
  visit = sVisit
}
```

however, we leave the definition of `sVisit` as a simple exercise for the reader.

Our final versions of `count` and `check` are now quite general: they rely on a generic implementation of the `visit` function to hide the recursion and are effectively independent of the actual collection types involved.

If we take a second look at our `visitor` contract we can see something quite remarkable: it counts as a definition of the famous _visitor pattern_. This is remarkable because although visitor patterns are a common design pattern in OO languages, it is often hard in those languages to be crisp about them; in fact, they are called patterns because they represent patterns of use which may be encoded in Java (say) whilst not necessarily being definable in them.

The combination of `contract` and `implementation` represents a quite formal way of defining patterns like the visitor pattern.

There is something else here that is quite important too: we are able to define and implement the `visitor` contract _without_ having to modify in any way the type definition of `tree` or `sTree`. From a software engineering point of view this is quite important: we are able to gain all the benefits of interfaces without needing to entangle them with our types. This becomes critical in situations where we are not able to modify types — because they don't belong to us and/or we don't have access to the source.

###Polymorphic Arithmetic[polymorphic-arithmetic]

There are other ways in which programs can be polymorphic. In particular, let us focus for a while on arithmetic. One of the issues in arithmetic functions is that there are many different kinds of numbers. Pretty much every programming language distinguishes several kinds of numbers; for example, Java distinguishes `byte`, `short`, `int`, `long`, `float`, `double`, `BigInteger` and `BigDecimal` — and this does not count the wrapped versions. Other languages have even more choice.

One question that might seem relevant is why? The basic answer is that different applications call for different properties of numbers and no one numeric type seems to fit all needs. However, the variety comes at a cost: when we _use_ numbers we tend to have to make too early a choice for the numeric type.

For example, consider the `double` function we saw earlier:

```
double(X) => X+X
```

What type should `double` have? In particular, what should the type of `+` be? Most people would be reluctant to use different arithmetic operators for different types of numbers.[^Although some languages — such as ML — do require this.] This is resolved in **Star** by relying on contracts for the arithmetic operations.

The result is that the type computed for `double` is exquisitely tuned:

```
double:all t ~~ arithmetic[t] |: (t)=>t
```

This type is precisely the minimal type that `double` could have. Any further constraints result in making a potentially premature choice for the numeric type.

If we take another look at our factorial function:

```
fact(0) => 1
fact(N) => N*fact(N-1)
```

this is constrained to be a function from `integer` to `integer` because we introduced the literal integers `0` and `1`. However, the `arithmetic` contract contains synonyms for these very common literals. Using `zero` and `one` allow us to be abstract in many arithmetic functions:

```
genFact:all a ~~ arithmetic[a] |: (a)=>a.
genFact(zero) => one.
genFact(N) => N*genFact(N-one).
```

>We call out `zero` and `one` for special treatment because they occur very frequently in numerical functions.

We can actually introduce other numeric literals without compromising our type by using _coercion_; although it is more clumsy:

```
factorialC:all t ~~
    arithmetic[t],coercion[integer,t] |: (t)=>t.
factorialC(N) where N==0 :: t => 1 :: t.
factorialC(N) => N*factorialC(N-1 :: t).
```

The expressions `0 :: t` and `1 :: t` are coercions from `integer` to `t`.

Of course, coercion is also governed by contract, a fact represented in the type signature by the `coercion` contract constraints on the type of `t`.

In any case, using these techniques, it is possible to write numeric functions without unnecessarily committing to specific number types. That in turn helps to make them more useful.

##A Word About Type Inference

We have seen some powerful forms of types in this chapter: recursive types defined using algebraic type definitions, generic types and even function types. Recall also that **Star** only requires programmers to explicitly declare the types of top-level variables and functions. It is worth pausing a little to see how this might be done.

Recall our original `factorial` function:

```
fact(0) => 1.
fact(N) => N*fact(N-1).
```

The compiler is able to compute the types of the various variables automatically through a process known as _type inference_. Type inference may seem magical, but is actually (mostly) quite simple. Let us take a look at the expression:

```
N-1
```

which is buried within the recursive call in `fact`. Although it looks like a special operator, **Star** does not treat arithmetic expressions in a special way; the `-` function is just a function from numbers to numbers.

The type of arithmetic subtraction is given by:

  ```
  (-) : all t ~~ arithmetic[t] |: (t,t)=>t
  ```
 However, we should simplify this a little in order to make the explanation of type inference a little simpler:

```
(-) : (integer,integer)=>integer
```

>This is a legal **Star** type annotation: the parentheses around the `-` operator are used to signal a use of the operator as a stand-alone identifier; and to suppress its use as an infix operator.

Type inference proceeds by using special _type inference rules_ which relate expressions to types, in this case the applicable rule is that a function application is consistent if the function's parameter types are consistent with the types of the actual arguments. If they are consistent, then the type of the function application is the return type of the function.

The type inference process initially gives every variable an unknown type — represented by a new type variable not appearing anywhere else. For our tiny `N-1` example, we will give `N` the type t~N~.

The `(-)` function has two arguments whose types can be expressed as a tuple of types:

```
(integer,integer)
```

and the types of the actual arguments are also a tuple:

`(`t~N~ `,integer)`

In order for the expression to be type correct, the actual types of the arguments must be consistent with the expected types of the function; which we can do by making them _the same_. There is a particular process used to do this — called _unification_.

![Inferring the Type of `N-1`][minusType]

[minusType]:images/minustype.jpg width=350px

**Unification**
: An algorithm that replaces variables with values in such a way as to make two terms _identical_.

Unification matches equals with equals and handles (type) variables by substitutions — for example, we can unify these two type expressions by _binding_ the type variable t~N~ to `integer`.

We initially picked the type of `N` to be an arbitrary type variable, but the process of checking consistency leads us to refine this and make the type concrete. I.e., the use of `N` in a context where an `integer` is expected is enough to allow the compiler to infer that the type of `N` is indeed `integer` and not t~N~.

Of course, if there are multiple occurrences of `N` then each of those occurrences must also be consistent with `integer`; and if an occurrence is not consistent then the compiler will report an error — a given variable may only have one type!

The bottom line is that **Star**'s types are based on a combination of unification for comparing types and a series of type rules that have the effect of introducing _constraints_ on types based on which language features the programmer uses. The type checker is really a constraint solver: if the constraints are not satisfiable (for example by trying to 'call' a variable and add a number to it) then there is a type error in the program.

The magic of type inference arises because it turns out that solving these constraints is sufficient for robustly type checking programs.

>A sharp-eyed reader will notice that **Star**'s type system is different in nature to that found (say) in OO languages. In **Star**'s type system, types are considered to be consistent in **Star** if they are _equal_.[^This is a slight over-simplification.] This is quite different to the notion of consistency in OO languages where an argument to a function is consistent if its type is a _sub-type_ of the expected type.

However, we would note that the apparent restriction to the type system imposed by type equality is much less severe in practice than in theory — and that OO languages' type systems also incorporate some of the same restrictions.

We are only able to scratch the surface of the type system here. It is certainly true that — like many modern functional languages — **Star**'s type system is complex and subtle. The primary motivation for this complexity is to reduce the burden for the programmer: by being able to infer types automatically, and by being able to address many programming subtleties, the type system comes to be seen as the programmer's friend rather than as an obstacle to be 'gotten around'.

##Are We There Yet?[are-we-there-yet]

The straightforward answer to this is no. There is a great deal more to functional programming than can be captured in a few pages. However, we have covered some of the key features of functional programming — particularly as it applies to **Star**. In subsequent chapters we will take a closer look at collections, at modular programming, at concurrency and even take a pot shot at Monads.

If there is a single idea to take away from this chapter it should be that functional programming is natural. If there is a single piece of advice for the budding functional **Star** programmer, it should be to _hide the recursion_. If there is a single bit of comfort to offer programmers it should be that _Rome was not built in a day_.
